This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
backtester/
  __init__.py
  backtester_v1.py
  basic_backtester.py
  utils.py
common_utils_dev/
  __init__.py
  common_utils_dev.py
dataset_builder/
  build_dataset.py
rawdata_builder/
  __init__.py
  build_rawdata.py
  candidate_assets.txt
  download_kaggle_data.py
reviewer/
  paramset.py
  reviewer_v1.py
  utils.py
trainer/
  datasets/
    dataset.py
  models/
    backbones/
      __init__.py
      backbone_v1.py
      stack_backbone_v1.py
    __init__.py
    basic_predictor.py
    criterions.py
    predictor_v1.py
    stack_predictor_v1.py
    utils.py
  modules/
    block_1d/
      __init__.py
      dense_block.py
      norms.py
      seblock.py
      self_attention.py
    acts.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="backtester/__init__.py">
from .backtester_v1 import BacktesterV1
</file>

<file path="backtester/backtester_v1.py">
import os
from .utils import nan_to_zero
from .basic_backtester import BasicBacktester
from tqdm import tqdm
import gc


CONFIG = {
    "report_prefix": "v001",
    "detail_report": False,
    "position_side": "longshort",
    "entry_ratio": 0.2,
    "commission": {"entry": 0.0004, "exit": 0.0002, "spread": 0.0004},
    "min_holding_minutes": 1,
    "max_holding_minutes": 30,
    "compound_interest": True,
    "order_criterion": "capital",
    "possible_in_debt": True,
    "exit_if_achieved": True,
    "achieve_ratio": 1,
    "achieved_with_commission": False,
    "max_n_updated": 0,
    "positive_entry_threshold": 8,
    "negative_entry_threshold": 8,
    "exit_threshold": "auto",
    "positive_probability_threshold": 8,
    "negative_probability_threshold": 8,
    "adjust_prediction": False,
}


class BacktesterV1(BasicBacktester):
    def __init__(
        self,
        base_currency,
        dataset_dir,
        exp_dir,
        report_prefix=CONFIG["report_prefix"],
        detail_report=CONFIG["detail_report"],
        position_side=CONFIG["position_side"],
        entry_ratio=CONFIG["entry_ratio"],
        commission=CONFIG["commission"],
        min_holding_minutes=CONFIG["min_holding_minutes"],
        max_holding_minutes=CONFIG["max_holding_minutes"],
        compound_interest=CONFIG["compound_interest"],
        order_criterion=CONFIG["order_criterion"],
        possible_in_debt=CONFIG["possible_in_debt"],
        exit_if_achieved=CONFIG["exit_if_achieved"],
        achieve_ratio=CONFIG["achieve_ratio"],
        achieved_with_commission=CONFIG["achieved_with_commission"],
        max_n_updated=CONFIG["max_n_updated"],
        positive_entry_threshold=CONFIG["positive_entry_threshold"],
        negative_entry_threshold=CONFIG["negative_entry_threshold"],
        exit_threshold=CONFIG["exit_threshold"],
        positive_probability_threshold=CONFIG["positive_probability_threshold"],
        negative_probability_threshold=CONFIG["negative_probability_threshold"],
        adjust_prediction=CONFIG["adjust_prediction"],
    ):
        super().__init__(
            base_currency=base_currency,
            dataset_dir=dataset_dir,
            exp_dir=exp_dir,
            report_prefix=report_prefix,
            detail_report=detail_report,
            position_side=position_side,
            entry_ratio=CONFIG["entry_ratio"],
            commission=CONFIG["commission"],
            min_holding_minutes=min_holding_minutes,
            max_holding_minutes=max_holding_minutes,
            compound_interest=compound_interest,
            order_criterion=order_criterion,
            possible_in_debt=possible_in_debt,
            exit_if_achieved=exit_if_achieved,
            achieve_ratio=achieve_ratio,
            achieved_with_commission=achieved_with_commission,
            max_n_updated=max_n_updated,
            positive_entry_threshold=positive_entry_threshold,
            negative_entry_threshold=negative_entry_threshold,
            exit_threshold=exit_threshold,
            positive_probability_threshold=positive_probability_threshold,
            negative_probability_threshold=negative_probability_threshold,
            adjust_prediction=adjust_prediction,
        )

    def run(self, display=True):

        self.build()
        self.initialize()

        for now in tqdm(self.index):
            # Step1: Prepare pricing and signal
            pricing = self.historical_data_dict["pricing"].loc[now]
            predictions = self.historical_data_dict["predictions"].loc[now]
            probabilities = self.historical_data_dict["probabilities"].loc[now]

            # Set assets which has signals
            positive_assets = self.tradable_coins[
                (predictions >= self.positive_entry_bins)
                & (probabilities >= self.positive_probability_bins)
            ]
            negative_assets = self.tradable_coins[
                (predictions <= self.negative_entry_bins)
                & (probabilities >= self.negative_probability_bins)
            ]

            # Debug: Print current state
            print(f"\nTime: {now}")
            print(f"Number of positions: {len(self.positions)}")
            print(f"Number of trade returns: {len(self.historical_trade_returns)}")
            if len(self.positions) > 0:
                print("Current positions:")
                for pos in self.positions:
                    print(f"  {pos.asset} ({pos.side})")

            # Exit
            self.handle_exit(
                positive_assets=positive_assets,
                negative_assets=negative_assets,
                pricing=pricing,
                now=now,
            )

            # Debug: Print state after exit
            print(f"Number of positions after exit: {len(self.positions)}")
            print(f"Number of trade returns after exit: {len(self.historical_trade_returns)}")

            # Compute how much use cache
            if self.compound_interest is False:
                cache_to_order = self.entry_ratio
            else:
                if self.order_criterion == "cache":
                    if self.cache > 0:
                        cache_to_order = nan_to_zero(
                            value=(self.cache * self.entry_ratio)
                        )
                    else:
                        cache_to_order = 0

                elif self.order_criterion == "capital":
                    # Entry with capital base
                    cache_to_order = nan_to_zero(
                        value=(
                            self.compute_capital(pricing=pricing, now=now)
                            * self.entry_ratio
                        )
                    )

            # Entry
            self.handle_entry(
                predictions=predictions,
                cache_to_order=cache_to_order,
                positive_assets=positive_assets,
                negative_assets=negative_assets,
                pricing=pricing,
                now=now,
            )

            # To report
            self.report(value=self.cache, target="historical_caches", now=now)
            self.report(
                value=self.compute_capital(pricing=pricing, now=now),
                target="historical_capitals",
                now=now,
            )
            self.report(value=self.positions, target="historical_positions", now=now)


        report = self.generate_report()



        


        self.store_report(report=report)

        if display is True:
            self.display_metrics()
            self.display_report(report=report)

        # Remove historical data dict to reduce memory usage
        del self.historical_data_dict
        gc.collect()


if __name__ == "__main__":
    import fire

    fire.Fire(BacktesterV1)
</file>

<file path="backtester/basic_backtester.py">
import os
import json
import numpy as np
import pandas as pd
from copy import copy
import matplotlib.pyplot as plt
from abc import abstractmethod
from IPython.display import display, display_markdown
from .utils import load_parquet, Position
from develop.src.common_utils_dev import make_dirs
from collections import OrderedDict, defaultdict
import empyrical as emp
from develop.src.common_utils_dev import to_parquet


CONFIG = {
    "report_prefix": "v001",
    "detail_report": False,
    "position_side": "longshort",
    "entry_ratio": 0.055,
    "commission": {"entry": 0.0004, "exit": 0.0002, "spread": 0.0004},
    "min_holding_minutes": 1,
    "max_holding_minutes": 30,
    "compound_interest": True,
    "order_criterion": "capital",
    "possible_in_debt": False,
    "exit_if_achieved": True,
    "achieve_ratio": 1,
    "achieved_with_commission": False,
    "max_n_updated": 0,
    "positive_entry_threshold": 8,
    "negative_entry_threshold": 8,
    "exit_threshold": "auto",
    "positive_probability_threshold": 8,
    "negative_probability_threshold": 8,
    "adjust_prediction": False,
}


def make_flat(series):
    flatten = []
    for key, values in series.to_dict().items():
        if isinstance(values, list):
            for value in values:
                flatten.append(pd.Series({key: value}))
        else:
            flatten.append(pd.Series({key: values}))

    return pd.concat(flatten).sort_index()


class BasicBacktester:
    def __init__(
        self,
        base_currency,
        dataset_dir,
        exp_dir,
        report_prefix=CONFIG["report_prefix"],
        detail_report=CONFIG["detail_report"],
        position_side=CONFIG["position_side"],
        entry_ratio=CONFIG["entry_ratio"],
        commission=CONFIG["commission"],
        min_holding_minutes=CONFIG["min_holding_minutes"],
        max_holding_minutes=CONFIG["max_holding_minutes"],
        compound_interest=CONFIG["compound_interest"],
        order_criterion=CONFIG["order_criterion"],
        possible_in_debt=CONFIG["possible_in_debt"],
        exit_if_achieved=CONFIG["exit_if_achieved"],
        achieve_ratio=CONFIG["achieve_ratio"],
        achieved_with_commission=CONFIG["achieved_with_commission"],
        max_n_updated=CONFIG["max_n_updated"],
        positive_entry_threshold=CONFIG["positive_entry_threshold"],
        negative_entry_threshold=CONFIG["negative_entry_threshold"],
        exit_threshold=CONFIG["exit_threshold"],
        positive_probability_threshold=CONFIG["positive_probability_threshold"],
        negative_probability_threshold=CONFIG["negative_probability_threshold"],
        adjust_prediction=CONFIG["adjust_prediction"],
    ):
        assert position_side in ("long", "short", "longshort")
        self.base_currency = base_currency
        self.report_prefix = report_prefix
        self.detail_report = detail_report
        self.position_side = position_side
        self.entry_ratio = entry_ratio
        self.commission = commission
        self.min_holding_minutes = min_holding_minutes
        self.max_holding_minutes = max_holding_minutes
        self.compound_interest = compound_interest
        self.order_criterion = order_criterion
        assert self.order_criterion in ("cache", "capital")

        self.possible_in_debt = possible_in_debt
        self.exit_if_achieved = exit_if_achieved
        self.achieve_ratio = achieve_ratio
        self.achieved_with_commission = achieved_with_commission
        self.max_n_updated = max_n_updated
        self.positive_entry_threshold = positive_entry_threshold
        self.negative_entry_threshold = negative_entry_threshold
        self.exit_threshold = exit_threshold
        assert isinstance(exit_threshold, (float, int, str))
        if type(exit_threshold) == str:
            assert (exit_threshold == "auto") or ("*" in exit_threshold)

        self.positive_probability_threshold = positive_probability_threshold
        self.negative_probability_threshold = negative_probability_threshold

        self.adjust_prediction = adjust_prediction

        self.dataset_dir = dataset_dir
        self.exp_dir = exp_dir

        self.initialize()

    def _load_prediction_abs_bins(self):
        return load_parquet(
            path=os.path.join(
                self.exp_dir, "generated_output/prediction_abs_bins.parquet.zstd"
            )
        )

    def _load_probability_bins(self):
        return load_parquet(
            path=os.path.join(
                self.exp_dir, "generated_output/probability_bins.parquet.zstd"
            )
        )

    def _build_historical_data_dict(self, base_currency, historical_data_path_dict):
        historical_data_path_dict = copy(historical_data_path_dict)

        data_dict = {}

        # We use open pricing to handling, entry: open, exit: open
        data_dict["pricing"] = (
            load_parquet(path=historical_data_path_dict.pop("pricing"))
            .xs("open", axis=1, level=1)
            .astype("float16")
        )
        columns = data_dict["pricing"].columns
        columns_with_base_currency = columns[
            columns.str.endswith(base_currency.upper())
        ]
        data_dict["pricing"] = data_dict["pricing"][columns_with_base_currency]

        for data_type, data_path in historical_data_path_dict.items():
            data_dict[data_type] = load_parquet(path=data_path).astype("float16")

            # Filter by base_currency
            data_dict[data_type] = data_dict[data_type][columns_with_base_currency]

        return data_dict

    def _set_bins(self, prediction_abs_bins, probability_bins, index):
        assert (prediction_abs_bins >= 0).all().all()
        assert (probability_bins >= 0).all().all()

        self.positive_entry_bins = None
        self.negative_entry_bins = None
        self.exit_bins = None
        self.positive_probability_bins = None
        self.negative_probability_bins = None

        if isinstance(self.positive_entry_threshold, str):
            if "*" in self.positive_entry_threshold:
                self.positive_entry_bins = (
                    prediction_abs_bins.loc[
                        int(self.positive_entry_threshold.split("*")[0])
                    ]
                    * float(self.positive_entry_threshold.split("*")[-1])
                )[index]
        else:
            self.positive_entry_bins = prediction_abs_bins.loc[
                self.positive_entry_threshold
            ][index]

        if isinstance(self.negative_entry_threshold, str):
            if "*" in self.negative_entry_threshold:
                self.negative_entry_bins = -(
                    prediction_abs_bins.loc[
                        int(self.negative_entry_threshold.split("*")[0])
                    ]
                    * float(self.negative_entry_threshold.split("*")[-1])
                )[index]
        else:
            self.negative_entry_bins = -prediction_abs_bins.loc[
                self.negative_entry_threshold
            ][index]

        if isinstance(self.exit_threshold, str):
            if "*" in self.exit_threshold:
                self.exit_bins = (
                    prediction_abs_bins.loc[int(self.exit_threshold.split("*")[0])]
                    * float(self.exit_threshold.split("*")[-1])
                )[index]
        else:
            self.exit_bins = prediction_abs_bins.loc[self.exit_threshold][index]

        if isinstance(self.positive_probability_threshold, str):
            if "*" in self.positive_probability_threshold:
                self.positive_probability_bins = (
                    probability_bins.loc[
                        int(self.positive_probability_threshold.split("*")[0])
                    ]
                    * float(self.positive_probability_threshold.split("*")[-1])
                )[index]
        else:
            self.positive_probability_bins = probability_bins.loc[
                self.positive_probability_threshold
            ][index]

        if isinstance(self.negative_probability_threshold, str):
            if "*" in self.negative_probability_threshold:
                self.negative_probability_bins = (
                    probability_bins.loc[
                        int(self.negative_probability_threshold.split("*")[0])
                    ]
                    * float(self.negative_probability_threshold.split("*")[-1])
                )[index]
        else:
            self.negative_probability_bins = probability_bins.loc[
                self.negative_probability_threshold
            ][index]

    def build(self):
        self.report_store_dir = os.path.join(self.exp_dir, "reports/")
        make_dirs([self.report_store_dir])

        self.historical_data_dict = self._build_historical_data_dict(
            base_currency=self.base_currency,
            historical_data_path_dict={
                "pricing": os.path.join(self.dataset_dir, "test/pricing.parquet.zstd"),
                "predictions": os.path.join(
                    self.exp_dir, "generated_output/predictions.parquet.zstd"
                ),
                "probabilities": os.path.join(
                    self.exp_dir, "generated_output/probabilities.parquet.zstd"
                ),
                "labels": os.path.join(
                    self.exp_dir, "generated_output/labels.parquet.zstd"
                ),
            },
        )



        self.tradable_coins = self.historical_data_dict["predictions"].columns
        # self.index = (
        #     self.historical_data_dict["predictions"].index
        #     & self.historical_data_dict["pricing"].index
        # ).sort_values()

        common_dates = np.intersect1d(
            self.historical_data_dict["predictions"].index,
            self.historical_data_dict["pricing"].index
        )
        self.index = pd.DatetimeIndex(common_dates).sort_values()

        import pdb
        # pdb.set_trace()




        for key in self.historical_data_dict.keys():
            self.historical_data_dict[key] = self.historical_data_dict[key].reindex(
                self.index
            )

        prediction_abs_bins = self._load_prediction_abs_bins()
        probability_bins = self._load_probability_bins()
        self._set_bins(
            prediction_abs_bins=prediction_abs_bins,
            probability_bins=probability_bins,
            index=self.tradable_coins,
        )

    def initialize(self):
        self.historical_caches = {}
        self.historical_capitals = {}
        self.historical_trade_returns = defaultdict(list)

        if self.detail_report is True:
            self.historical_entry_reasons = defaultdict(list)
            self.historical_exit_reasons = defaultdict(list)
            self.historical_profits = defaultdict(list)
            self.historical_positions = {}

        self.positions = []
        self.cache = 1

    def report(self, value, target, now, append=False):
        if hasattr(self, target) is False:
            return

        if append is True:
            getattr(self, target)[now].append(value)
            return

        assert now not in getattr(self, target)
        getattr(self, target)[now] = value

    def generate_report(self):
        historical_caches = pd.Series(self.historical_caches).rename("cache")
        historical_capitals = pd.Series(self.historical_capitals).rename("capital")
        historical_returns = (
            pd.Series(self.historical_capitals)
            .pct_change(fill_method=None)
            .fillna(0)
            .rename("return")
        )
        historical_trade_returns = pd.Series(self.historical_trade_returns).rename(
            "trade_return"
        )

        report = [
            historical_caches,
            historical_capitals,
            historical_returns,
            historical_trade_returns,
        ]

        if self.detail_report is True:
            historical_entry_reasons = pd.Series(self.historical_entry_reasons).rename(
                "entry_reason"
            )
            historical_exit_reasons = pd.Series(self.historical_exit_reasons).rename(
                "exit_reason"
            )
            historical_profits = pd.Series(self.historical_profits).rename("profit")
            historical_positions = pd.Series(self.historical_positions).rename(
                "position"
            )

            report += [
                historical_entry_reasons,
                historical_exit_reasons,
                historical_profits,
                historical_positions,
            ]

        report = pd.concat(report, axis=1).sort_index()
        report.index = pd.to_datetime(report.index)

        return report

    def store_report(self, report):
        metrics = self.build_metrics().to_frame().T
        to_parquet(
            df=metrics.astype("float32"),
            path=os.path.join(
                self.report_store_dir,
                f"metrics_{self.report_prefix}_{self.base_currency}.parquet.zstd",
            ),
        )

        to_parquet(
            df=report,
            path=os.path.join(
                self.report_store_dir,
                f"report_{self.report_prefix}_{self.base_currency}.parquet.zstd",
            ),
        )

        params = {
            "base_currency": self.base_currency,
            "position_side": self.position_side,
            "entry_ratio": self.entry_ratio,
            "commission": self.commission,
            "min_holding_minutes": self.min_holding_minutes,
            "max_holding_minutes": self.max_holding_minutes,
            "compound_interest": self.compound_interest,
            "order_criterion": self.order_criterion,
            "possible_in_debt": self.possible_in_debt,
            "achieved_with_commission": self.achieved_with_commission,
            "max_n_updated": self.max_n_updated,
            "tradable_coins": tuple(self.tradable_coins.tolist()),
            "exit_if_achieved": self.exit_if_achieved,
            "achieve_ratio": self.achieve_ratio,
            "positive_entry_threshold": self.positive_entry_threshold,
            "negative_entry_threshold": self.negative_entry_threshold,
            "exit_threshold": self.exit_threshold,
            "positive_probability_threshold": self.positive_probability_threshold,
            "negative_probability_threshold": self.negative_probability_threshold,
            "adjust_prediction": self.adjust_prediction,
        }
        with open(
            os.path.join(
                self.report_store_dir,
                f"params_{self.report_prefix}_{self.base_currency}.json",
            ),
            "w",
        ) as f:
            json.dump(params, f)

        print(f"[+] Report is stored: {self.report_prefix}_{self.base_currency}")

    def build_metrics(self):
        assert len(self.historical_caches) != 0
        assert len(self.historical_capitals) != 0
        assert len(self.historical_trade_returns) != 0

        historical_returns = (
            pd.Series(self.historical_capitals).pct_change(fill_method=None).fillna(0)
        )
        historical_trade_returns = make_flat(
            pd.Series(self.historical_trade_returns).rename("trade_return").dropna()
        )

        metrics = OrderedDict()
        metrics["trade_winning_ratio"] = (
            historical_trade_returns[historical_trade_returns != 0] > 0
        ).mean()
        metrics["trade_sharpe_ratio"] = emp.sharpe_ratio(historical_trade_returns)
        metrics["trade_avg_return"] = historical_trade_returns.mean()
        metrics["max_drawdown"] = emp.max_drawdown(historical_returns)
        metrics["total_return"] = historical_returns.add(1).cumprod().sub(1).iloc[-1]

        return pd.Series(metrics)

    def display_metrics(self):
        display_markdown(f"#### Performance metrics: {self.base_currency}", raw=True)
        display(self.build_metrics())

    def display_report(self, report):
        display_markdown(f"#### Report: {self.base_currency}", raw=True)
        _, ax = plt.subplots(4, 1, figsize=(12, 12), sharex=True)

        for idx, column in enumerate(["capital", "cache", "return", "trade_return"]):
            if column == "trade_return":
                report[column].dropna().apply(lambda x: sum(x)).plot(ax=ax[idx])

            else:
                report[column].plot(ax=ax[idx])

            ax[idx].set_title(f"historical {column}")

        plt.tight_layout()
        plt.show()

    def compute_cost_to_order(self, position):
        cache_to_order = position.entry_price * position.qty
        commission_to_order = cache_to_order * (
            self.commission["entry"] + self.commission["spread"]
        )

        return cache_to_order + commission_to_order

    def check_if_executable_order(self, cost):
        if self.possible_in_debt is True:
            return True

        return bool((self.cache - cost) >= 0)

    def pay_cache(self, cost):
        self.cache = self.cache - cost

    def deposit_cache(self, profit):
        self.cache = self.cache + profit

    def compute_adjusted_prediction(
        self, side, entry_price, current_price, entry_prediction, current_prediction
    ):
        if side == "long":
            if entry_price * (1 + entry_prediction) < current_price * (
                1 + current_prediction
            ):
                return ((current_price * (1 + current_prediction)) / entry_price) - 1

        if side == "short":
            if entry_price * (1 + entry_prediction) > current_price * (
                1 + current_prediction
            ):
                return ((current_price * (1 + current_prediction)) / entry_price) - 1

        return entry_prediction

    def update_position_if_already_have(self, position):
        for idx, exist_position in enumerate(self.positions):
            if (exist_position.asset == position.asset) and (
                exist_position.side == position.side
            ):
                # Skip when max_n_updated is None
                if self.max_n_updated is None:
                    return True

                # Skip when position has max_n_updated
                if exist_position.n_updated == self.max_n_updated:

                    adjusted_prediction = exist_position.prediction
                    if self.adjust_prediction is True:
                        adjusted_prediction = self.compute_adjusted_prediction(
                            side=exist_position.side,
                            entry_price=exist_position.entry_price,
                            current_price=position.entry_price,
                            entry_prediction=exist_position.prediction,
                            current_prediction=position.prediction,
                        )

                    # Update only prediction, and entry_at
                    update_position = Position(
                        asset=exist_position.asset,
                        side=exist_position.side,
                        qty=exist_position.qty,
                        entry_price=exist_position.entry_price,
                        prediction=adjusted_prediction,
                        entry_at=position.entry_at,
                        n_updated=exist_position.n_updated,
                    )

                    self.positions[idx] = update_position
                    # return fake updated mark
                    return True

                update_entry_price = (
                    (exist_position.entry_price * exist_position.qty)
                    + (position.entry_price * position.qty)
                ) / (exist_position.qty + position.qty)

                # This is currently invalid way, but acceptable.
                update_prediction = (
                    (exist_position.prediction * exist_position.qty)
                    + (position.prediction * position.qty)
                ) / (exist_position.qty + position.qty)

                # Update entry_price, entry_at and qty
                update_position = Position(
                    asset=exist_position.asset,
                    side=exist_position.side,
                    qty=exist_position.qty + position.qty,
                    entry_price=update_entry_price,
                    entry_at=position.entry_at,
                    prediction=update_prediction,
                    n_updated=exist_position.n_updated + 1,
                )

                # Compute cost by only current order
                cost = self.compute_cost_to_order(position=position)
                executable_order = self.check_if_executable_order(cost=cost)

                # Update
                if executable_order is True:
                    self.pay_cache(cost=cost)
                    self.positions[idx] = update_position

                    # updated
                    return True

        return False

    def compute_profit(self, position, pricing, now, achieved=False):
        current_price = pricing[position.asset]

        if position.side == "long":
            profit_without_commission = current_price * position.qty

        if position.side == "short":
            profit_without_commission = position.entry_price * position.qty
            profit_without_commission += (
                (current_price - position.entry_price) * position.qty * -1
            )

        exit_commission = self.commission["exit"]
        if achieved is not True:
            exit_commission = (self.commission["exit"] * 2) + self.commission["spread"]

        commission_to_order = profit_without_commission * exit_commission

        return profit_without_commission - commission_to_order

    def compute_capital(self, pricing, now):
        # capital = cache + value of positions
        capital = self.cache

        for position in self.positions:
            current_price = pricing[position.asset]

            if position.side == "long":
                capital += current_price * position.qty

            if position.side == "short":
                capital += position.entry_price * position.qty
                capital += (current_price - position.entry_price) * position.qty * -1

        return capital

    def check_if_opposite_position_exists(self, order_asset, order_side):
        if order_side == "long":
            opposite_side = "short"
        if order_side == "short":
            opposite_side = "long"

        for exist_position in self.positions:
            if (exist_position.asset == order_asset) and (
                exist_position.side == opposite_side
            ):
                return True

        return False

    def entry_order(self, asset, side, cache_to_order, pricing, prediction, now):
        if cache_to_order == 0:
            return

        # if opposite position exists, we dont entry
        if (
            self.check_if_opposite_position_exists(order_asset=asset, order_side=side)
            is True
        ):
            return

        entry_price = pricing[asset]
        qty = cache_to_order / entry_price

        position = Position(
            asset=asset,
            side=side,
            qty=qty,
            entry_price=entry_price,
            prediction=prediction,
            entry_at=now,
        )

        updated = self.update_position_if_already_have(position=position)
        if updated is True:
            self.report(
                value={asset: "updated"},
                target="historical_entry_reasons",
                now=now,
                append=True,
            )
            return
        else:
            cost = self.compute_cost_to_order(position=position)
            executable_order = self.check_if_executable_order(cost=cost)

            if executable_order is True:
                self.pay_cache(cost=cost)
                self.positions.append(position)
                self.report(
                    value={asset: "signal"},
                    target="historical_entry_reasons",
                    now=now,
                    append=True,
                )

    def exit_order(self, position, pricing, now, achieved=False):
        print(f"\nCalculating profit for {position.asset} ({position.side})")
        profit = self.compute_profit(
            position=position, pricing=pricing, now=now, achieved=achieved
        )
        print(f"Profit: {profit}")
        self.deposit_cache(profit=profit)

        net_profit = profit - (position.entry_price * position.qty)
        print(f"Net profit: {net_profit}")
        print(f"Trade return: {net_profit / (position.entry_price * position.qty)}")
        
        self.report(value=net_profit, target="historical_profits", now=now, append=True)
        self.report(
            value=(net_profit / (position.entry_price * position.qty)),
            target="historical_trade_returns",
            now=now,
            append=True,
        )
        print(f"Historical trade returns after update: {len(self.historical_trade_returns)}")

    def handle_entry(
        self,
        predictions,
        cache_to_order,
        positive_assets,
        negative_assets,
        pricing,
        now,
    ):
        # Entry order
        if self.position_side in ("long", "longshort"):
            for order_asset in positive_assets:
                self.entry_order(
                    asset=order_asset,
                    side="long",
                    cache_to_order=cache_to_order,
                    pricing=pricing,
                    prediction=predictions[order_asset],
                    now=now,
                )

        if self.position_side in ("short", "longshort"):
            for order_asset in negative_assets:
                self.entry_order(
                    asset=order_asset,
                    side="short",
                    cache_to_order=cache_to_order,
                    pricing=pricing,
                    prediction=predictions[order_asset],
                    now=now,
                )

    def handle_exit(self, positive_assets, negative_assets, pricing, now):
        import pdb
        # pdb.set_trace()

        for position_idx, position in enumerate(self.positions):
            # Handle achievement
            if self.exit_if_achieved is True:
                if (
                    self.check_if_achieved(position=position, pricing=pricing, now=now)
                    is True
                ):
                    print(f"\nExiting position due to achievement: {position.asset} ({position.side})")
                    self.exit_order(
                        position=position, pricing=pricing, now=now, achieved=True
                    )
                    self.report(
                        value={position.asset: "achieved"},
                        target="historical_exit_reasons",
                        now=now,
                        append=True,
                    )
                    self.positions[position_idx].is_exited = True
                    continue

            # Keep position if matched
            if (position.side == "long") and (position.asset in positive_assets):
                continue

            if (position.side == "short") and (position.asset in negative_assets):
                continue

            passed_minutes = (
                pd.Timestamp(now) - pd.Timestamp(position.entry_at)
            ).total_seconds() / 60

            # Handle min_holding_minutes
            if passed_minutes <= self.min_holding_minutes:
                continue

            # Handle max_holding_minutes
            if passed_minutes >= self.max_holding_minutes:
                print(f"\nExiting position due to max holding time: {position.asset} ({position.side})")
                self.exit_order(position=position, pricing=pricing, now=now)
                self.report(
                    value={position.asset: "max_holding_minutes"},
                    target="historical_exit_reasons",
                    now=now,
                    append=True,
                )
                self.positions[position_idx].is_exited = True
                continue

            # Handle exit signal
            if (position.side == "long") and (position.asset in negative_assets):
                print(f"\nExiting position due to opposite signal: {position.asset} ({position.side})")
                self.exit_order(position=position, pricing=pricing, now=now)
                self.report(
                    value={position.asset: "opposite_signal"},
                    target="historical_exit_reasons",
                    now=now,
                    append=True,
                )
                self.positions[position_idx].is_exited = True
                continue

            if (position.side == "short") and (position.asset in positive_assets):
                print(f"\nExiting position due to opposite signal: {position.asset} ({position.side})")
                self.exit_order(position=position, pricing=pricing, now=now)
                self.report(
                    value={position.asset: "opposite_signal"},
                    target="historical_exit_reasons",
                    now=now,
                    append=True,
                )
                self.positions[position_idx].is_exited = True
                continue

        # Delete exited positions
        self.positions = [
            position for position in self.positions if position.is_exited is not True
        ]

    def check_if_achieved(self, position, pricing, now):
        current_price = pricing[position.asset]

        diff_price = current_price - position.entry_price
        if self.achieved_with_commission is True:
            if position.side == "long":
                commission = (
                    current_price
                    * (self.commission["exit"] + self.commission["spread"])
                ) + (
                    position.entry_price
                    * (self.commission["entry"] + self.commission["spread"])
                )
            if position.side == "short":
                commission = -(
                    (
                        current_price
                        * (self.commission["exit"] + self.commission["spread"])
                    )
                    + (
                        position.entry_price
                        * (self.commission["entry"] + self.commission["spread"])
                    )
                )

            diff_price = diff_price - commission

        if diff_price != 0:
            trade_return = diff_price / position.entry_price
        else:
            trade_return = 0

        trade_return = trade_return / self.achieve_ratio

        if self.exit_threshold == "auto":
            if position.side == "long":
                assert position.prediction > 0
                if trade_return >= position.prediction:
                    return True

            if position.side == "short":
                assert position.prediction < 0
                if trade_return <= position.prediction:
                    return True
        else:
            if position.side == "long":
                if trade_return >= self.exit_bins[position.asset]:
                    return True

            if position.side == "short":
                if trade_return <= -self.exit_bins[position.asset]:
                    return True

        return False

    @abstractmethod
    def run(self):
        pass
</file>

<file path="backtester/utils.py">
import pandas as pd


def nan_to_zero(value):
    if str(value) in ("nan", "None"):
        return 0

    return value


def load_parquet(path):
    return pd.read_parquet(path)


class Position:
    def __init__(
        self,
        asset,
        side,
        qty,
        entry_price,
        prediction,
        entry_at,
        n_updated=0,
        is_exited=False,
    ):
        self.asset = asset
        self.side = side
        self.qty = qty
        self.entry_price = entry_price
        self.prediction = prediction
        self.entry_at = entry_at
        self.n_updated = n_updated
        self.is_exited = is_exited

    def __getitem__(self, key):
        return getattr(self, key)

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __repr__(self):
        return f"Position(asset={self.asset}, side={self.side}, qty={self.qty}, entry_price={self.entry_price:.4f}, n_updated={self.n_updated}, is_exited={str(self.is_exited)})"

    def __str__(self):
        return self.__repr__()
</file>

<file path="common_utils_dev/__init__.py">
from .common_utils_dev import (
    make_dirs,
    load_text,
    load_json,
    to_parquet,
    get_filename_by_path,
    to_abs_path,
    get_parent_dir,
)
</file>

<file path="common_utils_dev/common_utils_dev.py">
import os
import json
import pyarrow.parquet as pq
import pyarrow as pa
from pathlib import Path


def make_dirs(dirs):
    for dir in dirs:
        os.makedirs(dir, exist_ok=True)


def load_text(path):
    with open(path, "r") as f:
        text = f.read().splitlines()

    return text


def load_json(path):
    with open(path, "r") as f:
        loaded = json.load(f)

    return loaded


def to_parquet(df, path, compression="zstd"):
    pq.write_table(table=pa.Table.from_pandas(df), where=path, compression=compression)


def get_filename_by_path(path):
    return Path(path).stem.split(".")[0]


def get_parent_dir(path):
    return Path(path).parent


def to_abs_path(file, relative_path):
    return os.path.normpath(
        os.path.join(os.path.dirname(os.path.abspath(file)), relative_path)
    )
</file>

<file path="dataset_builder/build_dataset.py">
import os
import gc
import json
from glob import glob
from typing import Optional, List
import pandas as pd
import numpy as np
from tqdm import tqdm
from functools import partial
from itertools import combinations
from sklearn import preprocessing
import joblib
from develop.src.common_utils_dev import make_dirs, to_parquet, to_abs_path, get_filename_by_path
from pandarallel import pandarallel
from dataclasses import dataclass


CONFIG = {
    "rawdata_dir": to_abs_path(__file__, "../../storage/dataset/rawdata/cleaned/"),
    "data_store_dir": to_abs_path(__file__, "../../storage/dataset/dataset/v001/"),
    "lookahead_window": 30,
    "train_ratio": 0.80,
    "scaler_type": "StandardScaler",
    "winsorize_threshold": 6,
    "query_min_start_dt": "2018-06-01",
}
OHLC = ["open", "high", "low", "close"]


@dataclass
class DatasetBuilder:
    # Defined in running code.
    # Need to give below parameters when build in trader
    tradable_coins: Optional[List] = None
    feature_columns: Optional[List] = None
    feature_scaler: Optional[preprocessing.StandardScaler] = None
    label_scaler: Optional[preprocessing.StandardScaler] = None

    def build_rawdata(self, file_names, query_min_start_dt):
        def _load_rawdata_row(file_name):
            rawdata = pd.read_parquet(file_name)[OHLC]
            rawdata.index = pd.to_datetime(rawdata.index)
            rawdata = rawdata[query_min_start_dt:]

            return rawdata

        rawdata = {}
        for file_name in tqdm(file_names):
            coin = get_filename_by_path(file_name)
            rawdata[coin] = _load_rawdata_row(file_name=file_name)

        rawdata = pd.concat(rawdata, axis=1).sort_index()

        self.tradable_coins = sorted(rawdata.columns.levels[0].tolist())

        return rawdata[self.tradable_coins]

    def _build_feature_by_rawdata_row(self, rawdata_row):
        returns_1320m = (
            rawdata_row[OHLC]
            .pct_change(1320, fill_method=None)
            .rename(columns={key: key + "_return(1320)" for key in OHLC})
        ).dropna()

        madiv_1320m = (
            (
                rawdata_row[OHLC]
                .rolling(1320)
                .mean()
                .rename(columns={key: key + "_madiv(1320)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        returns_600m = (
            (
                rawdata_row[OHLC]
                .pct_change(600, fill_method=None)
                .rename(columns={key: key + "_return(600)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        madiv_600m = (
            (
                rawdata_row[OHLC]
                .rolling(600)
                .mean()
                .rename(columns={key: key + "_madiv(600)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        returns_240m = (
            (
                rawdata_row[OHLC]
                .pct_change(240, fill_method=None)
                .rename(columns={key: key + "_return(240)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        madiv_240m = (
            (
                rawdata_row[OHLC]
                .rolling(240)
                .mean()
                .rename(columns={key: key + "_madiv(240)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        returns_120m = (
            (
                rawdata_row[OHLC]
                .pct_change(120, fill_method=None)
                .rename(columns={key: key + "_return(120)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        madiv_120m = (
            (
                rawdata_row[OHLC]
                .rolling(120)
                .mean()
                .rename(columns={key: key + "_madiv(120)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        returns_1m = (
            (
                rawdata_row[OHLC]
                .pct_change(1, fill_method=None)
                .rename(columns={key: key + "_return(1)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        inner_changes = []
        for column_pair in sorted(list(combinations(OHLC, 2))):
            inner_changes.append(
                rawdata_row[list(column_pair)]
                .pct_change(1, axis=1, fill_method=None)[column_pair[-1]]
                .rename("_".join(column_pair) + "_change")
            )

        inner_changes = pd.concat(inner_changes, axis=1).reindex(returns_1320m.index)

        feature = pd.concat(
            [
                returns_1320m,
                madiv_1320m,
                returns_600m,
                madiv_600m,
                returns_240m,
                madiv_240m,
                returns_120m,
                madiv_120m,
                returns_1m,
                inner_changes,
            ],
            axis=1,
        ).sort_index()

        return feature

    def build_features(self, rawdata):
        features = {}
        for coin in tqdm(self.tradable_coins):
            features[coin] = self._build_feature_by_rawdata_row(
                rawdata_row=rawdata[coin]
            )

        features = pd.concat(features, axis=1).sort_index()[self.tradable_coins]

        if self.feature_columns is None:
            self.feature_columns = features.columns
            return features

        return features[self.feature_columns]

    def build_scaler(self, data, scaler_type):
        scaler = getattr(preprocessing, scaler_type)()
        scaler.fit(data)

        return scaler

    def preprocess_features(self, features, winsorize_threshold):
        assert self.feature_scaler is not None

        features = pd.DataFrame(
            self.feature_scaler.transform(features),
            index=features.index,
            columns=features.columns,
        )

        if winsorize_threshold is not None:
            features = (
                features.clip(-winsorize_threshold, winsorize_threshold)
                / winsorize_threshold
            )

        return features

    def preprocess_labels(self, labels, winsorize_threshold):
        assert self.label_scaler is not None

        labels = pd.DataFrame(
            self.label_scaler.transform(labels),
            index=labels.index,
            columns=labels.columns,
        )

        if winsorize_threshold is not None:
            labels = (
                labels.clip(-winsorize_threshold, winsorize_threshold)
                / winsorize_threshold
            )

        return labels

    def _build_label(self, rawdata_row, lookahead_window):
        # build fwd_return(window)
        pricing = rawdata_row["open"].sort_index()
        fwd_return = (
            pricing.pct_change(lookahead_window, fill_method=None)
            .shift(-lookahead_window - 1)
            .rename(f"fwd_return({lookahead_window})")
            .sort_index()
        )[: -lookahead_window - 1]

        return fwd_return

    def build_labels(self, rawdata, lookahead_window):
        labels = []
        for coin in tqdm(self.tradable_coins):
            labels.append(
                self._build_label(
                    rawdata_row=rawdata[coin], lookahead_window=lookahead_window
                ).rename(coin)
            )

        labels = pd.concat(labels, axis=1).sort_index()[self.tradable_coins]

        return labels

    def store_artifacts(
        self,
        features,
        labels,
        pricing,
        feature_scaler,
        label_scaler,
        train_ratio,
        params,
        data_store_dir,
    ):
        # Make dirs
        train_data_store_dir = os.path.join(data_store_dir, "train")
        test_data_store_dir = os.path.join(data_store_dir, "test")
        make_dirs([train_data_store_dir, test_data_store_dir])

        # Store params
        joblib.dump(feature_scaler, os.path.join(data_store_dir, "feature_scaler.pkl"))
        joblib.dump(label_scaler, os.path.join(data_store_dir, "label_scaler.pkl"))

        with open(os.path.join(data_store_dir, "dataset_params.json"), "w") as f:
            json.dump(params, f)

        print(f"[+] Metadata is stored")

        # Store dataset
        boundary_index = int(len(features.index) * train_ratio)

        for file_name, data in [
            ("X.parquet.zstd", features),
            ("Y.parquet.zstd", labels),
            ("pricing.parquet.zstd", pricing),
        ]:
            to_parquet(
                df=data.iloc[:boundary_index],
                path=os.path.join(train_data_store_dir, file_name),
            )

            to_parquet(
                df=data.iloc[boundary_index:],
                path=os.path.join(test_data_store_dir, file_name),
            )

        print(f"[+] Dataset is stored")

    def build(
        self,
        rawdata_dir=CONFIG["rawdata_dir"],
        data_store_dir=CONFIG["data_store_dir"],
        lookahead_window=CONFIG["lookahead_window"],
        train_ratio=CONFIG["train_ratio"],
        scaler_type=CONFIG["scaler_type"],
        winsorize_threshold=CONFIG["winsorize_threshold"],
        query_min_start_dt=CONFIG["query_min_start_dt"],
    ):
        assert scaler_type in ("RobustScaler", "StandardScaler")
        pandarallel.initialize()

        # Make dirs
        make_dirs([data_store_dir])

        # Set file_names
        file_names = sorted(glob(os.path.join(rawdata_dir, "*")))
        assert len(file_names) != 0

        # Build rawdata
        rawdata = self.build_rawdata(
            file_names=file_names, query_min_start_dt=query_min_start_dt
        )
        gc.collect()

        # Build features
        features = self.build_features(rawdata=rawdata)
        self.feature_scaler = self.build_scaler(data=features, scaler_type=scaler_type)
        features = self.preprocess_features(
            features=features, winsorize_threshold=winsorize_threshold
        )
        gc.collect()

        # build labels
        labels = self.build_labels(rawdata=rawdata, lookahead_window=lookahead_window)
        self.label_scaler = self.build_scaler(data=labels, scaler_type=scaler_type)
        labels = self.preprocess_labels(
            labels=labels, winsorize_threshold=winsorize_threshold
        )
        gc.collect()

        # Masking with common index
        # common_index = (features.index & labels.index).sort_values()
        common_index = pd.Index(np.intersect1d(features.index, labels.index)).sort_values()


        features = features.reindex(common_index)
        labels = labels.reindex(common_index)
        pricing = rawdata.reindex(common_index)

        params = {
            "lookahead_window": lookahead_window,
            "train_ratio": train_ratio,
            "scaler_type": scaler_type,
            "features_columns": features.columns.tolist(),
            "labels_columns": labels.columns.tolist(),
            "tradable_coins": self.tradable_coins,
            "winsorize_threshold": winsorize_threshold,
            "query_min_start_dt": query_min_start_dt,
        }

        # Store Artifacts
        self.store_artifacts(
            features=features,
            labels=labels,
            pricing=pricing,
            feature_scaler=self.feature_scaler,
            label_scaler=self.label_scaler,
            train_ratio=train_ratio,
            params=params,
            data_store_dir=data_store_dir,
        )


if __name__ == "__main__":
    import fire

    fire.Fire(DatasetBuilder)
</file>

<file path="rawdata_builder/__init__.py">
from .build_rawdata import build_rawdata
</file>

<file path="rawdata_builder/build_rawdata.py">
import os
import pandas as pd
from glob import glob
from tqdm import tqdm
from develop.src.common_utils_dev import (
    make_dirs,
    load_text,
    get_filename_by_path,
    to_parquet,
    get_filename_by_path,
    to_abs_path,
)


CONFIG = {
    "raw_spot_rawdata_dir": to_abs_path(
        __file__, "../../storage/dataset/rawdata/raw/spot/"
    ),
    "raw_future_rawdata_dir": to_abs_path(
        # __file__, "../../storage/dataset/rawdata/raw/future/"
        __file__, "../../storage/dataset/rawdata/raw/spot/"
    ),
    "cleaned_rawdata_store_dir": to_abs_path(
        __file__, "../../storage/dataset/rawdata/cleaned/"
    ),
    "candidate_assets_path": to_abs_path(__file__, "./candidate_assets.txt"),
    "query_min_start_dt": "2018-01-01",
    "boundary_dt_must_have_data": "2019-09-01",
}


def build_rawdata(
    raw_spot_rawdata_dir=CONFIG["raw_spot_rawdata_dir"],
    raw_future_rawdata_dir=CONFIG["raw_future_rawdata_dir"],
    cleaned_rawdata_store_dir=CONFIG["cleaned_rawdata_store_dir"],
    candidate_assets_path=CONFIG["candidate_assets_path"],
    query_min_start_dt=CONFIG["query_min_start_dt"],
    boundary_dt_must_have_data=CONFIG["boundary_dt_must_have_data"],
):
    make_dirs([cleaned_rawdata_store_dir])
    candidate_assets = load_text(path=candidate_assets_path)

    count_files = 0
    for candidate_asset in tqdm(candidate_assets):
        spot_file_path = os.path.join(
            raw_spot_rawdata_dir, f"{candidate_asset}.parquet"
        )
        future_file_path = os.path.join(
            raw_future_rawdata_dir, f"{candidate_asset}.parquet"
        )

        spot_df = pd.read_parquet(spot_file_path)[
            ["open", "high", "low", "close"]
        ].sort_index()
        future_df = pd.read_parquet(future_file_path)[
            ["open", "high", "low", "close"]
        ].sort_index()

        df = pd.concat([spot_df[spot_df.index < future_df.index[0]], future_df])
        df = df.resample("1T").ffill()

        df = df[query_min_start_dt:]
        if df.index[0] > pd.Timestamp(boundary_dt_must_have_data):
            print(f"[!] Skiped: {candidate_asset}")
            continue

        assert not df.isnull().any().any()
        assert len(df.index.unique()) == len(df.index)

        store_filename = candidate_asset + ".parquet.zstd"
        df.index = df.index.tz_localize("utc")
        to_parquet(df=df, path=os.path.join(cleaned_rawdata_store_dir, store_filename))
        count_files += 1

    print(f"[+] Built rawdata: {count_files}")


if __name__ == "__main__":
    import fire

    fire.Fire(build_rawdata)
</file>

<file path="rawdata_builder/candidate_assets.txt">
BTC-USDT
ETH-USDT
LINK-USDT
FIL-USDT
YFI-USDT
LTC-USDT
BCH-USDT
YFII-USDT
XRP-USDT
DOT-USDT
BNB-USDT
TRB-USDT
ADA-USDT
UNI-USDT
AAVE-USDT
EOS-USDT
WAVES-USDT
RSR-USDT
THETA-USDT
EGLD-USDT
SXP-USDT
TRX-USDT
AVAX-USDT
VET-USDT
XTZ-USDT
COMP-USDT
ZEC-USDT
CRV-USDT
XMR-USDT
ATOM-USDT
MKR-USDT
OMG-USDT
BAND-USDT
XLM-USDT
SUSHI-USDT
BZRX-USDT
RUNE-USDT
KAVA-USDT
SNX-USDT
SOL-USDT
BEL-USDT
RLC-USDT
</file>

<file path="rawdata_builder/download_kaggle_data.py">
import os
import kaggle
from common_utils_dev import to_abs_path, make_dirs


SPOT_CONFIG = {
    "dataset_name": "jorijnsmit/binance-full-history",
    "store_dir": to_abs_path(__file__, "../../storage/dataset/rawdata/raw/spot/"),
}

FUTURE_CONFIG = {
    "dataset_name": "nicolaes/binance-futures",
    "store_dir": to_abs_path(__file__, "../../storage/dataset/rawdata/raw/future/"),
}


def download(username, key):
    for config in [SPOT_CONFIG, FUTURE_CONFIG]:
        dataset_name = config["dataset_name"]
        store_dir = config["store_dir"]

        make_dirs([store_dir])

        # Set env to authenticate
        os.environ["KAGGLE_USERNAME"] = username
        os.environ["KAGGLE_KEY"] = key
        kaggle.api.authenticate()

        # Download
        kaggle.api.dataset_download_files(dataset_name, path=store_dir, unzip=True)


if __name__ == "__main__":
    import fire

    fire.Fire(download)
</file>

<file path="reviewer/paramset.py">
from collections import OrderedDict

V1_SET1 = OrderedDict(
    dict(
        base_currency="USDT",
        position_side="longshort",
        exit_if_achieved=True,
        achieve_ratio=[0.8, 1],
        achieved_with_commission=True,
        min_holding_minutes=[0, 1],
        max_holding_minutes=30,
        entry_ratio=0.09,
        commission={"entry": 0.0, "exit": 0.0, "spread": 0.0},
        compound_interest=True,
        order_criterion="capital",
        max_n_updated=0,
        positive_entry_threshold=[9],
        negative_entry_threshold=[9],
        exit_threshold="auto",
        positive_probability_threshold=[5, 6, 7, 8, 9],
        negative_probability_threshold=[8, 9, "9*1.25", "9*1.5"],
        adjust_prediction=False,
        possible_in_debt=False,
    )
)

V1_SET2 = OrderedDict(
    dict(
        base_currency="USDT",
        position_side="longshort",
        exit_if_achieved=True,
        achieve_ratio=[0.8, 1],
        achieved_with_commission=True,
        min_holding_minutes=[0, 1],
        max_holding_minutes=30,
        entry_ratio=0.1,
        commission={"entry": 0.0, "exit": 0.0, "spread": 0.0},
        compound_interest=True,
        order_criterion="cache",
        max_n_updated=0,
        positive_entry_threshold=[8, 9],
        negative_entry_threshold=[8, 9],
        exit_threshold="auto",
        positive_probability_threshold=[5, 6, 7, 8, 9],
        negative_probability_threshold=[8, 9, "9*1.25", "9*1.5"],
        adjust_prediction=False,
    )
)


V1_CSET1 = OrderedDict(
    dict(
        base_currency="USDT",
        position_side="longshort",
        exit_if_achieved=True,
        achieve_ratio=[0.8, 1],
        achieved_with_commission=True,
        min_holding_minutes=[0, 1],
        max_holding_minutes=30,
        entry_ratio=0.09,
        commission={"entry": 0.0004, "exit": 0.0002, "spread": 0.0004},
        compound_interest=True,
        order_criterion="capital",
        max_n_updated=0,
        positive_entry_threshold=[9],
        negative_entry_threshold=[9],
        exit_threshold="auto",
        positive_probability_threshold=[5, 6, 7, 8, 9],
        negative_probability_threshold=[8, 9, "9*1.25", "9*1.5"],
        adjust_prediction=False,
        possible_in_debt=False,
    )
)

V1_CSET2 = OrderedDict(
    dict(
        base_currency="USDT",
        position_side="longshort",
        exit_if_achieved=True,
        achieve_ratio=[0.8, 1],
        achieved_with_commission=True,
        min_holding_minutes=[0, 1],
        max_holding_minutes=30,
        entry_ratio=0.1,
        commission={"entry": 0.0004, "exit": 0.0002, "spread": 0.0004},
        compound_interest=True,
        order_criterion="cache",
        max_n_updated=0,
        positive_entry_threshold=[8, 9],
        negative_entry_threshold=[8, 9],
        exit_threshold="auto",
        positive_probability_threshold=[5, 6, 7, 8, 9],
        negative_probability_threshold=[8, 9, "9*1.25", "9*1.5"],
        adjust_prediction=False,
    )
)
</file>

<file path="reviewer/reviewer_v1.py">
from dataclasses import dataclass
from typing import Dict, List, Union
from joblib import Parallel, delayed
from IPython.display import display, display_markdown
from tqdm import tqdm
from develop.src import backtester
import os
import pandas as pd
from glob import glob
import matplotlib.pyplot as plt
from .utils import grid
import json
from develop.src.reviewer import paramset
from develop.src.common_utils_dev import to_abs_path
from tabulate import tabulate
import fancytable as ft


@dataclass
class ReviewerV1:
    # dataset_dir: str = to_abs_path(__file__, "../../storage/dataset/dataset/v001/")
    dataset_dir = "develop/storage/dataset/dataset/v001/"
    # exp_dir: str = to_abs_path(__file__, "../../storage/experiments/v001/")
    exp_dir = "develop/storage/experiments/v001/"
    reviewer_prefix: str = "v001"
    grid_params: Union[str, Dict[str, List]] = "V1_SET1"
    backtester_type: str = "BacktesterV1"
    exec_start: int = 0
    exec_end: int = None
    n_jobs: int = 16

    def __post_init__(self):
        if isinstance(self.grid_params, str):
            self.grid_params = getattr(paramset, self.grid_params)

        self.grid_params["dataset_dir"] = self.dataset_dir
        self.grid_params["exp_dir"] = self.exp_dir

        self._build_backtesters()

    def _load_data_dict(self):
        data_dict = {}
        for key in ("labels", "predictions", "probabilities"):
            data_dict[key] = pd.read_parquet(
                os.path.join(self.exp_dir, f"generated_output/{key}.parquet.zstd")
            )

        return data_dict

    def _display_timeseries(self, data_dict):
        columns = data_dict["predictions"].columns
        _, ax = plt.subplots(len(columns), 1, figsize=(24, 2.5 * len(columns)))

        for idx, column in enumerate(columns):
            data_dict["labels"][column].rename("label").plot(ax=ax[idx], alpha=0.5)
            data_dict["predictions"][column].rename("prediction").plot(ax=ax[idx])
            ax[idx].legend()
            ax[idx].set_title(column)

        plt.tight_layout()
        plt.show()

    def _build_levels(self, data):
        levels = {}
        for column in data.columns:
            levels[column] = pd.qcut(data[column], 10, labels=False, retbins=False)

        return pd.concat(levels, axis=1)

    def _build_total_performance(self, data_dict):
        total_performance = (data_dict["labels"] * data_dict["predictions"] >= 0).mean()
        total_performance["mean"] = total_performance.mean()

        return total_performance

    def _build_performance_on_levels(self, data_dict, levels):
        performance = data_dict["labels"] * data_dict["predictions"] >= 0

        performance_on_levels = []
        for column in performance.columns:
            performance_on_levels.append(
                performance[column].groupby(levels[column]).mean()
            )

        performance_on_levels = pd.concat(performance_on_levels, axis=1)
        performance_on_levels["mean"] = performance_on_levels.mean(axis=1)

        return performance_on_levels

    def display_performance(self):
        data_dict = self._load_data_dict()

        display_markdown("#### Timeseries", raw=True)
        self._display_timeseries(data_dict=data_dict)

        display_markdown("#### Total Performance", raw=True)
        total_performance = self._build_total_performance(data_dict=data_dict)
        display(ft.display(total_performance.rename("bin_acc").to_frame().T, axis=1))

        # Build levels
        label_levels = self._build_levels(data=data_dict["labels"])
        prediction_levels = self._build_levels(data=data_dict["predictions"])
        abs_prediction_levels = self._build_levels(data=data_dict["predictions"].abs())
        probability_levels = self._build_levels(data=data_dict["probabilities"])

        display_markdown("#### Performance on label levels", raw=True)
        display(
            ft.display(
                self._build_performance_on_levels(
                    data_dict=data_dict, levels=label_levels
                )
            )
        )

        display_markdown("#### Performance on prediction levels", raw=True)
        display(
            ft.display(
                self._build_performance_on_levels(
                    data_dict=data_dict, levels=prediction_levels
                )
            )
        )

        display_markdown("#### Performance on abs(prediction) levels", raw=True)
        display(
            ft.display(
                self._build_performance_on_levels(
                    data_dict=data_dict, levels=abs_prediction_levels
                )
            )
        )

        display_markdown("#### Performance on probability levels", raw=True)
        display(
            ft.display(
                self._build_performance_on_levels(
                    data_dict=data_dict, levels=probability_levels
                )
            )
        )

    def _exists_artifact(self, index):
        exists = []
        for artifact_type in ["metrics", "report", "params"]:
            file_path = os.path.join(
                self.grid_params["exp_dir"],
                f"reports/{artifact_type}_{self.reviewer_prefix}_{index}_{self.grid_params['base_currency']}.parquet.zstd",
            )

            if artifact_type in ("params"):
                exists.append(
                    os.path.exists(file_path.replace(".parquet.zstd", ".json"))
                )
                continue

            exists.append(os.path.exists(file_path))

        exists = all(exists)

        if exists is True:
            print(f"[!] Found backtests already done: {index}")

        return exists

    def _build_backtesters(self):
        def _is_valid_params(param):
            if param["adjust_prediction"] is True:
                if isinstance(param["exit_threshold"], (int, float)):
                    return False

                if param["max_n_updated"] is None:
                    return False

            if param["exit_threshold"] != "auto":
                if param["achieve_ratio"] != 1:
                    return False

            return True

        grid_params = list(grid(self.grid_params))

        # Filter grid_params
        grid_params = [
            grid_param
            for grid_param in grid_params
            if _is_valid_params(param=grid_param) is True
        ]

        # Build backtesters
        self.backtesters = [
            getattr(backtester, self.backtester_type)(
                report_prefix=f"{self.reviewer_prefix}_{idx}", **params
            )
            for idx, params in enumerate(grid_params)
        ][self.exec_start : self.exec_end]
        self.backtesters = [
            backtester
            for backtester in self.backtesters
            if self._exists_artifact(index=backtester.report_prefix.split("_")[-1])
            is not True
        ]
        import pdb
        # pdb.set_trace()

    def _load_artifact(self, artifact_type, index):
        assert artifact_type in ("metrics", "report", "params")

        file_path = os.path.join(
            self.grid_params["exp_dir"],
            f"reports/{artifact_type}_{self.reviewer_prefix}_{index}_{self.grid_params['base_currency']}.parquet.zstd",
        )

        if artifact_type in ("metrics", "report"):
            artifact = pd.read_parquet(file_path)
        else:
            artifact = json.load(open(file_path.replace(".parquet.zstd", ".json"), "r"))

        return artifact

    def _load_artifacts(self, artifact_type, with_index=False):
        assert artifact_type in ("metrics", "report")

        file_paths = glob(
            os.path.join(
                self.grid_params["exp_dir"],
                f"reports/{artifact_type}_{self.reviewer_prefix}_*_{self.grid_params['base_currency']}.parquet.zstd",
            )
        )
        file_paths = sorted(
            file_paths,
            key=lambda x: int(
                x.split(f"{self.reviewer_prefix}_")[-1].split(
                    f'_{self.grid_params["base_currency"]}'
                )[0]
            ),
        )

        artifacts = [pd.read_parquet(file_path) for file_path in file_paths]
        index = pd.Index(
            [
                int(
                    file_path.split(f"{artifact_type}_{self.reviewer_prefix}_")[
                        -1
                    ].split(f"_{self.grid_params['base_currency']}.parquet.zstd")[0]
                )
                for file_path in file_paths
            ]
        )

        if with_index is True:
            return artifacts, index

        return artifacts

    def _build_metrics(self):
        artifacts, index = self._load_artifacts(
            artifact_type="metrics", with_index=True
        )
        metrics = pd.concat(artifacts)
        metrics.index = index

        return metrics

    def display_params(self, index, in_shell=False):
        display_markdown(f"#### Params: {index}", raw=True)

        params = (
            pd.Series(self._load_artifact(artifact_type="params", index=index))
            .rename("params")
            .to_frame()
        )

        if in_shell is True:
            print(tabulate(params, headers="keys", tablefmt="psql"))
        else:
            display(params)

    def display_report(self, index, in_shell=False):
        report = self._load_artifact(artifact_type="report", index=index)

        display_markdown(f"#### Report: {index}", raw=True)
        _, ax = plt.subplots(4, 1, figsize=(12, 12), sharex=True)

        for idx, column in enumerate(["capital", "cache", "return", "trade_return"]):
            if column == "trade_return":
                report[column].dropna().apply(lambda x: sum(x)).plot(ax=ax[idx])

            else:
                report[column].plot(ax=ax[idx])

            ax[idx].set_title(f"historical {column}")

        plt.tight_layout()

        if in_shell is True:
            plt.show(block=True)
        else:
            plt.show()

    def display_metrics(self, in_shell=False):
        metrics = self._build_metrics()

        if in_shell is True:
            print(tabulate(metrics, headers="keys", tablefmt="psql"))
        else:
            display(metrics)

    def display(self, in_shell=False):
        self.display_metrics(in_shell=in_shell)

        metrics = self._build_metrics()
        best_index = metrics["total_return"].sort_values(ascending=False).index[0]

        display_markdown(f"### [+] Best index: {best_index}", raw=True)

        display(metrics.loc[best_index])
        self.display_params(index=best_index, in_shell=in_shell)
        self.display_report(index=best_index, in_shell=in_shell)

    def run(self, in_shell=False, display_performance=False):
        if in_shell is False:
            if display_performance is True:
                self.display_performance()

        print(f"[+] Found backtests to start: {len(self.backtesters)}")

        Parallel(n_jobs=self.n_jobs, verbose=1)(
            [delayed(backtester.run)(display=False) for backtester in self.backtesters]
        )

        self.display(in_shell=in_shell)


if __name__ == "__main__":
    import fire

    fire.Fire(ReviewerV1)
</file>

<file path="reviewer/utils.py">
from itertools import product
from collections import OrderedDict


def grid(params):
    def _handle_value(value):
        return list(value) if type(value) == list or type(value) == tuple else [value]

    params = OrderedDict(
        [(key, _handle_value(value=params[key])) for key in sorted(list(params.keys()))]
    )

    keys = list(params.keys())
    for values in product(*list(params.values())):
        param = dict(zip(keys, values))
        yield param






import pandas as pd


def nan_to_zero(value):
    if str(value) in ("nan", "None"):
        return 0

    return value


def load_parquet(path):
    return pd.read_parquet(path)


class Position:
    def __init__(
        self,
        asset,
        side,
        qty,
        entry_price,
        prediction,
        entry_at,
        n_updated=0,
        is_exited=False,
    ):
        self.asset = asset
        self.side = side
        self.qty = qty
        self.entry_price = entry_price
        self.prediction = prediction
        self.entry_at = entry_at
        self.n_updated = n_updated
        self.is_exited = is_exited

    def __getitem__(self, key):
        return getattr(self, key)

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __repr__(self):
        return f"Position(asset={self.asset}, side={self.side}, qty={self.qty}, entry_price={self.entry_price:.4f}, n_updated={self.n_updated}, is_exited={str(self.is_exited)})"

    def __str__(self):
        return self.__repr__()
</file>

<file path="trainer/datasets/dataset.py">
from torch.utils.data import Dataset as _Dataset
from typing import Dict, List, Callable
import os
import numpy as np
import pandas as pd
from tqdm import tqdm
import gc


FILENAME_TEMPLATE = {
    "X": "X.parquet.zstd",
    "Y": "Y.parquet.zstd",
}


def build_X_and_BX(features, base_feature_assets):
    BX = features[base_feature_assets]
    return features, BX


class Dataset(_Dataset):
    def __init__(
        self,
        data_dir: str,
        transforms: Dict[str, Callable],
        base_feature_assets: List[str],
        asset_to_id: Dict[str, int],
        lookback_window: int = 120,
    ):
        print("[+] Start to build dataset")
        self.data_caches = {}

        # Build inputs
        self.data_caches["X"], self.data_caches["BX"] = build_X_and_BX(
            features=(
                pd.read_parquet(
                    os.path.join(data_dir, FILENAME_TEMPLATE["X"]), engine="pyarrow"
                ).astype("float32")
            ),
            base_feature_assets=base_feature_assets,
        )

        assert (self.data_caches["BX"].index == self.data_caches["X"].index).all()

        self.index = []
        for asset in tqdm(self.data_caches["X"].columns.levels[0]):
            self.index += [
                (index, asset)
                for index in self.data_caches["X"][[asset]]
                .dropna()
                .iloc[lookback_window - 1 :]
                .index
            ]

        self.index = pd.Index(self.index)
        gc.collect()

        # Build labels
        self.data_caches["Y"] = (
            pd.read_parquet(
                os.path.join(data_dir, FILENAME_TEMPLATE["Y"]), engine="pyarrow",
            )
            .sort_index()
            .stack()
            .reindex(self.index)
        ).astype("float32")

        self.transforms = transforms
        self.n_data = len(self.index)
        self.lookback_window = lookback_window
        self.asset_to_id = asset_to_id

        gc.collect()
        print("[+] built dataset")

    def __len__(self):
        return self.n_data

    def __getitem__(self, idx):
        # astype -> Y: int, else: float32
        data_dict = {}

        boundary_index = self.data_caches["X"][self.index[idx][1]].index.get_loc(
            self.index[idx][0]
        )

        # Concat with BX
        concat_df = pd.concat(
            [
                self.data_caches["BX"].iloc[
                    boundary_index - (self.lookback_window - 1) : boundary_index + 1
                ],
                self.data_caches["X"][self.index[idx][1]].iloc[
                    boundary_index - (self.lookback_window - 1) : boundary_index + 1
                ],
            ],
            axis=1,
        )

        data_dict["X"] = np.swapaxes(concat_df.values, 0, 1)

        data_dict["Y"] = self.data_caches["Y"].iloc[idx]

        data_dict["ID"] = self.asset_to_id[self.index[idx][1]]

        # transform
        for data_type, transform in self.transforms.items():
            data_dict[data_type] = transform(data_dict[data_type])

        del concat_df

        return data_dict
</file>

<file path="trainer/models/backbones/__init__.py">
from .backbone_v1 import BackboneV1
from .stack_backbone_v1 import StackBackboneV1
</file>

<file path="trainer/models/backbones/backbone_v1.py">
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

# from trainer.modules.block_1d import DenseBlock, TransitionBlock, NORMS
# from trainer.modules import acts



from develop.src.trainer.modules.block_1d import DenseBlock, TransitionBlock, NORMS
from develop.src.trainer.modules import acts




def identity(x):
    return x


class BackboneV1(nn.Module):
    def __init__(
        self,
        in_channels,
        n_assets,
        n_blocks=3,
        n_block_layers=6,
        growth_rate=12,
        dropout=0.0,
        channel_reduction=0.5,
        activation="relu",
        normalization="bn",
        seblock=True,
        sablock=True,
    ):
        super(BackboneV1, self).__init__()
        self.in_channels = in_channels
        self.n_assets = n_assets

        self.n_blocks = n_blocks
        self.n_block_layers = n_block_layers
        self.growth_rate = growth_rate
        self.dropout = dropout
        self.channel_reduction = channel_reduction

        self.activation = activation
        self.normalization = normalization
        self.seblock = seblock
        self.sablock = sablock

        # Build first_conv
        out_channels = 4 * growth_rate
        self.first_conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=3,
            stride=1,
            padding=1,
            bias=False,
        )

        # Build blocks
        in_channels = out_channels

        blocks = []
        for idx in range(n_blocks):

            blocks.append(
                self._build_block(
                    in_channels=in_channels,
                    use_transition_block=True if idx != n_blocks - 1 else False,
                )
            )

            # mutate in_channels for next block
            in_channels = self._compute_out_channels(
                in_channels=in_channels,
                use_transition_block=True if idx != n_blocks - 1 else False,
            )

        self.blocks = nn.Sequential(*blocks)

        # Last layers
        self.norm = identity
        if normalization is not None:
            self.norm = NORMS[normalization.upper()](num_channels=in_channels)

        self.act = getattr(acts, activation)
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)

        self.embed = nn.Embedding(n_assets, in_channels)
        self.pred_fc = nn.Linear(in_channels, 2)
        self.last_sigmoid = nn.Sigmoid()

        # Initialize
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                n = m.kernel_size[0] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(1.0 / n))

            elif isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)

                if m.bias is not None:
                    m.bias.data.zero_()

            elif isinstance(m, nn.Linear):
                if m.bias is not None:
                    m.bias.data.zero_()

    def _compute_out_channels(self, in_channels, use_transition_block=True):
        if use_transition_block is True:
            return int(
                math.floor(
                    (in_channels + (self.n_block_layers * self.growth_rate))
                    * self.channel_reduction
                )
            )

        return in_channels + (self.n_block_layers * self.growth_rate)

    def _build_block(self, in_channels, use_transition_block=True):
        assert use_transition_block in (False, True)

        dense_block = DenseBlock(
            n_layers=self.n_block_layers,
            in_channels=in_channels,
            growth_rate=self.growth_rate,
            dropout=self.dropout,
            activation=self.activation,
            normalization=self.normalization,
            seblock=self.seblock if use_transition_block is True else False,
            sablock=self.sablock if use_transition_block is True else False,
        )

        if use_transition_block is True:
            in_channels = int(in_channels + (self.n_block_layers * self.growth_rate))
            out_channels = int(math.floor(in_channels * self.channel_reduction))
            transition_block = TransitionBlock(
                in_channels=in_channels,
                out_channels=out_channels,
                dropout=self.dropout,
                activation=self.activation,
                normalization=self.normalization,
            )

            return nn.Sequential(*[dense_block, transition_block])

        return dense_block

    def forward(self, x, id):
        B, _, _ = x.size()
        out = self.blocks(self.first_conv(x))
        out = self.global_avg_pool(self.act(self.norm(out))).view(B, -1)

        preds = self.pred_fc(out) + (out * self.embed(id)).sum(axis=-1, keepdim=True)

        return preds[:, 0], self.last_sigmoid(preds[:, 1])
</file>

<file path="trainer/models/backbones/stack_backbone_v1.py">
from .backbone_v1 import BackboneV1, identity
import math
import torch.nn as nn
from develop.src.trainer.modules.block_1d import NORMS
from develop.src.trainer.modules import acts


class StackBackboneV1(BackboneV1):
    def __init__(
        self,
        in_channels,
        n_assets,
        n_blocks=3,
        n_block_layers=6,
        growth_rate=12,
        dropout=0.0,
        channel_reduction=0.5,
        activation="relu",
        normalization="bn",
        seblock=True,
        sablock=True,
    ):
        super(BackboneV1, self).__init__()
        self.in_channels = in_channels
        self.n_assets = n_assets

        self.n_blocks = n_blocks
        self.n_block_layers = n_block_layers
        self.growth_rate = growth_rate
        self.dropout = dropout
        self.channel_reduction = channel_reduction

        self.activation = activation
        self.normalization = normalization
        self.seblock = seblock
        self.sablock = sablock

        # Build first_conv
        out_channels = 4 * growth_rate
        self.first_conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=3,
            stride=1,
            padding=1,
            bias=False,
        )

        # Build blocks
        in_channels = out_channels

        blocks = []
        for idx in range(n_blocks):

            blocks.append(
                self._build_block(
                    in_channels=in_channels,
                    use_transition_block=True if idx != n_blocks - 1 else False,
                )
            )

            # mutate in_channels for next block
            in_channels = self._compute_out_channels(
                in_channels=in_channels,
                use_transition_block=True if idx != n_blocks - 1 else False,
            )

        self.blocks = nn.Sequential(*blocks)

        # Last layers
        self.norm = identity
        if normalization is not None:
            self.norm = NORMS[normalization.upper()](num_channels=in_channels)

        self.act = getattr(acts, activation)
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)

        self.embed = nn.Embedding(n_assets, in_channels)
        self.pred_fc = nn.Linear(in_channels, 4)
        self.last_sigmoid = nn.Sigmoid()

        # Initialize
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                n = m.kernel_size[0] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(1.0 / n))

            elif isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)

                if m.bias is not None:
                    m.bias.data.zero_()

            elif isinstance(m, nn.Linear):
                if m.bias is not None:
                    m.bias.data.zero_()

    def forward(self, x, id):
        B, _, _ = x.size()
        out = self.blocks(self.first_conv(x))
        out = self.global_avg_pool(self.act(self.norm(out))).view(B, -1)

        preds = self.pred_fc(out) + (out * self.embed(id)).sum(axis=-1, keepdim=True)

        return (
            preds[:, 0],
            self.last_sigmoid(preds[:, 1]),
            preds[:, 2],
            self.last_sigmoid(preds[:, 3]),
        )
</file>

<file path="trainer/models/__init__.py">
from .predictor_v1 import PredictorV1
</file>

<file path="trainer/models/basic_predictor.py">
import os
import shutil
import fire
import json
import joblib
from tqdm import tqdm
import pandas as pd
from copy import copy
from contextlib import contextmanager
from abc import abstractmethod

import torch
import torch.nn as nn
from develop.src.common_utils_dev import load_text, load_json, to_abs_path, get_parent_dir
from .utils import save_model, load_model, weights_init
from .criterions import CRITERIONS
from ..datasets.dataset import Dataset
from torch.utils.data import DataLoader
# from trainer.models import backbones
from develop.src.trainer.models import backbones

COMMON_CONFIG = {
    "data_dir": to_abs_path(__file__, "../../../storage/dataset/dataset/v001/train"),
    "exp_dir": to_abs_path(__file__, "../../../storage/experiments/v001"),
    "test_data_dir": to_abs_path(
        __file__, "../../../storage/dataset/dataset/v001/test"
    ),
}


DATA_CONFIG = {
    "checkpoint_dir": "./check_point",
    "generate_output_dir": "./generated_output",
    "base_feature_assets": ["BTC-USDT"],
}

MODEL_CONFIG = {
    "lookback_window": 120,
    "batch_size": 512,
    "lr": 0.0001,
    "epochs": 15,
    "print_epoch": 1,
    "print_iter": 50,
    "save_epoch": 1,
    "criterion": "l2",
    "criterion_params": {},
    "load_strict": False,
    "model_name": "BackboneV1",
    "model_params": {
        "in_channels": 84,
        "n_blocks": 5,
        "n_block_layers": 8,
        "growth_rate": 12,
        "dropout": 0.1,
        "channel_reduction": 0.5,
        "activation": "selu",
        "normalization": None,
        "seblock": True,
        "sablock": True,
    },
}


def _mutate_config_path(data_config, exp_dir):
    for key in ["checkpoint_dir", "generate_output_dir"]:
        if data_config[key][0] != "/":
            data_config[key] = os.path.join(exp_dir, data_config[key])

        if "_dir" in key:
            os.makedirs(data_config[key], exist_ok=True)

    return data_config


class BasicPredictor:
    def __init__(
        self,
        data_dir=COMMON_CONFIG["data_dir"],
        test_data_dir=COMMON_CONFIG["test_data_dir"],
        d_config={},
        m_config={},
        exp_dir=COMMON_CONFIG["exp_dir"],
        device=None,
        pin_memory=False,
        num_workers=8,
        mode="train",
        default_d_config=DATA_CONFIG,
        default_m_config=MODEL_CONFIG,
    ):
        #   
        if device is None:
            if torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
        self.device = device

        # MPS   num_workers 
        if self.device == "mps":
            self.num_workers = 0  # macOS MPS   num_workers=0 


        assert mode in ("train", "test", "predict")
        self.data_dir = data_dir
        self.test_data_dir = test_data_dir
        self.exp_dir = exp_dir
        self.device = device
        self.pin_memory = pin_memory
        self.num_workers = num_workers
        self.mode = mode

        # Build params & configs
        self._load_dataset_params(mode=mode)
        self.data_config, self.model_config = self._build_config(
            d_config=d_config,
            m_config=m_config,
            default_d_config=default_d_config,
            default_m_config=default_m_config,
        )
        self.asset_to_id = self._build_asset_to_id()

        self.model = self._build_model()

        self.iterable_train_data_loader = None
        self.iterable_test_data_loader = None

        if mode == "train":
            self.train_data_loader, self.test_data_loader = self._build_data_loaders(
                mode=mode
            )
            self.optimizer = self._build_optimizer()
            self.criterion = self._build_criterion()
            self.binary_cross_entropy = CRITERIONS["bce"]().to(self.device)

            # Store params
            self._copy_dataset_artifacts()
            self._store_params()

        if mode == "test":
            _, self.test_data_loader = self._build_data_loaders(mode=mode)

        if mode in ("test", "predict"):
            self._load_label_scaler()

    def _copy_dataset_artifacts(self):
        # Copy files from dataset
        for base_file, target_file in [
            (
                os.path.join(get_parent_dir(self.data_dir), "dataset_params.json"),
                os.path.join(self.exp_dir, "dataset_params.json"),
            ),
            (
                os.path.join(get_parent_dir(self.data_dir), "feature_scaler.pkl"),
                os.path.join(self.exp_dir, "feature_scaler.pkl"),
            ),
            (
                os.path.join(get_parent_dir(self.data_dir), "label_scaler.pkl"),
                os.path.join(self.exp_dir, "label_scaler.pkl"),
            ),
        ]:
            shutil.copy(base_file, target_file)

    def _store_params(self):
        params = {
            "data_dir": self.data_dir,
            "test_data_dir": self.test_data_dir,
            "model_config": self.model_config,
            "data_config": self.data_config,
            "asset_to_id": self.asset_to_id,
        }
        with open(os.path.join(self.exp_dir, f"trainer_params.json"), "w") as f:
            json.dump(params, f)

        print(f"[+] Params are stored")

    def _load_label_scaler(self):
        self.label_scaler = joblib.load(os.path.join(self.exp_dir, "label_scaler.pkl"))

    def _load_dataset_params(self, mode):
        if mode == "train":
            self.dataset_params = load_json(
                os.path.join(get_parent_dir(self.data_dir), "dataset_params.json")
            )
            return

        self.dataset_params = load_json(
            os.path.join(self.exp_dir, "dataset_params.json")
        )

    def _build_config(self, d_config, m_config, default_d_config, default_m_config):
        # refine path with exp_dirs
        data_config = copy(default_d_config)
        model_config = copy(default_m_config)
        if not set(m_config.keys()).issubset(set(model_config.keys())):
            raise ValueError(f"{set(m_config.keys()) - set(model_config.keys())}")

        if not set(d_config.keys()).issubset(set(data_config.keys())):
            raise ValueError(f"{set(d_config.keys()) - set(data_config.keys())}")

        data_config = {**data_config, **d_config}

        model_params = {
            **model_config.pop("model_params", {}),
            **m_config.pop("model_params", {}),
        }
        model_config = {**model_config, **m_config, **{"model_params": model_params}}

        data_config = _mutate_config_path(data_config=data_config, exp_dir=self.exp_dir)

        # Mutate model_params' n_assets
        if "n_assets" not in model_config["model_params"]:
            n_assets = len(
                [
                    tradable_coin
                    for tradable_coin in self.dataset_params["tradable_coins"]
                ]
            )

            model_config["model_params"]["n_assets"] = n_assets

        return data_config, model_config

    def _build_asset_to_id(self):
        tradable_coins = [
            tradable_coin for tradable_coin in self.dataset_params["tradable_coins"]
        ]
        asset_to_id = {
            tradable_coin: idx for idx, tradable_coin in enumerate(tradable_coins)
        }
        return asset_to_id

    def _build_transfroms(self):
        return {}

    def _build_data_loaders(self, mode):
        assert mode in ("train", "test")
        transforms = self._build_transfroms()

        # Build base params
        base_dataset_params = {
            "transforms": transforms,
            "lookback_window": self.model_config["lookback_window"],
            "base_feature_assets": self.data_config["base_feature_assets"],
            "asset_to_id": self.asset_to_id,
        }

        #   (MPS  pin_memory=False  )
        base_data_loader_params = {
            "batch_size": self.model_config["batch_size"],
            "pin_memory": self.pin_memory if self.device != "mps" else False,
            "num_workers": self.num_workers,
        }

        # Build dataset & data_loader
        test_dataset = Dataset(data_dir=self.test_data_dir, **base_dataset_params)

        train_data_loader = None
        if mode == "train":
            # Define: dataset
            train_dataset = Dataset(data_dir=self.data_dir, **base_dataset_params)

            # Define data_loader
            train_data_loader = DataLoader(
                dataset=train_dataset, shuffle=True, **base_data_loader_params
            )

            test_data_loader = DataLoader(
                dataset=test_dataset, shuffle=True, **base_data_loader_params
            )

        if mode == "test":
            test_data_loader = DataLoader(
                dataset=test_dataset, shuffle=False, **base_data_loader_params
            )

        return train_data_loader, test_data_loader

    def _load_model(self, model):
        # load model (inplace)
        self.last_epoch = load_model(
            model=model,
            dir=self.data_config["checkpoint_dir"],
            strict=self.model_config["load_strict"],
            device=self.device,
        )
        if self.mode in ("test", "predict"):
            assert self.last_epoch != -1

    def _save_model(self, model, epoch):
        save_model(model=model, dir=self.data_config["checkpoint_dir"], epoch=epoch)

    def _build_model(self):
        # Define  model
        model = getattr(backbones, self.model_config["model_name"])(
            **self.model_config["model_params"]
        )

        # Init model's weights
        model.apply(weights_init)

        # Setup device
        # if torch.cuda.device_count() > 1:
        #     print("Notice: use ", torch.cuda.device_count(), "GPUs")
        #     model = nn.DataParallel(model)
        # else:
        #     print(f"Notice: use {self.device}")
        if self.device.startswith("cuda") and torch.cuda.device_count() > 1:
            print(f"Notice: using {torch.cuda.device_count()} GPUs")
            model = nn.DataParallel(model)
        else:
            print(f"Notice: using {self.device}")


        # Load models
        self._load_model(model=model)
        model.to(self.device)

        return model

    def _build_optimizer(self):
        # set optimizer
        optimizer = torch.optim.AdamW(
            params=self.model.parameters(), lr=self.model_config["lr"]
        )

        return optimizer

    def _build_criterion(self):
        return CRITERIONS[self.model_config["criterion"]](
            **self.model_config["criterion_params"]
        ).to(self.device)

    def _generate_train_data_dict(self):
        if self.iterable_train_data_loader is None:
            self.iterable_train_data_loader = iter(self.train_data_loader)

        # Pick data
        try:
            train_data_dict = next(self.iterable_train_data_loader)
        except StopIteration:
            self.iterable_train_data_loader = iter(self.train_data_loader)
            train_data_dict = next(self.iterable_train_data_loader)

        train_data_dict = {
            key: value.to(self.device) for key, value in train_data_dict.items()
        }
        return train_data_dict

    def _generate_test_data_dict(self):
        if self.iterable_test_data_loader is None:
            self.iterable_test_data_loader = iter(self.test_data_loader)

        # Pick data
        try:
            test_data_dict = next(self.iterable_test_data_loader)
        except StopIteration:
            self.iterable_test_data_loader = iter(self.test_data_loader)
            test_data_dict = next(self.iterable_test_data_loader)

        test_data_dict = {
            key: value.to(self.device) for key, value in test_data_dict.items()
        }
        return test_data_dict

    @abstractmethod
    def _step(self, train_data_dict):
        pass

    @abstractmethod
    def train(self):
        """
        Train model
        """
        pass

    @abstractmethod
    def generate(self, save_dir=None):
        """
        Generate historical predictions csv
        """

    @abstractmethod
    def predict(self, X):
        """
        Predict
        """
</file>

<file path="trainer/models/criterions.py">
import torch.nn as nn


CRITERIONS = {"l1": nn.L1Loss, "l2": nn.MSELoss, "bce": nn.BCELoss}
</file>

<file path="trainer/models/predictor_v1.py">
import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Union, Optional, List, Dict
from tqdm import tqdm
from .basic_predictor import BasicPredictor
from .utils import inverse_preprocess_data
from develop.src.common_utils_dev import to_parquet, to_abs_path

COMMON_CONFIG = {
    "data_dir": to_abs_path(__file__, "../../../storage/dataset/dataset/v001/train"),
    "exp_dir": to_abs_path(__file__, "../../../storage/experiments/v001"),
    "test_data_dir": to_abs_path(
        __file__, "../../../storage/dataset/dataset/v001/test"
    ),
}

DATA_CONFIG = {
    "checkpoint_dir": "./check_point",
    "generate_output_dir": "./generated_output",
    "base_feature_assets": ["BTC-USDT"],
}

MODEL_CONFIG = {
    "lookback_window": 120,
    "batch_size": 512,
    "lr": 0.0001,
    # "epochs": 15,
    "epochs": 30,
    "print_epoch": 1,
    "print_iter": 50,
    "save_epoch": 1,
    "criterion": "l2",
    "criterion_params": {},
    "load_strict": False,
    "model_name": "BackboneV1",
    "model_params": {
        "in_channels": 84,
        "n_blocks": 5,
        "n_block_layers": 8,
        "growth_rate": 12,
        "dropout": 0.1,
        "channel_reduction": 0.5,
        "activation": "selu",
        "normalization": None,
        "seblock": True,
        "sablock": True,
    },
}


class PredictorV1(BasicPredictor):
    """
    Functions:
        train(): train the model with train_data
        generate(save_dir: str): generate predictions & labels with test_data
        predict(X: torch.Tensor): gemerate prediction with given data
    """

    def __init__(
        self,
        data_dir=COMMON_CONFIG["data_dir"],
        test_data_dir=COMMON_CONFIG["test_data_dir"],
        d_config={},
        m_config={},
        exp_dir=COMMON_CONFIG["exp_dir"],
        device=None,
        pin_memory=False,
        num_workers=8,
        mode="train",
        default_d_config=DATA_CONFIG,
        default_m_config=MODEL_CONFIG,
    ):
        #   
        if device is None:
            if torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
        self.device = device

        # MPS   num_workers 
        if self.device == "mps":
            self.num_workers = 0  # macOS MPS   num_workers=0 

        


        super().__init__(
            data_dir=data_dir,
            test_data_dir=test_data_dir,
            d_config=d_config,
            m_config=m_config,
            exp_dir=exp_dir,
            device=device,
            pin_memory=pin_memory,
            num_workers=num_workers,
            mode=mode,
            default_d_config=default_d_config,
            default_m_config=default_m_config,
        )

    def _invert_to_prediction(self, pred_abs_factor, pred_sign_factor):
        multiply = ((pred_sign_factor >= 0.5) * 1.0) + ((pred_sign_factor < 0.5) * -1.0)
        return pred_abs_factor * multiply

    def _compute_train_loss(self, train_data_dict):
        # Set train mode
        self.model.train()
        self.model.zero_grad()

        # Set loss
        pred_abs_factor, pred_sign_factor = self.model(
            x=train_data_dict["X"], id=train_data_dict["ID"]
        )

        # Y loss
        loss = self.criterion(pred_abs_factor, train_data_dict["Y"].view(-1).abs()) * 10
        loss += self.binary_cross_entropy(
            pred_sign_factor, (train_data_dict["Y"].view(-1) >= 0) * 1.0
        )

        return (
            loss,
            self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            ),
        )

    def _compute_test_loss(self, test_data_dict):
        # Set eval mode
        self.model.eval()

        # Set loss
        pred_abs_factor, pred_sign_factor = self.model(
            x=test_data_dict["X"], id=test_data_dict["ID"]
        )

        # Y loss
        loss = self.criterion(pred_abs_factor, test_data_dict["Y"].view(-1).abs()) * 10
        loss += self.binary_cross_entropy(
            pred_sign_factor, (test_data_dict["Y"].view(-1) >= 0) * 1.0
        )

        return (
            loss,
            self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            ),
        )

    def _step(self, train_data_dict):
        loss, _ = self._compute_train_loss(train_data_dict=train_data_dict)
        loss.backward()
        self.optimizer.step()

        return loss

    def _display_info(self, train_loss, test_loss, test_predictions, test_labels):
        pred_norm = test_predictions[test_predictions >= 0].abs().mean()
        label_norm = test_labels[test_labels >= 0].abs().mean()

        # Print loss info
        print(
            f""" [+] train_loss: {train_loss:.2f}, test_loss: {test_loss:.2f} | [+] pred_norm: {pred_norm:.2f}, label_norm: {label_norm:.2f}"""
        )

    def _build_abs_bins(self, df):
        abs_bins = {}
        for column in df.columns:
            _, abs_bins[column] = pd.qcut(
                df[column].abs(), 10, labels=False, retbins=True
            )
            abs_bins[column] = np.concatenate([[0], abs_bins[column][1:-1], [np.inf]])

        return pd.DataFrame(abs_bins)

    def _build_probabilities(self, pred_sign_factor):
        return ((pred_sign_factor - 0.5) * 2).abs()

    def train(self):
        for epoch in range(self.model_config["epochs"]):
            if epoch <= self.last_epoch:
                continue

            for iter_ in tqdm(range(len(self.train_data_loader))):
                # Optimize
                train_data_dict = self._generate_train_data_dict()
                train_loss = self._step(train_data_dict=train_data_dict)

                # Display losses
                if epoch % self.model_config["print_epoch"] == 0:
                    if iter_ % self.model_config["print_iter"] == 0:
                        test_data_dict = self._generate_test_data_dict()
                        test_loss, test_predictions = self._compute_test_loss(
                            test_data_dict=test_data_dict
                        )
                        self._display_info(
                            train_loss=train_loss,
                            test_loss=test_loss,
                            test_predictions=test_predictions,
                            test_labels=test_data_dict["Y"],
                        )

            # Store the check-point
            if (epoch % self.model_config["save_epoch"] == 0) or (
                epoch == self.model_config["epochs"] - 1
            ):
                self._save_model(model=self.model, epoch=epoch)

    def generate(self, save_dir=None):
        assert self.mode in ("test")
        self.model.eval()

        if save_dir is None:
            save_dir = self.data_config["generate_output_dir"]

        import pdb
        # pdb.set_trace()

        # Mutate 1 min to handle logic, entry: open, exit: open
        index = self.test_data_loader.dataset.index
        index = index.set_levels(index.levels[0] + pd.Timedelta(minutes=1), level=0)

        predictions = []
        labels = []
        probabilities = []
        for idx in tqdm(range(len(self.test_data_loader))):
            test_data_dict = self._generate_test_data_dict()

            pred_abs_factor, pred_sign_factor = self.model(
                x=test_data_dict["X"], id=test_data_dict["ID"]
            )
            preds = self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            )

            predictions += preds.view(-1).cpu().tolist()
            labels += test_data_dict["Y"].view(-1).cpu().tolist()
            probabilities += (
                self._build_probabilities(pred_sign_factor=pred_sign_factor)
                .view(-1)
                .cpu()
                .tolist()
            )

        predictions = (
            pd.Series(predictions, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )
        labels = (
            pd.Series(labels, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )
        probabilities = (
            pd.Series(probabilities, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )

        # Rescale
        predictions = inverse_preprocess_data(
            data=predictions * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        )
        labels = inverse_preprocess_data(
            data=labels * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        )

        prediction_abs_bins = self._build_abs_bins(df=predictions)
        probability_bins = self._build_abs_bins(df=probabilities)

        # Store signals
        for data_type, data in [
            ("predictions", predictions),
            ("labels", labels),
            ("probabilities", probabilities),
            ("prediction_abs_bins", prediction_abs_bins),
            ("probability_bins", probability_bins),
        ]:
            to_parquet(
                df=data, path=os.path.join(save_dir, f"{data_type}.parquet.zstd"),
            )

    def predict(
        self,
        X: Union[np.ndarray, torch.Tensor],
        id: Union[List, torch.Tensor],
        id_to_asset: Optional[Dict] = None,
    ):
        assert self.mode in ("predict")
        self.model.eval()

        if not isinstance(X, torch.Tensor):
            X = torch.Tensor(X)
        if not isinstance(id, torch.Tensor):
            id = torch.Tensor(id)

        pred_abs_factor, pred_sign_factor = self.model(
            x=X.to(self.device), id=id.to(self.device).long()
        )
        preds = self._invert_to_prediction(
            pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
        )
        predictions = pd.Series(preds.view(-1).cpu().tolist(), index=id.int().tolist(),)
        probabilities = pd.Series(
            self._build_probabilities(pred_sign_factor=pred_sign_factor)
            .view(-1)
            .cpu()
            .tolist(),
            index=id.int().tolist(),
        )

        # Post-process
        assert id_to_asset is not None
        predictions.index = predictions.index.map(lambda x: id_to_asset[x])
        probabilities.index = probabilities.index.map(lambda x: id_to_asset[x])

        # Rescale
        labels_columns = self.dataset_params["labels_columns"]
        labels_columns = [
            labels_column.replace("-", "/") for labels_column in labels_columns
        ]

        predictions = predictions.rename("predictions").to_frame().T[labels_columns]
        predictions = inverse_preprocess_data(
            data=predictions * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        ).loc["predictions"]

        probabilities = probabilities.rename("probabilities")[labels_columns]

        return {"predictions": predictions, "probabilities": probabilities}


if __name__ == "__main__":
    import fire

    fire.Fire(PredictorV1)
</file>

<file path="trainer/models/stack_predictor_v1.py">
import os
import shutil
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Union, Optional, List, Dict
from tqdm import tqdm
from .basic_predictor import BasicPredictor
from .utils import inverse_preprocess_data
from common_utils_dev import to_parquet, to_abs_path, load_json
from trainer.models.predictor_v1 import PredictorV1

COMMON_CONFIG = {
    "data_dir": to_abs_path(__file__, "../../../storage/dataset/dataset/v001/train"),
    "exp_dir": to_abs_path(__file__, "../../../storage/experiments/v001_stack"),
    "test_data_dir": to_abs_path(
        __file__, "../../../storage/dataset/dataset/v001/test"
    ),
    "feature_extractor_dir": to_abs_path(__file__, "../../../storage/experiments/v001"),
}

DATA_CONFIG = {
    "checkpoint_dir": "./check_point",
    "generate_output_dir": "./generated_output",
    "base_feature_assets": ["BTC-USDT"],
}

MODEL_CONFIG = {
    "local_lookback_window": 30,
    "lookback_window": 149,
    "batch_size": 512,
    "lr": 0.0001,
    "epochs": 15,
    "print_epoch": 1,
    "print_iter": 50,
    "save_epoch": 1,
    "criterion": "l2",
    "criterion_params": {},
    "load_strict": False,
    "model_name": "StackBackboneV1",
    "model_params": {
        "in_channels": 86,
        "n_blocks": 4,
        "n_block_layers": 8,
        "growth_rate": 12,
        "dropout": 0.1,
        "channel_reduction": 0.5,
        "activation": "selu",
        "normalization": None,
        "seblock": True,
        "sablock": True,
    },
}


class StackPredictorV1(BasicPredictor):
    """
    Functions:
        train(): train the model with train_data
        generate(save_dir: str): generate predictions & labels with test_data
        predict(X: torch.Tensor): gemerate prediction with given data
    """

    def __init__(
        self,
        data_dir=COMMON_CONFIG["data_dir"],
        test_data_dir=COMMON_CONFIG["test_data_dir"],
        d_config={},
        m_config={},
        feature_extractor_dir=COMMON_CONFIG["exp_dir"],
        exp_dir=COMMON_CONFIG["exp_dir"],
        device="cuda",
        pin_memory=False,
        num_workers=8,
        mode="train",
        default_d_config=DATA_CONFIG,
        default_m_config=MODEL_CONFIG,
    ):
        super().__init__(
            data_dir=data_dir,
            test_data_dir=test_data_dir,
            d_config=d_config,
            m_config=m_config,
            exp_dir=exp_dir,
            device=device,
            pin_memory=pin_memory,
            num_workers=num_workers,
            mode=mode,
            default_d_config=default_d_config,
            default_m_config=default_m_config,
        )

        self._build_feature_extractor_params(
            feature_extractor_dir=feature_extractor_dir
        )
        self._build_feature_extractor(feature_extractor_dir=feature_extractor_dir)
        assert (
            self.feature_extractor_params["model_config"]["lookback_window"]
            + self.model_config["local_lookback_window"]
            - 1
            == self.model_config["lookback_window"]
        )

    def _build_feature_extractor_params(self, feature_extractor_dir):
        self.feature_extractor_params = load_json(
            os.path.join(feature_extractor_dir, "trainer_params.json")
        )

    def _build_feature_extractor(self, feature_extractor_dir):
        self.feature_extractor = PredictorV1(
            exp_dir=feature_extractor_dir,
            m_config=self.feature_extractor_params["model_config"],
            d_config=self.feature_extractor_params["data_config"],
            device=self.device,
            mode="predict",
        )

    def _invert_to_sign(self, pred_sign_factor, boundary=0.5):
        return ((pred_sign_factor >= boundary) * 1.0) + (
            (pred_sign_factor < boundary) * -1.0
        )

    def _invert_to_prediction(self, pred_abs_factor, pred_sign_factor):
        multiply = self._invert_to_sign(pred_sign_factor=pred_sign_factor)
        return pred_abs_factor * multiply

    def _extract_features(self, data_dict):
        with torch.no_grad():
            abs_factor_features, sign_factor_features = self.feature_extractor.model(
                x=torch.cat(
                    [
                        data_dict["X"][
                            :,
                            :,
                            idx : self.feature_extractor_params["model_config"][
                                "lookback_window"
                            ]
                            + idx,
                        ]
                        for idx in range(self.model_config["local_lookback_window"])
                    ],
                    dim=0,
                ),
                id=torch.cat(
                    [
                        data_dict["ID"]
                        for _ in range(self.model_config["local_lookback_window"])
                    ],
                    dim=0,
                ),
            )

            abs_factor_features = torch.stack(
                abs_factor_features.squeeze().split(data_dict["ID"].size()[0], dim=0),
                dim=-1,
            )
            sign_factor_features = torch.stack(
                sign_factor_features.squeeze().split(data_dict["ID"].size()[0], dim=0),
                dim=-1,
            )
            return (
                abs_factor_features,
                sign_factor_features,
            )

    def _build_stacked_features(self, data_dict):
        abs_factor_features, sign_factor_features = self._extract_features(
            data_dict=data_dict
        )

        return torch.cat(
            [
                data_dict["X"][:, :, -self.model_config["local_lookback_window"] :],
                abs_factor_features.unsqueeze(dim=1),
                sign_factor_features.unsqueeze(dim=1),
            ],
            dim=1,
        )

    def _compute_train_loss(self, train_data_dict):
        # Set train mode
        self.model.train()
        self.model.zero_grad()

        # Set loss
        x = self._build_stacked_features(data_dict=train_data_dict)

        (
            pred_abs_factor,
            pred_sign_factor,
            pred_abs_error_factor,
            pred_sign_error_factor,
        ) = self.model(x=x, id=train_data_dict["ID"],)

        # Y loss
        loss = self.criterion(pred_abs_factor, train_data_dict["Y"].view(-1).abs()) * 10
        loss += self.binary_cross_entropy(
            pred_sign_factor, (train_data_dict["Y"].view(-1) >= 0) * 1.0
        )

        loss += (
            self.criterion(
                pred_abs_error_factor,
                (
                    train_data_dict["Y"].view(-1).abs() - x[:, -2, -1].view(-1).abs()
                ).abs(),
            )
            * 2
        )
        loss += (
            self.binary_cross_entropy(
                pred_sign_error_factor,
                (
                    (
                        self._invert_to_sign(
                            train_data_dict["Y"].view(-1), boundary=0.0
                        )
                        * self._invert_to_sign(x[:, -1, -1].view(-1), boundary=0.5)
                    )
                    >= 0
                )
                * 1.0,
            )
            * 0.2
        )

        return (
            loss,
            self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            ),
        )

    def _compute_test_loss(self, test_data_dict):
        # Set eval mode
        self.model.eval()

        # Set loss
        x = self._build_stacked_features(data_dict=test_data_dict)

        (
            pred_abs_factor,
            pred_sign_factor,
            pred_abs_error_factor,
            pred_sign_error_factor,
        ) = self.model(x=x, id=test_data_dict["ID"])

        # Y loss
        loss = self.criterion(pred_abs_factor, test_data_dict["Y"].view(-1).abs()) * 10
        loss += self.binary_cross_entropy(
            pred_sign_factor, (test_data_dict["Y"].view(-1) >= 0) * 1.0
        )

        loss += (
            self.criterion(
                pred_abs_error_factor,
                (
                    test_data_dict["Y"].view(-1).abs() - x[:, -2, -1].view(-1).abs()
                ).abs(),
            )
            * 2
        )
        loss += (
            self.binary_cross_entropy(
                pred_sign_error_factor,
                (
                    (
                        self._invert_to_sign(test_data_dict["Y"].view(-1), boundary=0.0)
                        * self._invert_to_sign(x[:, -1, -1].view(-1), boundary=0.5)
                    )
                    >= 0
                )
                * 1.0,
            )
            * 0.2
        )

        return (
            loss,
            self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            ),
        )

    def _step(self, train_data_dict):
        loss, _ = self._compute_train_loss(train_data_dict=train_data_dict)
        loss.backward()
        self.optimizer.step()

        return loss

    def _display_info(self, train_loss, test_loss, test_predictions, test_labels):
        pred_norm = test_predictions[test_predictions >= 0].abs().mean()
        label_norm = test_labels[test_labels >= 0].abs().mean()

        # Print loss info
        print(
            f""" [+] train_loss: {train_loss:.2f}, test_loss: {test_loss:.2f} | [+] pred_norm: {pred_norm:.2f}, label_norm: {label_norm:.2f}"""
        )

    def _build_abs_bins(self, df):
        abs_bins = {}
        for column in df.columns:
            _, abs_bins[column] = pd.qcut(
                df[column].abs(), 10, labels=False, retbins=True
            )
            abs_bins[column] = np.concatenate([[0], abs_bins[column][1:-1], [np.inf]])

        return pd.DataFrame(abs_bins)

    def _build_probabilities(self, pred_sign_factor):
        return ((pred_sign_factor - 0.5) * 2).abs()

    def train(self):
        for epoch in range(self.model_config["epochs"]):
            if epoch <= self.last_epoch:
                continue

            for iter_ in tqdm(range(len(self.train_data_loader))):
                # Optimize
                train_data_dict = self._generate_train_data_dict()
                train_loss = self._step(train_data_dict=train_data_dict)

                # Display losses
                if epoch % self.model_config["print_epoch"] == 0:
                    if iter_ % self.model_config["print_iter"] == 0:
                        test_data_dict = self._generate_test_data_dict()
                        test_loss, test_predictions = self._compute_test_loss(
                            test_data_dict=test_data_dict
                        )
                        self._display_info(
                            train_loss=train_loss,
                            test_loss=test_loss,
                            test_predictions=test_predictions,
                            test_labels=test_data_dict["Y"],
                        )

            # Store the check-point
            if (epoch % self.model_config["save_epoch"] == 0) or (
                epoch == self.model_config["epochs"] - 1
            ):
                self._save_model(model=self.model, epoch=epoch)

    def generate(self, save_dir=None):
        assert self.mode in ("test")
        self.model.eval()

        if save_dir is None:
            save_dir = self.data_config["generate_output_dir"]

        # Mutate 1 min to handle logic, entry: open, exit: open
        index = self.test_data_loader.dataset.index
        index = index.set_levels(index.levels[0] + pd.Timedelta(minutes=1), level=0)

        predictions = []
        labels = []
        probabilities = []
        for idx in tqdm(range(len(self.test_data_loader))):
            test_data_dict = self._generate_test_data_dict()

            x = self._build_stacked_features(data_dict=test_data_dict)

            (pred_abs_factor, pred_sign_factor, _, _,) = self.model(
                x=x, id=test_data_dict["ID"]
            )

            preds = self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            )

            predictions += preds.view(-1).cpu().tolist()
            labels += test_data_dict["Y"].view(-1).cpu().tolist()
            probabilities += (
                self._build_probabilities(pred_sign_factor=pred_sign_factor)
                .view(-1)
                .cpu()
                .tolist()
            )

        predictions = (
            pd.Series(predictions, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )
        labels = (
            pd.Series(labels, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )
        probabilities = (
            pd.Series(probabilities, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )

        # Rescale
        predictions = inverse_preprocess_data(
            data=predictions * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        )
        labels = inverse_preprocess_data(
            data=labels * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        )

        prediction_abs_bins = self._build_abs_bins(df=predictions)
        probability_bins = self._build_abs_bins(df=probabilities)

        # Store signals
        for data_type, data in [
            ("predictions", predictions),
            ("labels", labels),
            ("probabilities", probabilities),
            ("prediction_abs_bins", prediction_abs_bins),
            ("probability_bins", probability_bins),
        ]:
            to_parquet(
                df=data, path=os.path.join(save_dir, f"{data_type}.parquet.zstd"),
            )

    def predict(
        self,
        X: Union[np.ndarray, torch.Tensor],
        id: Union[List, torch.Tensor],
        id_to_asset: Optional[Dict] = None,
    ):
        assert self.mode in ("predict")
        self.model.eval()

        if not isinstance(X, torch.Tensor):
            X = torch.Tensor(X)
        if not isinstance(id, torch.Tensor):
            id = torch.Tensor(id)

        data_dict = {"X": X.to(self.device), "ID": id.to(self.device).long()}
        x = self._build_stacked_features(data_dict=data_dict)

        (pred_abs_factor, pred_sign_factor, _, _,) = self.model(x=x, id=data_dict["ID"])

        preds = self._invert_to_prediction(
            pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
        )
        predictions = pd.Series(preds.view(-1).cpu().tolist(), index=id.int().tolist(),)
        probabilities = pd.Series(
            self._build_probabilities(pred_sign_factor=pred_sign_factor)
            .view(-1)
            .cpu()
            .tolist(),
            index=id.int().tolist(),
        )

        # Post-process
        assert id_to_asset is not None
        predictions.index = predictions.index.map(lambda x: id_to_asset[x])
        probabilities.index = probabilities.index.map(lambda x: id_to_asset[x])

        # Rescale
        labels_columns = self.dataset_params["labels_columns"]
        labels_columns = [
            labels_column.replace("-", "/") for labels_column in labels_columns
        ]

        predictions = predictions.rename("predictions").to_frame().T[labels_columns]
        predictions = inverse_preprocess_data(
            data=predictions * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        ).loc["predictions"]

        probabilities = probabilities.rename("probabilities")[labels_columns]

        return {"predictions": predictions, "probabilities": probabilities}


if __name__ == "__main__":
    import fire

    fire.Fire(PredictorV1)
</file>

<file path="trainer/models/utils.py">
import os
import torch
from glob import glob
import torch.nn as nn
from logging import getLogger
import pandas as pd

logger = getLogger("model")


def save_model(model, dir, epoch):
    os.makedirs(dir, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(dir, f"checkpoint-{epoch}.ckpt"))

    print(f"[+] Model is saved. Epoch: {epoch}")


def load_model(model, dir, load_epoch=None, strict=True, device="cuda"):
    params_to_load = {}
    if device == "cpu":
        params_to_load["map_location"] = torch.device("cpu")

    if not os.path.isdir(dir):
        print("[!] Load is failed")
        return -1

    check_points = glob(os.path.join(dir, "checkpoint-*.ckpt"))
    check_points = sorted(
        check_points,
        key=lambda x: int(
            x.split("/")[-1].replace("checkpoint-", "").replace(".ckpt", "")
        ),
    )

    # skip if there are no checkpoints
    if len(check_points) == 0:
        print("[!] Load is failed")
        return -1

    check_point = check_points[-1]

    # If load_epoch has value
    if load_epoch is not None:

        model.load_state_dict(
            torch.load(
                os.path.join(dir, f"checkpoint-{load_epoch}.ckpt"), **params_to_load
            ),
            strict=strict,
        )
        print("[+] Model is loaded")
        print(f"[+] Epoch: {load_epoch}")
        logger.info(f"[+] Model is loaded | Epoch: {load_epoch}")

        return load_epoch

    model.load_state_dict(torch.load(check_point, **params_to_load), strict=strict)
    last_epoch = int(
        check_point.split("/")[-1].replace("checkpoint-", "").replace(".ckpt", "")
    )

    print("[+] Model is loaded")
    print(f"[+] Epoch: {last_epoch}")
    logger.info(f"[+] Model is loaded | Epoch: {last_epoch}")

    return last_epoch


def weights_init(m):
    if isinstance(m, (nn.Conv1d, nn.Linear, nn.BatchNorm1d, nn.GroupNorm)):
        nn.init.normal_(m.weight.data, 0.0, 0.02)
        if hasattr(m, "bias"):
            if m.bias is not None:
                nn.init.constant_(m.bias.data, 0)


def inverse_preprocess_data(data, scaler):
    processed_data = pd.DataFrame(
        scaler.inverse_transform(data), index=data.index, columns=data.columns
    )

    return processed_data
</file>

<file path="trainer/modules/block_1d/__init__.py">
from .norms import NORMS
from .self_attention import SelfAttention1d
from .dense_block import DenseBlock, TransitionBlock
</file>

<file path="trainer/modules/block_1d/dense_block.py">
import torch
import torch.nn as nn
import torch.nn.functional as F
from .norms import NORMS
from .seblock import SEBlock
from .self_attention import SelfAttention1d
from develop.src.trainer.modules import acts


def identity(x):
    return x


class BottleneckBlock(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        planes: int = None,
        dropout: float = 0.0,
        activation: str = "relu",
        normalization: str = "bn",
        seblock: bool = False,
        sablock: bool = False,
    ):
        super(BottleneckBlock, self).__init__()
        if planes is None:
            planes = out_channels * 4

        bias = True
        if normalization is not None:
            bias = False

        if activation == "selu":
            bias = False

        # Define blocks
        self.norm1 = identity
        if normalization is not None:
            self.norm1 = NORMS[normalization.upper()](num_channels=in_channels)

        self.conv1 = nn.Conv1d(
            in_channels, planes, kernel_size=1, stride=1, padding=0, bias=bias
        )

        self.norm2 = identity
        if normalization is not None:
            self.norm2 = NORMS[normalization.upper()](num_channels=planes)

        self.conv2 = nn.Conv1d(
            planes, out_channels, kernel_size=3, stride=1, padding=1, bias=bias
        )

        self.act = getattr(acts, activation)
        if activation == "selu":
            self.dropout = nn.Sequential(
                *[nn.AlphaDropout(dropout / 5), nn.Dropout2d(dropout)]
            )
        else:
            self.dropout = nn.Sequential(*[nn.Dropout(dropout), nn.Dropout2d(dropout)])

        # Optional blocks
        self.seblock = None
        if seblock is True:
            self.seblock = SEBlock(in_channels=in_channels, activation=activation)

        self.sablock = None
        if sablock is True:
            self.sablock = SelfAttention1d(in_channels=in_channels)

    def forward(self, x: torch.Tensor):

        after_norm = self.norm1(x)
        if self.seblock is not None:
            after_norm = self.seblock(after_norm)

        after_act = self.act(after_norm)
        if self.sablock is not None:
            after_act = self.sablock(after_act)

        out = self.conv1(self.dropout(after_act))
        out = self.conv2(self.dropout(self.act(self.norm2(out))))

        return torch.cat([x, out], dim=1)


class TransitionBlock(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        dropout: float = 0.0,
        activation: str = "relu",
        normalization: str = "bn",
    ):
        super(TransitionBlock, self).__init__()
        bias = True
        if normalization is not None:
            bias = False

        if activation == "selu":
            bias = False

        self.norm = identity
        if normalization is not None:
            self.norm = NORMS[normalization.upper()](num_channels=in_channels)

        self.act = getattr(acts, activation)
        self.conv = nn.Conv1d(
            in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias
        )
        if activation == "selu":
            self.dropout = nn.Sequential(
                *[nn.AlphaDropout(dropout / 5), nn.Dropout2d(dropout)]
            )
        else:
            self.dropout = nn.Sequential(*[nn.Dropout(dropout), nn.Dropout2d(dropout)])

    def forward(self, x: torch.Tensor):
        out = self.conv(self.dropout(self.act(self.norm(x))))
        return F.avg_pool1d(out, 2)


class DenseBlock(nn.Module):
    def __init__(
        self,
        n_layers: int,
        in_channels: int,
        growth_rate: int,
        planes: int = None,
        dropout: float = 0.0,
        activation: str = "relu",
        normalization: str = "bn",
        seblock: bool = False,
        sablock: bool = False,
    ):
        super(DenseBlock, self).__init__()

        layers = [
            BottleneckBlock(
                in_channels=in_channels + (idx * growth_rate),
                out_channels=growth_rate,
                planes=planes,
                dropout=dropout,
                activation=activation,
                normalization=normalization,
                seblock=seblock if idx == (n_layers - 1) else False,
                sablock=sablock if idx == (n_layers - 1) else False,
            )
            for idx in range(n_layers)
        ]
        self.layers = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor):
        return self.layers(x)
</file>

<file path="trainer/modules/block_1d/norms.py">
from torch import nn


NORMS = {
    "BN": lambda num_channels: nn.BatchNorm1d(num_features=num_channels),
    "GN": lambda num_channels: nn.GroupNorm(num_groups=3, num_channels=num_channels)
    if num_channels % 3 == 0
    else nn.GroupNorm(num_groups=2, num_channels=num_channels),
    "LN": lambda num_channels: nn.GroupNorm(num_groups=1, num_channels=num_channels),
    "IN": lambda num_channels: nn.GroupNorm(
        num_groups=num_channels, num_channels=num_channels
    ),
}


def perform_sn(module, sn=False):
    if sn is False:
        return module
    if sn is True:
        return nn.utils.spectral_norm(module)
</file>

<file path="trainer/modules/block_1d/seblock.py">
from torch import nn
from .norms import perform_sn
from develop.src.trainer.modules import acts


get_activation_func = {
    "relu": nn.ReLU,
    "prelu": nn.PReLU,
    "leaky_relu": nn.LeakyReLU,
    "selu": nn.SELU,
    "mish": acts.Mish,
    "tanhexp": acts.TanhExp,
}


class SEBlock(nn.Module):
    def __init__(self, in_channels, reduction=16, activation="relu", sn=False):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(output_size=1)
        self.fc = nn.Sequential(
            perform_sn(
                nn.Linear(in_channels, in_channels // reduction, bias=False), sn=sn
            ),
            get_activation_func[activation](),
            perform_sn(
                nn.Linear(in_channels // reduction, in_channels, bias=False), sn=sn
            ),
            nn.Sigmoid(),
        )

    def forward(self, x):
        b, c, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y.expand_as(x)
</file>

<file path="trainer/modules/block_1d/self_attention.py">
import torch
from torch import nn
from .norms import perform_sn


class SelfAttention1d(nn.Module):
    def __init__(self, in_channels, sn=False):
        super().__init__()
        self.f = nn.Sequential(
            perform_sn(
                nn.Conv1d(
                    in_channels=in_channels,
                    out_channels=in_channels // 4,
                    kernel_size=1,
                    stride=1,
                    padding=0,
                ),
                sn=sn,
            ),
            nn.MaxPool1d(kernel_size=2, stride=2),
        )
        self.g = perform_sn(
            nn.Conv1d(
                in_channels=in_channels,
                out_channels=in_channels // 4,
                kernel_size=1,
                stride=1,
                padding=0,
            ),
            sn=sn,
        )
        self.h = nn.Sequential(
            perform_sn(
                nn.Conv1d(
                    in_channels=in_channels,
                    out_channels=in_channels // 2,
                    kernel_size=1,
                    stride=1,
                    padding=0,
                ),
                sn=sn,
            ),
            nn.MaxPool1d(kernel_size=2, stride=2),
        )

        self.attn_conv = perform_sn(
            nn.Conv1d(
                in_channels=in_channels // 2,
                out_channels=in_channels,
                kernel_size=1,
                stride=1,
                padding=0,
            ),
            sn=sn,
        )

        self.softmax = nn.Softmax(dim=-1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        b, ch, w = x.size()

        s = torch.bmm(
            self.f(x).view(-1, ch // 4, w // 2).permute(0, 2, 1),
            self.g(x).view(-1, ch // 4, w),
        )  # bmm(B X N X CH//4, B X CH//4 X N) -> B x N//2 x N
        beta = self.softmax(s)

        o = torch.bmm(
            self.h(x).view(-1, ch // 2, w // 2), beta
        )  # bmm(B x C//2 x N//2,  B x N//2 x N) -> B x C//2 x N
        o = self.attn_conv(o.view(b, ch // 2, w))  # -> B x C x N
        x = self.gamma * o + x

        return x
</file>

<file path="trainer/modules/acts.py">
import torch
import torch.nn as nn
from torch.nn.functional import *


def mish(x):
    return x * torch.tanh(softplus(x))


def tanhexp(x):
    return x * torch.tanh(torch.exp(x))


class Mish(nn.Module):
    def __init__(self):
        super(Mish, self).__init__()

    def forward(self, input):
        return mish(input)


class TanhExp(nn.Module):
    def __init__(self):
        super(TanhExp, self).__init__()

    def forward(self, input):
        return tanhexp(input)
</file>

</files>
