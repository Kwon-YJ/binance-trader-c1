This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
develop/
  dockerfiles/
    Dockerfile
    requirements.txt
  src/
    backtester/
      __init__.py
      backtester_v1.py
      basic_backtester.py
      utils.py
    common_utils_dev/
      __init__.py
      common_utils_dev.py
    dataset_builder/
      build_dataset.py
    rawdata_builder/
      __init__.py
      build_rawdata.py
      candidate_assets.txt
      download_kaggle_data.py
    reviewer/
      paramset.py
      reviewer_v1.py
      utils.py
    trainer/
      datasets/
        dataset.py
      models/
        backbones/
          __init__.py
          backbone_v1.py
          stack_backbone_v1.py
        __init__.py
        basic_predictor.py
        criterions.py
        predictor_v1.py
        stack_predictor_v1.py
        utils.py
      modules/
        block_1d/
          __init__.py
          dense_block.py
          norms.py
          seblock.py
          self_attention.py
        acts.py
  Makefile
services/
  dockerfiles/
    Dockerfile
    requirements.txt
  k8s/
    admin/
      namespace.yml
    deployments-template/
      data_collector-template.yml
      database-template.yml
      trader-template.yml
    service/
      service.yml
  src/
    common_utils_svc/
      __init__.py
      common_utils_svc.py
    config/
      __init__.py
    data_collector/
      data_collector.py
    database/
      database.py
      models.py
      usecase.py
    exchange/
      custom_client.py
    handler/
      __init__.py
      slack_handler.py
    trader/
      trader_v1.py
      utils.py
  Makefile
src/
  binance_trader_c1/
    __init__.py
.envrc
.gitignore
cmd.txt
Makefile
pyproject.toml
README.md
requirements-dev.lock
requirements.lock
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="develop/dockerfiles/Dockerfile">
ARG BASE_IMAGE
FROM $BASE_IMAGE
USER root

RUN apt-get update \
  && apt-get upgrade -y \
  && apt-get install build-essential -y \
  && apt-get install -y \
    git-core \
    vim \
    gettext-base \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

RUN mkdir /app
COPY . /app
WORKDIR /app

RUN pip install -r requirements.txt
RUN rm -rf /app/*

ENV PYTHONPATH $PYTHONPATH:/app:/app/src:
</file>

<file path="develop/src/backtester/__init__.py">
from .backtester_v1 import BacktesterV1
</file>

<file path="develop/src/rawdata_builder/__init__.py">
from .build_rawdata import build_rawdata
</file>

<file path="develop/src/trainer/models/__init__.py">
from .predictor_v1 import PredictorV1
</file>

<file path="develop/src/trainer/modules/block_1d/norms.py">
from torch import nn


NORMS = {
    "BN": lambda num_channels: nn.BatchNorm1d(num_features=num_channels),
    "GN": lambda num_channels: nn.GroupNorm(num_groups=3, num_channels=num_channels)
    if num_channels % 3 == 0
    else nn.GroupNorm(num_groups=2, num_channels=num_channels),
    "LN": lambda num_channels: nn.GroupNorm(num_groups=1, num_channels=num_channels),
    "IN": lambda num_channels: nn.GroupNorm(
        num_groups=num_channels, num_channels=num_channels
    ),
}


def perform_sn(module, sn=False):
    if sn is False:
        return module
    if sn is True:
        return nn.utils.spectral_norm(module)
</file>

<file path="develop/src/trainer/modules/block_1d/self_attention.py">
import torch
from torch import nn
from .norms import perform_sn


class SelfAttention1d(nn.Module):
    def __init__(self, in_channels, sn=False):
        super().__init__()
        self.f = nn.Sequential(
            perform_sn(
                nn.Conv1d(
                    in_channels=in_channels,
                    out_channels=in_channels // 4,
                    kernel_size=1,
                    stride=1,
                    padding=0,
                ),
                sn=sn,
            ),
            nn.MaxPool1d(kernel_size=2, stride=2),
        )
        self.g = perform_sn(
            nn.Conv1d(
                in_channels=in_channels,
                out_channels=in_channels // 4,
                kernel_size=1,
                stride=1,
                padding=0,
            ),
            sn=sn,
        )
        self.h = nn.Sequential(
            perform_sn(
                nn.Conv1d(
                    in_channels=in_channels,
                    out_channels=in_channels // 2,
                    kernel_size=1,
                    stride=1,
                    padding=0,
                ),
                sn=sn,
            ),
            nn.MaxPool1d(kernel_size=2, stride=2),
        )

        self.attn_conv = perform_sn(
            nn.Conv1d(
                in_channels=in_channels // 2,
                out_channels=in_channels,
                kernel_size=1,
                stride=1,
                padding=0,
            ),
            sn=sn,
        )

        self.softmax = nn.Softmax(dim=-1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        b, ch, w = x.size()

        s = torch.bmm(
            self.f(x).view(-1, ch // 4, w // 2).permute(0, 2, 1),
            self.g(x).view(-1, ch // 4, w),
        )  # bmm(B X N X CH//4, B X CH//4 X N) -> B x N//2 x N
        beta = self.softmax(s)

        o = torch.bmm(
            self.h(x).view(-1, ch // 2, w // 2), beta
        )  # bmm(B x C//2 x N//2,  B x N//2 x N) -> B x C//2 x N
        o = self.attn_conv(o.view(b, ch // 2, w))  # -> B x C x N
        x = self.gamma * o + x

        return x
</file>

<file path="develop/src/trainer/modules/acts.py">
import torch
import torch.nn as nn
from torch.nn.functional import *


def mish(x):
    return x * torch.tanh(softplus(x))


def tanhexp(x):
    return x * torch.tanh(torch.exp(x))


class Mish(nn.Module):
    def __init__(self):
        super(Mish, self).__init__()

    def forward(self, input):
        return mish(input)


class TanhExp(nn.Module):
    def __init__(self):
        super(TanhExp, self).__init__()

    def forward(self, input):
        return tanhexp(input)
</file>

<file path="services/dockerfiles/Dockerfile">
FROM continuumio/miniconda3:latest

RUN apt-get update \
  && apt-get upgrade -y \
  && apt-get install build-essential -y \
  && apt-get install -y \
    git-core \
    vim \
    gettext-base \
    postgresql-client \
    python-psycopg2 \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/*

RUN mkdir /app
COPY . /app
WORKDIR /app

RUN pip install -r requirements.txt
RUN conda install --yes pytorch torchvision cpuonly -c pytorch && conda clean -afy
RUN rm -rf /app/*

ENV PYTHONPATH $PYTHONPATH:/app:/app/src:/app/dev/src:
</file>

<file path="services/k8s/admin/namespace.yml">
kind: Namespace
apiVersion: v1
metadata:
  name: dev
  labels:
    name: dev
</file>

<file path="services/k8s/service/service.yml">
kind: Service
apiVersion: v1
metadata:
  name: database
  namespace: dev
spec:
  selector:
    app: database
  type: LoadBalancer
  ports:
    - name: postgres
      port: 5432
      targetPort: 5432
</file>

<file path="services/src/database/models.py">
from sqlalchemy import Column, Integer, FLOAT, String, TIMESTAMP, UniqueConstraint
from database import database as DB


class Pricing(DB.BASE):
    __tablename__ = "pricings"

    id = Column(Integer, primary_key=True)

    timestamp = Column(TIMESTAMP(timezone=True), nullable=False)
    asset = Column(String, nullable=False)
    open = Column(FLOAT, nullable=False)
    high = Column(FLOAT, nullable=False)
    low = Column(FLOAT, nullable=False)
    close = Column(FLOAT, nullable=False)
    volume = Column(FLOAT, nullable=False)

    __table_args__ = (UniqueConstraint("timestamp", "asset"),)

    def __init__(self, timestamp, asset, open, high, low, close, volume):
        self.timestamp = timestamp
        self.asset = asset
        self.open = open
        self.high = high
        self.low = low
        self.close = close
        self.volume = volume


class Sync(DB.BASE):
    __tablename__ = "syncs"

    id = Column(Integer, primary_key=True)
    timestamp = Column(TIMESTAMP(timezone=True), nullable=False, unique=True)

    def __init__(self, timestamp):
        self.timestamp = timestamp


class Trade(DB.BASE):
    __tablename__ = "trades"

    id = Column(Integer, primary_key=True)
    timestamp = Column(TIMESTAMP(timezone=True), nullable=False, unique=True)

    def __init__(self, timestamp):
        self.timestamp = timestamp
</file>

<file path="services/src/handler/__init__.py">
from .slack_handler import SlackHandler
</file>

<file path=".envrc">
export PYTHONPATH=$PYTHONPATH:$(pwd):$(pwd)/develop/:$(pwd)/develop/src:$(pwd)/services/:$(pwd)/services/src:
</file>

<file path="develop/dockerfiles/requirements.txt">
pandarallel
fire
black
joblib
ipython
jupyterlab
numpy
pandas
pillow
matplotlib
fastparquet
pyarrow
empyrical
kaggle
scikit-learn==0.23.2
tabulate
werkzeug
fancytable
</file>

<file path="develop/src/backtester/utils.py">
import pandas as pd


def nan_to_zero(value):
    if str(value) in ("nan", "None"):
        return 0

    return value


def load_parquet(path):
    return pd.read_parquet(path)


class Position:
    def __init__(
        self,
        asset,
        side,
        qty,
        entry_price,
        prediction,
        entry_at,
        n_updated=0,
        is_exited=False,
    ):
        self.asset = asset
        self.side = side
        self.qty = qty
        self.entry_price = entry_price
        self.prediction = prediction
        self.entry_at = entry_at
        self.n_updated = n_updated
        self.is_exited = is_exited

    def __getitem__(self, key):
        return getattr(self, key)

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __repr__(self):
        return f"Position(asset={self.asset}, side={self.side}, qty={self.qty}, entry_price={self.entry_price:.4f}, n_updated={self.n_updated}, is_exited={str(self.is_exited)})"

    def __str__(self):
        return self.__repr__()
</file>

<file path="develop/src/common_utils_dev/__init__.py">
from .common_utils_dev import (
    make_dirs,
    load_text,
    load_json,
    to_parquet,
    get_filename_by_path,
    to_abs_path,
    get_parent_dir,
)
</file>

<file path="develop/src/common_utils_dev/common_utils_dev.py">
import os
import json
import pyarrow.parquet as pq
import pyarrow as pa
from pathlib import Path


def make_dirs(dirs):
    for dir in dirs:
        os.makedirs(dir, exist_ok=True)


def load_text(path):
    with open(path, "r") as f:
        text = f.read().splitlines()

    return text


def load_json(path):
    with open(path, "r") as f:
        loaded = json.load(f)

    return loaded


def to_parquet(df, path, compression="zstd"):
    pq.write_table(table=pa.Table.from_pandas(df), where=path, compression=compression)


def get_filename_by_path(path):
    return Path(path).stem.split(".")[0]


def get_parent_dir(path):
    return Path(path).parent


def to_abs_path(file, relative_path):
    return os.path.normpath(
        os.path.join(os.path.dirname(os.path.abspath(file)), relative_path)
    )
</file>

<file path="develop/src/rawdata_builder/candidate_assets.txt">
BTC-USDT
ETH-USDT
LINK-USDT
FIL-USDT
YFI-USDT
LTC-USDT
BCH-USDT
YFII-USDT
XRP-USDT
DOT-USDT
BNB-USDT
TRB-USDT
ADA-USDT
UNI-USDT
AAVE-USDT
EOS-USDT
WAVES-USDT
RSR-USDT
THETA-USDT
EGLD-USDT
SXP-USDT
TRX-USDT
AVAX-USDT
VET-USDT
XTZ-USDT
COMP-USDT
ZEC-USDT
CRV-USDT
XMR-USDT
ATOM-USDT
MKR-USDT
OMG-USDT
BAND-USDT
XLM-USDT
SUSHI-USDT
BZRX-USDT
RUNE-USDT
KAVA-USDT
SNX-USDT
SOL-USDT
BEL-USDT
RLC-USDT
</file>

<file path="develop/src/rawdata_builder/download_kaggle_data.py">
import os
import kaggle
from common_utils_dev import to_abs_path, make_dirs


SPOT_CONFIG = {
    "dataset_name": "jorijnsmit/binance-full-history",
    "store_dir": to_abs_path(__file__, "../../storage/dataset/rawdata/raw/spot/"),
}

FUTURE_CONFIG = {
    "dataset_name": "nicolaes/binance-futures",
    "store_dir": to_abs_path(__file__, "../../storage/dataset/rawdata/raw/future/"),
}


def download(username, key):
    for config in [SPOT_CONFIG, FUTURE_CONFIG]:
        dataset_name = config["dataset_name"]
        store_dir = config["store_dir"]

        make_dirs([store_dir])

        # Set env to authenticate
        os.environ["KAGGLE_USERNAME"] = username
        os.environ["KAGGLE_KEY"] = key
        kaggle.api.authenticate()

        # Download
        kaggle.api.dataset_download_files(dataset_name, path=store_dir, unzip=True)


if __name__ == "__main__":
    import fire

    fire.Fire(download)
</file>

<file path="develop/src/reviewer/utils.py">
from itertools import product
from collections import OrderedDict


def grid(params):
    def _handle_value(value):
        return list(value) if type(value) == list or type(value) == tuple else [value]

    params = OrderedDict(
        [(key, _handle_value(value=params[key])) for key in sorted(list(params.keys()))]
    )

    keys = list(params.keys())
    for values in product(*list(params.values())):
        param = dict(zip(keys, values))
        yield param






import pandas as pd


def nan_to_zero(value):
    if str(value) in ("nan", "None"):
        return 0

    return value


def load_parquet(path):
    return pd.read_parquet(path)


class Position:
    def __init__(
        self,
        asset,
        side,
        qty,
        entry_price,
        prediction,
        entry_at,
        n_updated=0,
        is_exited=False,
    ):
        self.asset = asset
        self.side = side
        self.qty = qty
        self.entry_price = entry_price
        self.prediction = prediction
        self.entry_at = entry_at
        self.n_updated = n_updated
        self.is_exited = is_exited

    def __getitem__(self, key):
        return getattr(self, key)

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __repr__(self):
        return f"Position(asset={self.asset}, side={self.side}, qty={self.qty}, entry_price={self.entry_price:.4f}, n_updated={self.n_updated}, is_exited={str(self.is_exited)})"

    def __str__(self):
        return self.__repr__()
</file>

<file path="develop/src/trainer/models/backbones/__init__.py">
from .backbone_v1 import BackboneV1
from .stack_backbone_v1 import StackBackboneV1
</file>

<file path="develop/src/trainer/models/stack_predictor_v1.py">
import os
import shutil
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Union, Optional, List, Dict
from tqdm import tqdm
from .basic_predictor import BasicPredictor
from .utils import inverse_preprocess_data
from common_utils_dev import to_parquet, to_abs_path, load_json
from trainer.models.predictor_v1 import PredictorV1

COMMON_CONFIG = {
    "data_dir": to_abs_path(__file__, "../../../storage/dataset/dataset/v001/train"),
    "exp_dir": to_abs_path(__file__, "../../../storage/experiments/v001_stack"),
    "test_data_dir": to_abs_path(
        __file__, "../../../storage/dataset/dataset/v001/test"
    ),
    "feature_extractor_dir": to_abs_path(__file__, "../../../storage/experiments/v001"),
}

DATA_CONFIG = {
    "checkpoint_dir": "./check_point",
    "generate_output_dir": "./generated_output",
    "base_feature_assets": ["BTC-USDT"],
}

MODEL_CONFIG = {
    "local_lookback_window": 30,
    "lookback_window": 149,
    "batch_size": 512,
    "lr": 0.0001,
    "epochs": 15,
    "print_epoch": 1,
    "print_iter": 50,
    "save_epoch": 1,
    "criterion": "l2",
    "criterion_params": {},
    "load_strict": False,
    "model_name": "StackBackboneV1",
    "model_params": {
        "in_channels": 86,
        "n_blocks": 4,
        "n_block_layers": 8,
        "growth_rate": 12,
        "dropout": 0.1,
        "channel_reduction": 0.5,
        "activation": "selu",
        "normalization": None,
        "seblock": True,
        "sablock": True,
    },
}


class StackPredictorV1(BasicPredictor):
    """
    Functions:
        train(): train the model with train_data
        generate(save_dir: str): generate predictions & labels with test_data
        predict(X: torch.Tensor): gemerate prediction with given data
    """

    def __init__(
        self,
        data_dir=COMMON_CONFIG["data_dir"],
        test_data_dir=COMMON_CONFIG["test_data_dir"],
        d_config={},
        m_config={},
        feature_extractor_dir=COMMON_CONFIG["exp_dir"],
        exp_dir=COMMON_CONFIG["exp_dir"],
        device="cuda",
        pin_memory=False,
        num_workers=8,
        mode="train",
        default_d_config=DATA_CONFIG,
        default_m_config=MODEL_CONFIG,
    ):
        super().__init__(
            data_dir=data_dir,
            test_data_dir=test_data_dir,
            d_config=d_config,
            m_config=m_config,
            exp_dir=exp_dir,
            device=device,
            pin_memory=pin_memory,
            num_workers=num_workers,
            mode=mode,
            default_d_config=default_d_config,
            default_m_config=default_m_config,
        )

        self._build_feature_extractor_params(
            feature_extractor_dir=feature_extractor_dir
        )
        self._build_feature_extractor(feature_extractor_dir=feature_extractor_dir)
        assert (
            self.feature_extractor_params["model_config"]["lookback_window"]
            + self.model_config["local_lookback_window"]
            - 1
            == self.model_config["lookback_window"]
        )

    def _build_feature_extractor_params(self, feature_extractor_dir):
        self.feature_extractor_params = load_json(
            os.path.join(feature_extractor_dir, "trainer_params.json")
        )

    def _build_feature_extractor(self, feature_extractor_dir):
        self.feature_extractor = PredictorV1(
            exp_dir=feature_extractor_dir,
            m_config=self.feature_extractor_params["model_config"],
            d_config=self.feature_extractor_params["data_config"],
            device=self.device,
            mode="predict",
        )

    def _invert_to_sign(self, pred_sign_factor, boundary=0.5):
        return ((pred_sign_factor >= boundary) * 1.0) + (
            (pred_sign_factor < boundary) * -1.0
        )

    def _invert_to_prediction(self, pred_abs_factor, pred_sign_factor):
        multiply = self._invert_to_sign(pred_sign_factor=pred_sign_factor)
        return pred_abs_factor * multiply

    def _extract_features(self, data_dict):
        with torch.no_grad():
            abs_factor_features, sign_factor_features = self.feature_extractor.model(
                x=torch.cat(
                    [
                        data_dict["X"][
                            :,
                            :,
                            idx : self.feature_extractor_params["model_config"][
                                "lookback_window"
                            ]
                            + idx,
                        ]
                        for idx in range(self.model_config["local_lookback_window"])
                    ],
                    dim=0,
                ),
                id=torch.cat(
                    [
                        data_dict["ID"]
                        for _ in range(self.model_config["local_lookback_window"])
                    ],
                    dim=0,
                ),
            )

            abs_factor_features = torch.stack(
                abs_factor_features.squeeze().split(data_dict["ID"].size()[0], dim=0),
                dim=-1,
            )
            sign_factor_features = torch.stack(
                sign_factor_features.squeeze().split(data_dict["ID"].size()[0], dim=0),
                dim=-1,
            )
            return (
                abs_factor_features,
                sign_factor_features,
            )

    def _build_stacked_features(self, data_dict):
        abs_factor_features, sign_factor_features = self._extract_features(
            data_dict=data_dict
        )

        return torch.cat(
            [
                data_dict["X"][:, :, -self.model_config["local_lookback_window"] :],
                abs_factor_features.unsqueeze(dim=1),
                sign_factor_features.unsqueeze(dim=1),
            ],
            dim=1,
        )

    def _compute_train_loss(self, train_data_dict):
        # Set train mode
        self.model.train()
        self.model.zero_grad()

        # Set loss
        x = self._build_stacked_features(data_dict=train_data_dict)

        (
            pred_abs_factor,
            pred_sign_factor,
            pred_abs_error_factor,
            pred_sign_error_factor,
        ) = self.model(x=x, id=train_data_dict["ID"],)

        # Y loss
        loss = self.criterion(pred_abs_factor, train_data_dict["Y"].view(-1).abs()) * 10
        loss += self.binary_cross_entropy(
            pred_sign_factor, (train_data_dict["Y"].view(-1) >= 0) * 1.0
        )

        loss += (
            self.criterion(
                pred_abs_error_factor,
                (
                    train_data_dict["Y"].view(-1).abs() - x[:, -2, -1].view(-1).abs()
                ).abs(),
            )
            * 2
        )
        loss += (
            self.binary_cross_entropy(
                pred_sign_error_factor,
                (
                    (
                        self._invert_to_sign(
                            train_data_dict["Y"].view(-1), boundary=0.0
                        )
                        * self._invert_to_sign(x[:, -1, -1].view(-1), boundary=0.5)
                    )
                    >= 0
                )
                * 1.0,
            )
            * 0.2
        )

        return (
            loss,
            self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            ),
        )

    def _compute_test_loss(self, test_data_dict):
        # Set eval mode
        self.model.eval()

        # Set loss
        x = self._build_stacked_features(data_dict=test_data_dict)

        (
            pred_abs_factor,
            pred_sign_factor,
            pred_abs_error_factor,
            pred_sign_error_factor,
        ) = self.model(x=x, id=test_data_dict["ID"])

        # Y loss
        loss = self.criterion(pred_abs_factor, test_data_dict["Y"].view(-1).abs()) * 10
        loss += self.binary_cross_entropy(
            pred_sign_factor, (test_data_dict["Y"].view(-1) >= 0) * 1.0
        )

        loss += (
            self.criterion(
                pred_abs_error_factor,
                (
                    test_data_dict["Y"].view(-1).abs() - x[:, -2, -1].view(-1).abs()
                ).abs(),
            )
            * 2
        )
        loss += (
            self.binary_cross_entropy(
                pred_sign_error_factor,
                (
                    (
                        self._invert_to_sign(test_data_dict["Y"].view(-1), boundary=0.0)
                        * self._invert_to_sign(x[:, -1, -1].view(-1), boundary=0.5)
                    )
                    >= 0
                )
                * 1.0,
            )
            * 0.2
        )

        return (
            loss,
            self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            ),
        )

    def _step(self, train_data_dict):
        loss, _ = self._compute_train_loss(train_data_dict=train_data_dict)
        loss.backward()
        self.optimizer.step()

        return loss

    def _display_info(self, train_loss, test_loss, test_predictions, test_labels):
        pred_norm = test_predictions[test_predictions >= 0].abs().mean()
        label_norm = test_labels[test_labels >= 0].abs().mean()

        # Print loss info
        print(
            f""" [+] train_loss: {train_loss:.2f}, test_loss: {test_loss:.2f} | [+] pred_norm: {pred_norm:.2f}, label_norm: {label_norm:.2f}"""
        )

    def _build_abs_bins(self, df):
        abs_bins = {}
        for column in df.columns:
            _, abs_bins[column] = pd.qcut(
                df[column].abs(), 10, labels=False, retbins=True
            )
            abs_bins[column] = np.concatenate([[0], abs_bins[column][1:-1], [np.inf]])

        return pd.DataFrame(abs_bins)

    def _build_probabilities(self, pred_sign_factor):
        return ((pred_sign_factor - 0.5) * 2).abs()

    def train(self):
        for epoch in range(self.model_config["epochs"]):
            if epoch <= self.last_epoch:
                continue

            for iter_ in tqdm(range(len(self.train_data_loader))):
                # Optimize
                train_data_dict = self._generate_train_data_dict()
                train_loss = self._step(train_data_dict=train_data_dict)

                # Display losses
                if epoch % self.model_config["print_epoch"] == 0:
                    if iter_ % self.model_config["print_iter"] == 0:
                        test_data_dict = self._generate_test_data_dict()
                        test_loss, test_predictions = self._compute_test_loss(
                            test_data_dict=test_data_dict
                        )
                        self._display_info(
                            train_loss=train_loss,
                            test_loss=test_loss,
                            test_predictions=test_predictions,
                            test_labels=test_data_dict["Y"],
                        )

            # Store the check-point
            if (epoch % self.model_config["save_epoch"] == 0) or (
                epoch == self.model_config["epochs"] - 1
            ):
                self._save_model(model=self.model, epoch=epoch)

    def generate(self, save_dir=None):
        assert self.mode in ("test")
        self.model.eval()

        if save_dir is None:
            save_dir = self.data_config["generate_output_dir"]

        # Mutate 1 min to handle logic, entry: open, exit: open
        index = self.test_data_loader.dataset.index
        index = index.set_levels(index.levels[0] + pd.Timedelta(minutes=1), level=0)

        predictions = []
        labels = []
        probabilities = []
        for idx in tqdm(range(len(self.test_data_loader))):
            test_data_dict = self._generate_test_data_dict()

            x = self._build_stacked_features(data_dict=test_data_dict)

            (pred_abs_factor, pred_sign_factor, _, _,) = self.model(
                x=x, id=test_data_dict["ID"]
            )

            preds = self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            )

            predictions += preds.view(-1).cpu().tolist()
            labels += test_data_dict["Y"].view(-1).cpu().tolist()
            probabilities += (
                self._build_probabilities(pred_sign_factor=pred_sign_factor)
                .view(-1)
                .cpu()
                .tolist()
            )

        predictions = (
            pd.Series(predictions, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )
        labels = (
            pd.Series(labels, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )
        probabilities = (
            pd.Series(probabilities, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )

        # Rescale
        predictions = inverse_preprocess_data(
            data=predictions * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        )
        labels = inverse_preprocess_data(
            data=labels * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        )

        prediction_abs_bins = self._build_abs_bins(df=predictions)
        probability_bins = self._build_abs_bins(df=probabilities)

        # Store signals
        for data_type, data in [
            ("predictions", predictions),
            ("labels", labels),
            ("probabilities", probabilities),
            ("prediction_abs_bins", prediction_abs_bins),
            ("probability_bins", probability_bins),
        ]:
            to_parquet(
                df=data, path=os.path.join(save_dir, f"{data_type}.parquet.zstd"),
            )

    def predict(
        self,
        X: Union[np.ndarray, torch.Tensor],
        id: Union[List, torch.Tensor],
        id_to_asset: Optional[Dict] = None,
    ):
        assert self.mode in ("predict")
        self.model.eval()

        if not isinstance(X, torch.Tensor):
            X = torch.Tensor(X)
        if not isinstance(id, torch.Tensor):
            id = torch.Tensor(id)

        data_dict = {"X": X.to(self.device), "ID": id.to(self.device).long()}
        x = self._build_stacked_features(data_dict=data_dict)

        (pred_abs_factor, pred_sign_factor, _, _,) = self.model(x=x, id=data_dict["ID"])

        preds = self._invert_to_prediction(
            pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
        )
        predictions = pd.Series(preds.view(-1).cpu().tolist(), index=id.int().tolist(),)
        probabilities = pd.Series(
            self._build_probabilities(pred_sign_factor=pred_sign_factor)
            .view(-1)
            .cpu()
            .tolist(),
            index=id.int().tolist(),
        )

        # Post-process
        assert id_to_asset is not None
        predictions.index = predictions.index.map(lambda x: id_to_asset[x])
        probabilities.index = probabilities.index.map(lambda x: id_to_asset[x])

        # Rescale
        labels_columns = self.dataset_params["labels_columns"]
        labels_columns = [
            labels_column.replace("-", "/") for labels_column in labels_columns
        ]

        predictions = predictions.rename("predictions").to_frame().T[labels_columns]
        predictions = inverse_preprocess_data(
            data=predictions * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        ).loc["predictions"]

        probabilities = probabilities.rename("probabilities")[labels_columns]

        return {"predictions": predictions, "probabilities": probabilities}


if __name__ == "__main__":
    import fire

    fire.Fire(PredictorV1)
</file>

<file path="develop/src/trainer/modules/block_1d/__init__.py">
from .norms import NORMS
from .self_attention import SelfAttention1d
from .dense_block import DenseBlock, TransitionBlock
</file>

<file path="develop/src/trainer/modules/block_1d/seblock.py">
from torch import nn
from .norms import perform_sn
from develop.src.trainer.modules import acts


get_activation_func = {
    "relu": nn.ReLU,
    "prelu": nn.PReLU,
    "leaky_relu": nn.LeakyReLU,
    "selu": nn.SELU,
    "mish": acts.Mish,
    "tanhexp": acts.TanhExp,
}


class SEBlock(nn.Module):
    def __init__(self, in_channels, reduction=16, activation="relu", sn=False):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool1d(output_size=1)
        self.fc = nn.Sequential(
            perform_sn(
                nn.Linear(in_channels, in_channels // reduction, bias=False), sn=sn
            ),
            get_activation_func[activation](),
            perform_sn(
                nn.Linear(in_channels // reduction, in_channels, bias=False), sn=sn
            ),
            nn.Sigmoid(),
        )

    def forward(self, x):
        b, c, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1)
        return x * y.expand_as(x)
</file>

<file path="services/src/common_utils_svc/__init__.py">
from .common_utils_svc import (
    Position,
    make_dirs,
    load_text,
    load_json,
    to_parquet,
    get_filename_by_path,
    to_abs_path,
    initialize_main_logger,
    initialize_trader_logger,
)
</file>

<file path="services/src/database/usecase.py">
from dataclasses import dataclass
from database import database as DB
from database import models
from typing import List, Dict
import pandas as pd


@dataclass
class Usecase:
    sess = DB.SESSION

    def __post_init__(self):
        DB.wait_connection()

    def get_pricing(self, start_on: pd.Timestamp, end_on: pd.Timestamp):
        pricing = self.sess.execute(
            f"""
            select
                timestamp,
                asset,
                open,
                high,
                low,
                close,
                volume
            from pricings
            where
                timestamp >= '{start_on.isoformat()}'::timestamp and
                timestamp <= '{end_on.isoformat()}'::timestamp;
            """
        ).fetchall()

        pricing = pd.DataFrame(
            pricing,
            columns=["timestamp", "asset", "open", "high", "low", "close", "volume"],
        )
        pricing["timestamp"] = pd.DatetimeIndex(pricing["timestamp"]).tz_convert("UTC")
        pricing = pricing.set_index(["timestamp", "asset"]).sort_index()

        return pricing

    def get_last_sync_on(self):
        quried = (
            self.sess.query(models.Sync).order_by(models.Sync.timestamp.desc()).first()
        )
        if quried is None:
            return None

        return pd.Timestamp(quried.timestamp).tz_convert("UTC").floor("T")

    def get_last_trade_on(self):
        queried = (
            self.sess.query(models.Trade)
            .order_by(models.Trade.timestamp.desc())
            .first()
        )

        if queried is None:
            return None

        return pd.Timestamp(queried.timestamp).tz_convert("UTC").floor("T")

    def insert_pricings(self, inserts: List[Dict], n_buffer: int = 500):
        tmpl = """
        INSERT INTO
            pricings (
                timestamp,
                asset,
                open,
                high,
                low,
                close,
                volume
            )
        VALUES {};
        """.strip()

        def to_tuple(x, i):
            return (
                f"(:p{i}_1,:p{i}_2,:p{i}_3,:p{i}_4,:p{i}_5,:p{i}_6,:p{i}_7)",
                (
                    {
                        f"p{i}_1": x["timestamp"],
                        f"p{i}_2": x["asset"],
                        f"p{i}_3": x["open"],
                        f"p{i}_4": x["high"],
                        f"p{i}_5": x["low"],
                        f"p{i}_6": x["close"],
                        f"p{i}_7": x["volume"],
                    }
                ),
            )

        for i in range(0, len(inserts), n_buffer):

            items = [
                to_tuple(item, j) for j, item in enumerate(inserts[i : i + n_buffer])
            ]

            query = tmpl.format(",".join([item[0] for item in items]))

            params = {}
            for item in items:
                params.update(item[1])

            self.sess.execute(query, params)

        self.sess.commit()

    def insert_syncs(self, inserts: List[Dict], n_buffer: int = 500):
        tmpl = """
        INSERT INTO
            syncs (
                timestamp
            )
        VALUES {};
        """.strip()

        def to_tuple(x, i):
            return (f"(:p{i}_1)", ({f"p{i}_1": x["timestamp"]}))

        for i in range(0, len(inserts), n_buffer):

            items = [
                to_tuple(item, j) for j, item in enumerate(inserts[i : i + n_buffer])
            ]

            query = tmpl.format(",".join([item[0] for item in items]))

            params = {}
            for item in items:
                params.update(item[1])

            self.sess.execute(query, params)

        self.sess.commit()

    def insert_trade(self, insert: Dict):
        self.sess.add(models.Trade(timestamp=insert["timestamp"]))

        self.sess.commit()

    def update_pricings(self, updates: List[Dict], n_buffer: int = 500):
        tup_str = ""
        min_timestamp = []
        for update in updates:
            tup_str += f"('{update['timestamp'].isoformat()}'::timestamp, '{update['asset']}'),"
            min_timestamp.append(update["timestamp"])
        tup_str = tup_str[:-1]
        min_timestamp = min(min_timestamp)

        db_items = self.sess.execute(
            f"""
            SELECT id,
                    TIMESTAMP,
                    asset,
                    volume
            FROM   pricings
            WHERE  ( TIMESTAMP, asset ) IN (VALUES {tup_str})
                    AND TIMESTAMP >= '{min_timestamp.isoformat()}'::timestamp;
            """
        )

        key = lambda ts, asset: f"{ts.isoformat()}_{asset}"
        db_items_dict = dict()
        for db_item in db_items:
            db_items_dict[key(ts=db_item[1], asset=db_item[2])] = {
                "id": db_item[0],
                "volume": db_item[3],
            }

        ids_to_delete = list()
        to_insert = list()
        for update in updates:
            k = key(ts=update["timestamp"], asset=update["asset"])
            if k in db_items_dict:
                db_item = db_items_dict[k]
                if db_item["volume"] != update["volume"]:
                    # Value changed
                    ids_to_delete.append(db_item["id"])
                else:
                    continue

            to_insert.append(update)

        # Delete if changed
        self.sess.query(models.Pricing).filter(
            models.Pricing.id.in_(ids_to_delete)
        ).delete(synchronize_session=False)

        # Insert
        if len(to_insert):
            self.insert_pricings(inserts=to_insert, n_buffer=n_buffer)
        else:
            self.sess.commit()

    def update_syncs(self, updates: List[Dict], n_buffer: int = 500):
        tup_str = ""
        min_timestamp = []
        for update in updates:
            tup_str += f"('{update['timestamp'].isoformat()}'::timestamp),"
            min_timestamp.append(update["timestamp"])
        tup_str = tup_str[:-1]
        min_timestamp = min(min_timestamp)

        db_items = self.sess.execute(
            f"""
            SELECT TIMESTAMP
            FROM   syncs
            WHERE  ( TIMESTAMP ) IN (VALUES {tup_str})
                    AND TIMESTAMP >= '{min_timestamp.isoformat()}'::timestamp;
            """
        )

        key = lambda ts: f"{ts.isoformat()}"
        db_timestamps = set([key(ts=db_item[0]) for db_item in db_items])

        to_insert = list()
        for update in updates:
            k = key(ts=update["timestamp"])
            if k in db_timestamps:
                continue

            to_insert.append(update)

        # Insert
        if len(to_insert):
            self.insert_syncs(inserts=to_insert, n_buffer=n_buffer)

    def delete_old_records(self, table: str, limit: int):
        assert table in ("pricings", "syncs", "trades")
        if table == "pricings":
            table_class = models.Pricing
        elif table == "syncs":
            table_class = models.Sync
        elif table == "trades":
            table_class = models.Trade
        else:
            raise NotImplementedError

        table_counts = self.sess.execute(
            f"""
            SELECT count(*)
            FROM   {table};
            """
        ).first()[0]

        if table_counts > limit:
            # Delete overflowed records
            id_subqueries = (
                self.sess.query(table_class.id)
                .order_by(table_class.timestamp.asc())
                .limit(table_counts - limit)
            )
            self.sess.query(table_class).filter(
                table_class.id.in_(id_subqueries)
            ).delete(synchronize_session=False)
            self.sess.commit()
</file>

<file path="src/binance_trader_c1/__init__.py">
def hello() -> str:
    return "Hello from binance-trader-c1!"
</file>

<file path=".gitignore">
__pycache__
.pyc
.pytest_cache
.ipynb_checkpoints
*.pyc
*.h
*.c
*.so
.vscode
.venv
.python-version
.nbconvert
notebook_cookie_secret
*.csv
*.git
*.pkl
*.pkl.gz
*.DS_Store
tmp/
*.egg-info/
*/storage/*
*/k8s/deployments/

*.parquet
*.zstd
</file>

<file path="cmd.txt">
source .venv/bin/activate

python -m develop.src.rawdata_builder.build_rawdata

python -m develop.src.dataset_builder.build_dataset build

python -m develop.src.trainer.models.predictor_v1 train --mode=train

python -m develop.src.trainer.models.predictor_v1 generate --mode=test

python -m develop.src.reviewer.reviewer_v1 run --in_shell True --n_jobs 1
</file>

<file path="Makefile">
dev_rm:
	@make -C develop rm

dev_run:
	@make -C develop run

dev_bash:
	@make -C develop bash

dev_download_kaggle_data:
	@make -C develop download_kaggle_data

dev_build_rawdata:
	@make -C develop build_rawdata

dev_build_dataset:
	@make -C develop build_dataset

dev_train:
	@make -C develop train

dev_generate:
	@make -C develop generate

dev_review:
	@make -C develop review

dev_display_review:
	@make -C develop display_review

svc_install:
	@make -C services install

svc_run:
	@make -C services run

svc_rm:
	@make -C services rm

svc_delete:
	@make -C services delete

svc_reapply:
	@make -C services reapply

svc_pods:
	@make -C services pods

svc_db_bash:
	@make -C services db_bash

svc_dc_bash:
	@make -C services dc_bash

svc_td_bash:
	@make -C services td_bash
</file>

<file path="pyproject.toml">
[project]
name = "binance-trader-c1"
version = "0.1.0"
description = "Add your description here"
dependencies = [
    "torch>=2.6.0",
    "torchvision>=0.21.0",
    "torchaudio>=2.6.0",
    "fire>=0.7.0",
    "numpy==1.26",
    "tqdm>=4.67.1",
    "scikit-learn>=1.6.1",
    "spot>=0.1a0",
    "joblib>=1.4.2",
    "pyarrow>=19.0.1",
    "ipython>=9.0.2",
    "matplotlib>=3.10.1",
    "empyrical>=0.5.5",
    "tabulate>=0.9.0",
    "fancytable>=0.0.3",
    "pandas>=2.2.3",
    "pandarallel>=1.6.5",
]
readme = "README.md"
requires-python = ">= 3.8"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.rye]
managed = true
dev-dependencies = []

[tool.hatch.metadata]
allow-direct-references = true

[tool.hatch.build.targets.wheel]
packages = ["src/binance_trader_c1"]
</file>

<file path="requirements-dev.lock">
# generated by rye
# use `rye lock` or `rye sync` to update this lockfile
#
# last locked with the following flags:
#   pre: false
#   features: []
#   all-features: false
#   with-sources: false
#   generate-hashes: false
#   universal: false

-e file:.
asttokens==3.0.0
    # via stack-data
certifi==2025.1.31
    # via requests
charset-normalizer==3.4.1
    # via requests
contourpy==1.3.1
    # via matplotlib
cycler==0.12.1
    # via matplotlib
decorator==5.2.1
    # via ipython
dill==0.3.9
    # via pandarallel
empyrical==0.5.5
    # via binance-trader-c1
executing==2.2.0
    # via stack-data
fancytable==0.0.3
    # via binance-trader-c1
filelock==3.18.0
    # via torch
fire==0.7.0
    # via binance-trader-c1
fonttools==4.56.0
    # via matplotlib
fsspec==2025.3.0
    # via torch
idna==3.10
    # via requests
ipython==9.0.2
    # via binance-trader-c1
ipython-pygments-lexers==1.1.1
    # via ipython
jedi==0.19.2
    # via ipython
jinja2==3.1.6
    # via torch
joblib==1.4.2
    # via binance-trader-c1
    # via scikit-learn
kiwisolver==1.4.8
    # via matplotlib
lxml==5.3.1
    # via pandas-datareader
markupsafe==3.0.2
    # via jinja2
matplotlib==3.10.1
    # via binance-trader-c1
matplotlib-inline==0.1.7
    # via ipython
mpmath==1.3.0
    # via sympy
networkx==3.4.2
    # via torch
numpy==1.26.0
    # via binance-trader-c1
    # via contourpy
    # via empyrical
    # via matplotlib
    # via pandas
    # via scikit-learn
    # via scipy
    # via torchvision
nvidia-cublas-cu12==12.4.5.8
    # via nvidia-cudnn-cu12
    # via nvidia-cusolver-cu12
    # via torch
nvidia-cuda-cupti-cu12==12.4.127
    # via torch
nvidia-cuda-nvrtc-cu12==12.4.127
    # via torch
nvidia-cuda-runtime-cu12==12.4.127
    # via torch
nvidia-cudnn-cu12==9.1.0.70
    # via torch
nvidia-cufft-cu12==11.2.1.3
    # via torch
nvidia-curand-cu12==10.3.5.147
    # via torch
nvidia-cusolver-cu12==11.6.1.9
    # via torch
nvidia-cusparse-cu12==12.3.1.170
    # via nvidia-cusolver-cu12
    # via torch
nvidia-cusparselt-cu12==0.6.2
    # via torch
nvidia-nccl-cu12==2.21.5
    # via torch
nvidia-nvjitlink-cu12==12.4.127
    # via nvidia-cusolver-cu12
    # via nvidia-cusparse-cu12
    # via torch
nvidia-nvtx-cu12==12.4.127
    # via torch
packaging==24.2
    # via matplotlib
pandarallel==1.6.5
    # via binance-trader-c1
pandas==2.2.3
    # via binance-trader-c1
    # via empyrical
    # via pandarallel
    # via pandas-datareader
pandas-datareader==0.10.0
    # via empyrical
parso==0.8.4
    # via jedi
pexpect==4.9.0
    # via ipython
pillow==11.1.0
    # via matplotlib
    # via torchvision
prompt-toolkit==3.0.50
    # via ipython
psutil==7.0.0
    # via pandarallel
ptyprocess==0.7.0
    # via pexpect
pure-eval==0.2.3
    # via stack-data
pyarrow==19.0.1
    # via binance-trader-c1
pygments==2.19.1
    # via ipython
    # via ipython-pygments-lexers
pyparsing==3.2.3
    # via matplotlib
python-dateutil==2.9.0.post0
    # via matplotlib
    # via pandas
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via spot
requests==2.32.3
    # via pandas-datareader
scikit-learn==1.6.1
    # via binance-trader-c1
scipy==1.15.2
    # via empyrical
    # via scikit-learn
six==1.17.0
    # via python-dateutil
spot==0.1a0
    # via binance-trader-c1
stack-data==0.6.3
    # via ipython
sympy==1.13.1
    # via torch
tabulate==0.9.0
    # via binance-trader-c1
termcolor==2.5.0
    # via fire
threadpoolctl==3.6.0
    # via scikit-learn
torch==2.6.0
    # via binance-trader-c1
    # via torchaudio
    # via torchvision
torchaudio==2.6.0
    # via binance-trader-c1
torchvision==0.21.0
    # via binance-trader-c1
tqdm==4.67.1
    # via binance-trader-c1
traitlets==5.14.3
    # via ipython
    # via matplotlib-inline
triton==3.2.0
    # via torch
typing-extensions==4.12.2
    # via ipython
    # via torch
tzdata==2025.2
    # via pandas
urllib3==2.3.0
    # via requests
wcwidth==0.2.13
    # via prompt-toolkit
</file>

<file path="requirements.lock">
# generated by rye
# use `rye lock` or `rye sync` to update this lockfile
#
# last locked with the following flags:
#   pre: false
#   features: []
#   all-features: false
#   with-sources: false
#   generate-hashes: false
#   universal: false

-e file:.
asttokens==3.0.0
    # via stack-data
certifi==2025.1.31
    # via requests
charset-normalizer==3.4.1
    # via requests
contourpy==1.3.1
    # via matplotlib
cycler==0.12.1
    # via matplotlib
decorator==5.2.1
    # via ipython
dill==0.3.9
    # via pandarallel
empyrical==0.5.5
    # via binance-trader-c1
executing==2.2.0
    # via stack-data
fancytable==0.0.3
    # via binance-trader-c1
filelock==3.18.0
    # via torch
fire==0.7.0
    # via binance-trader-c1
fonttools==4.56.0
    # via matplotlib
fsspec==2025.3.0
    # via torch
idna==3.10
    # via requests
ipython==9.0.2
    # via binance-trader-c1
ipython-pygments-lexers==1.1.1
    # via ipython
jedi==0.19.2
    # via ipython
jinja2==3.1.6
    # via torch
joblib==1.4.2
    # via binance-trader-c1
    # via scikit-learn
kiwisolver==1.4.8
    # via matplotlib
lxml==5.3.1
    # via pandas-datareader
markupsafe==3.0.2
    # via jinja2
matplotlib==3.10.1
    # via binance-trader-c1
matplotlib-inline==0.1.7
    # via ipython
mpmath==1.3.0
    # via sympy
networkx==3.4.2
    # via torch
numpy==1.26.0
    # via binance-trader-c1
    # via contourpy
    # via empyrical
    # via matplotlib
    # via pandas
    # via scikit-learn
    # via scipy
    # via torchvision
nvidia-cublas-cu12==12.4.5.8
    # via nvidia-cudnn-cu12
    # via nvidia-cusolver-cu12
    # via torch
nvidia-cuda-cupti-cu12==12.4.127
    # via torch
nvidia-cuda-nvrtc-cu12==12.4.127
    # via torch
nvidia-cuda-runtime-cu12==12.4.127
    # via torch
nvidia-cudnn-cu12==9.1.0.70
    # via torch
nvidia-cufft-cu12==11.2.1.3
    # via torch
nvidia-curand-cu12==10.3.5.147
    # via torch
nvidia-cusolver-cu12==11.6.1.9
    # via torch
nvidia-cusparse-cu12==12.3.1.170
    # via nvidia-cusolver-cu12
    # via torch
nvidia-cusparselt-cu12==0.6.2
    # via torch
nvidia-nccl-cu12==2.21.5
    # via torch
nvidia-nvjitlink-cu12==12.4.127
    # via nvidia-cusolver-cu12
    # via nvidia-cusparse-cu12
    # via torch
nvidia-nvtx-cu12==12.4.127
    # via torch
packaging==24.2
    # via matplotlib
pandarallel==1.6.5
    # via binance-trader-c1
pandas==2.2.3
    # via binance-trader-c1
    # via empyrical
    # via pandarallel
    # via pandas-datareader
pandas-datareader==0.10.0
    # via empyrical
parso==0.8.4
    # via jedi
pexpect==4.9.0
    # via ipython
pillow==11.1.0
    # via matplotlib
    # via torchvision
prompt-toolkit==3.0.50
    # via ipython
psutil==7.0.0
    # via pandarallel
ptyprocess==0.7.0
    # via pexpect
pure-eval==0.2.3
    # via stack-data
pyarrow==19.0.1
    # via binance-trader-c1
pygments==2.19.1
    # via ipython
    # via ipython-pygments-lexers
pyparsing==3.2.3
    # via matplotlib
python-dateutil==2.9.0.post0
    # via matplotlib
    # via pandas
pytz==2025.2
    # via pandas
pyyaml==6.0.2
    # via spot
requests==2.32.3
    # via pandas-datareader
scikit-learn==1.6.1
    # via binance-trader-c1
scipy==1.15.2
    # via empyrical
    # via scikit-learn
six==1.17.0
    # via python-dateutil
spot==0.1a0
    # via binance-trader-c1
stack-data==0.6.3
    # via ipython
sympy==1.13.1
    # via torch
tabulate==0.9.0
    # via binance-trader-c1
termcolor==2.5.0
    # via fire
threadpoolctl==3.6.0
    # via scikit-learn
torch==2.6.0
    # via binance-trader-c1
    # via torchaudio
    # via torchvision
torchaudio==2.6.0
    # via binance-trader-c1
torchvision==0.21.0
    # via binance-trader-c1
tqdm==4.67.1
    # via binance-trader-c1
traitlets==5.14.3
    # via ipython
    # via matplotlib-inline
triton==3.2.0
    # via torch
typing-extensions==4.12.2
    # via ipython
    # via torch
tzdata==2025.2
    # via pandas
urllib3==2.3.0
    # via requests
wcwidth==0.2.13
    # via prompt-toolkit
</file>

<file path="develop/src/trainer/models/backbones/stack_backbone_v1.py">
from .backbone_v1 import BackboneV1, identity
import math
import torch.nn as nn
from develop.src.trainer.modules.block_1d import NORMS
from develop.src.trainer.modules import acts


class StackBackboneV1(BackboneV1):
    def __init__(
        self,
        in_channels,
        n_assets,
        n_blocks=3,
        n_block_layers=6,
        growth_rate=12,
        dropout=0.0,
        channel_reduction=0.5,
        activation="relu",
        normalization="bn",
        seblock=True,
        sablock=True,
    ):
        super(BackboneV1, self).__init__()
        self.in_channels = in_channels
        self.n_assets = n_assets

        self.n_blocks = n_blocks
        self.n_block_layers = n_block_layers
        self.growth_rate = growth_rate
        self.dropout = dropout
        self.channel_reduction = channel_reduction

        self.activation = activation
        self.normalization = normalization
        self.seblock = seblock
        self.sablock = sablock

        # Build first_conv
        out_channels = 4 * growth_rate
        self.first_conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=3,
            stride=1,
            padding=1,
            bias=False,
        )

        # Build blocks
        in_channels = out_channels

        blocks = []
        for idx in range(n_blocks):

            blocks.append(
                self._build_block(
                    in_channels=in_channels,
                    use_transition_block=True if idx != n_blocks - 1 else False,
                )
            )

            # mutate in_channels for next block
            in_channels = self._compute_out_channels(
                in_channels=in_channels,
                use_transition_block=True if idx != n_blocks - 1 else False,
            )

        self.blocks = nn.Sequential(*blocks)

        # Last layers
        self.norm = identity
        if normalization is not None:
            self.norm = NORMS[normalization.upper()](num_channels=in_channels)

        self.act = getattr(acts, activation)
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)

        self.embed = nn.Embedding(n_assets, in_channels)
        self.pred_fc = nn.Linear(in_channels, 4)
        self.last_sigmoid = nn.Sigmoid()

        # Initialize
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                n = m.kernel_size[0] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(1.0 / n))

            elif isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)

                if m.bias is not None:
                    m.bias.data.zero_()

            elif isinstance(m, nn.Linear):
                if m.bias is not None:
                    m.bias.data.zero_()

    def forward(self, x, id):
        B, _, _ = x.size()
        out = self.blocks(self.first_conv(x))
        out = self.global_avg_pool(self.act(self.norm(out))).view(B, -1)

        preds = self.pred_fc(out) + (out * self.embed(id)).sum(axis=-1, keepdim=True)

        return (
            preds[:, 0],
            self.last_sigmoid(preds[:, 1]),
            preds[:, 2],
            self.last_sigmoid(preds[:, 3]),
        )
</file>

<file path="develop/src/trainer/models/utils.py">
import os
import torch
from glob import glob
import torch.nn as nn
from logging import getLogger
import pandas as pd

logger = getLogger("model")


def save_model(model, dir, epoch):
    os.makedirs(dir, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(dir, f"checkpoint-{epoch}.ckpt"))

    print(f"[+] Model is saved. Epoch: {epoch}")


def load_model(model, dir, load_epoch=None, strict=True, device="cuda"):
    params_to_load = {}
    if device == "cpu":
        params_to_load["map_location"] = torch.device("cpu")

    if not os.path.isdir(dir):
        print("[!] Load is failed")
        return -1

    check_points = glob(os.path.join(dir, "checkpoint-*.ckpt"))
    check_points = sorted(
        check_points,
        key=lambda x: int(
            x.split("/")[-1].replace("checkpoint-", "").replace(".ckpt", "")
        ),
    )

    # skip if there are no checkpoints
    if len(check_points) == 0:
        print("[!] Load is failed")
        return -1

    check_point = check_points[-1]

    # If load_epoch has value
    if load_epoch is not None:

        model.load_state_dict(
            torch.load(
                os.path.join(dir, f"checkpoint-{load_epoch}.ckpt"), **params_to_load
            ),
            strict=strict,
        )
        print("[+] Model is loaded")
        print(f"[+] Epoch: {load_epoch}")
        logger.info(f"[+] Model is loaded | Epoch: {load_epoch}")

        return load_epoch

    model.load_state_dict(torch.load(check_point, **params_to_load), strict=strict)
    last_epoch = int(
        check_point.split("/")[-1].replace("checkpoint-", "").replace(".ckpt", "")
    )

    print("[+] Model is loaded")
    print(f"[+] Epoch: {last_epoch}")
    logger.info(f"[+] Model is loaded | Epoch: {last_epoch}")

    return last_epoch


def weights_init(m):
    if isinstance(m, (nn.Conv1d, nn.Linear, nn.BatchNorm1d, nn.GroupNorm)):
        nn.init.normal_(m.weight.data, 0.0, 0.02)
        if hasattr(m, "bias"):
            if m.bias is not None:
                nn.init.constant_(m.bias.data, 0)


def inverse_preprocess_data(data, scaler):
    processed_data = pd.DataFrame(
        scaler.inverse_transform(data), index=data.index, columns=data.columns
    )

    return processed_data
</file>

<file path="develop/Makefile">
GPU_TYPE=NVIDIA
CONTAINER_NAME=docker ps | grep binance_trader_develop:latest | cut -d ' ' -f 1
CONTAINER_NAMES=docker ps -a | grep binance_trader_develop:latest | cut -d ' ' -f 1


_build_container:
ifeq ($(shell echo $(GPU_TYPE) | tr [a-z] [A-Z]),NVIDIA)
	@docker build dockerfiles -t binance_trader_develop:latest --build-arg BASE_IMAGE=pytorch/pytorch:latest
else ifeq ($(shell echo $(GPU_TYPE) | tr [a-z] [A-Z]),AMD)
	@docker build dockerfiles -t amd_binance_trader_develop:latest --build-arg BASE_IMAGE=rocm/pytorch:latest
else
	$(error Unexpected GPU_TYPE: $(GPU_TYPE))
endif

_run_if_not_exists:
ifeq ($(shell $(CONTAINER_NAME)),)
	$(MAKE) run
endif
	@echo

rm:
	@docker rm -f $(shell $(CONTAINER_NAMES)) 2> /dev/null || echo

run: _build_container rm
ifeq ($(shell echo $(GPU_TYPE) | tr [a-z] [A-Z]),NVIDIA)
	@docker run -d --gpus all --shm-size=16gb -v $(shell pwd):/app binance_trader_develop:latest tail -f /dev/null 2> /dev/null || ($(MAKE) rm ; docker run -d --shm-size=16gb -v $(shell pwd):/app binance_trader_develop:latest tail -f /dev/null)
else ifeq ($(shell echo $(GPU_TYPE) | tr [a-z] [A-Z]),AMD)
	@docker run -d --network=host --device=/dev/kfd --device=/dev/dri --group-add video --ipc=host --shm-size=16gb -v $(shell pwd):/app amd_binance_trader_develop:latest tail -f /dev/null
else
	$(error Unexpected GPU_TYPE: $(GPU_TYPE))
endif

bash: _run_if_not_exists
	docker exec -it $(shell $(CONTAINER_NAME)) bash

download_kaggle_data: _run_if_not_exists
	docker exec -it $(shell $(CONTAINER_NAME)) python -m rawdata_builder.download_kaggle_data $(ARGS)

build_rawdata: _run_if_not_exists
	docker exec -it $(shell $(CONTAINER_NAME)) python -m rawdata_builder.build_rawdata $(ARGS)

build_dataset: _run_if_not_exists
	docker exec -it $(shell $(CONTAINER_NAME)) python -m dataset_builder.build_dataset build $(ARGS)

train: _run_if_not_exists
	docker exec -it $(shell $(CONTAINER_NAME)) python -m trainer.models.predictor_v1 train --mode=train $(ARGS)

generate: _run_if_not_exists
	docker exec -it $(shell $(CONTAINER_NAME)) python -m trainer.models.predictor_v1 generate --mode=test $(ARGS)

review: _run_if_not_exists
	docker exec -it $(shell $(CONTAINER_NAME)) python -m reviewer.reviewer_v1 run --in_shell True $(ARGS)

display_review: _run_if_not_exists
	docker exec -it $(shell $(CONTAINER_NAME)) python -m reviewer.reviewer_v1 display --in_shell True $(ARGS)
</file>

<file path="services/dockerfiles/requirements.txt">
pandarallel
fire
black
joblib
ipython
numpy
pandas
matplotlib
fastparquet
pyarrow
scikit-learn==0.23.2
tabulate
sqlalchemy
psycopg2-binary
ccxt==1.41.63
werkzeug
fancytable
</file>

<file path="services/k8s/deployments-template/database-template.yml">
kind: Deployment
apiVersion: apps/v1
metadata:
  name: database-deployment
  namespace: dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      name: database
      labels:
        app: database
    spec:
      containers:
        - name: database
          image: postgres:latest
          command: []
          imagePullPolicy: "Never"
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_DB
              value: postgres
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: password
          securityContext:
            runAsUser: 0
          resources:
            requests:
              cpu: 250m
              memory: 450Mi
</file>

<file path="services/src/database/database.py">
import os
import time
from sqlalchemy import create_engine
from sqlalchemy.exc import OperationalError
from sqlalchemy.orm import scoped_session, sessionmaker
from sqlalchemy.ext.declarative import declarative_base


ENGINE = create_engine(
    f"postgresql://{os.environ['POSTGRES_USER']}:{os.environ['POSTGRES_PASSWORD']}@{os.environ['POSTGRES_HOST']}/{os.environ['POSTGRES_DB']}",
    convert_unicode=False,
    connect_args={"connect_timeout": 60},
)

SESSION = scoped_session(sessionmaker(autocommit=False, autoflush=False, bind=ENGINE))

BASE = declarative_base()
BASE.query = SESSION.query_property()


def init():
    from database import models

    while True:
        try:
            BASE.metadata.drop_all(ENGINE)
            BASE.metadata.create_all(ENGINE)
            break
        except OperationalError:
            time.sleep(5)


def wait_connection():
    while True:
        try:
            ENGINE.connect()
            break
        except OperationalError:
            time.sleep(5)
</file>

<file path="services/src/trader/utils.py">
import pandas as pd
from IPython.display import display, display_markdown


def nan_to_zero(value):
    if str(value) in ("nan", "None"):
        return 0

    return value


def load_parquet(path):
    return pd.read_parquet(path)


def compute_quantile(x, bins):
    if str(x) in ("None", "nan"):
        x = 0

    for idx in range(len(bins) - 1):
        if bins[idx] < x <= bins[idx + 1]:
            return idx

    raise RuntimeError("unreachable")
</file>

<file path="develop/src/trainer/modules/block_1d/dense_block.py">
import torch
import torch.nn as nn
import torch.nn.functional as F
from .norms import NORMS
from .seblock import SEBlock
from .self_attention import SelfAttention1d
from develop.src.trainer.modules import acts


def identity(x):
    return x


class BottleneckBlock(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        planes: int = None,
        dropout: float = 0.0,
        activation: str = "relu",
        normalization: str = "bn",
        seblock: bool = False,
        sablock: bool = False,
    ):
        super(BottleneckBlock, self).__init__()
        if planes is None:
            planes = out_channels * 4

        bias = True
        if normalization is not None:
            bias = False

        if activation == "selu":
            bias = False

        # Define blocks
        self.norm1 = identity
        if normalization is not None:
            self.norm1 = NORMS[normalization.upper()](num_channels=in_channels)

        self.conv1 = nn.Conv1d(
            in_channels, planes, kernel_size=1, stride=1, padding=0, bias=bias
        )

        self.norm2 = identity
        if normalization is not None:
            self.norm2 = NORMS[normalization.upper()](num_channels=planes)

        self.conv2 = nn.Conv1d(
            planes, out_channels, kernel_size=3, stride=1, padding=1, bias=bias
        )

        self.act = getattr(acts, activation)
        if activation == "selu":
            self.dropout = nn.Sequential(
                *[nn.AlphaDropout(dropout / 5), nn.Dropout2d(dropout)]
            )
        else:
            self.dropout = nn.Sequential(*[nn.Dropout(dropout), nn.Dropout2d(dropout)])

        # Optional blocks
        self.seblock = None
        if seblock is True:
            self.seblock = SEBlock(in_channels=in_channels, activation=activation)

        self.sablock = None
        if sablock is True:
            self.sablock = SelfAttention1d(in_channels=in_channels)

    def forward(self, x: torch.Tensor):

        after_norm = self.norm1(x)
        if self.seblock is not None:
            after_norm = self.seblock(after_norm)

        after_act = self.act(after_norm)
        if self.sablock is not None:
            after_act = self.sablock(after_act)

        out = self.conv1(self.dropout(after_act))
        out = self.conv2(self.dropout(self.act(self.norm2(out))))

        return torch.cat([x, out], dim=1)


class TransitionBlock(nn.Module):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        dropout: float = 0.0,
        activation: str = "relu",
        normalization: str = "bn",
    ):
        super(TransitionBlock, self).__init__()
        bias = True
        if normalization is not None:
            bias = False

        if activation == "selu":
            bias = False

        self.norm = identity
        if normalization is not None:
            self.norm = NORMS[normalization.upper()](num_channels=in_channels)

        self.act = getattr(acts, activation)
        self.conv = nn.Conv1d(
            in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=bias
        )
        if activation == "selu":
            self.dropout = nn.Sequential(
                *[nn.AlphaDropout(dropout / 5), nn.Dropout2d(dropout)]
            )
        else:
            self.dropout = nn.Sequential(*[nn.Dropout(dropout), nn.Dropout2d(dropout)])

    def forward(self, x: torch.Tensor):
        out = self.conv(self.dropout(self.act(self.norm(x))))
        return F.avg_pool1d(out, 2)


class DenseBlock(nn.Module):
    def __init__(
        self,
        n_layers: int,
        in_channels: int,
        growth_rate: int,
        planes: int = None,
        dropout: float = 0.0,
        activation: str = "relu",
        normalization: str = "bn",
        seblock: bool = False,
        sablock: bool = False,
    ):
        super(DenseBlock, self).__init__()

        layers = [
            BottleneckBlock(
                in_channels=in_channels + (idx * growth_rate),
                out_channels=growth_rate,
                planes=planes,
                dropout=dropout,
                activation=activation,
                normalization=normalization,
                seblock=seblock if idx == (n_layers - 1) else False,
                sablock=sablock if idx == (n_layers - 1) else False,
            )
            for idx in range(n_layers)
        ]
        self.layers = nn.Sequential(*layers)

    def forward(self, x: torch.Tensor):
        return self.layers(x)
</file>

<file path="services/src/common_utils_svc/common_utils_svc.py">
import os
import json
import pyarrow.parquet as pq
import pyarrow as pa
from pathlib import Path


class Position:
    def __init__(
        self,
        asset,
        side,
        qty,
        entry_price,
        entry_at,
        n_updated=0,
        profit=None,
        is_exited=False,
    ):
        self.asset = asset
        self.side = side
        self.qty = qty
        self.entry_price = entry_price
        self.entry_at = entry_at
        self.n_updated = n_updated
        self.profit = profit
        self.is_exited = is_exited

    def __getitem__(self, key):
        return getattr(self, key)

    def __setitem__(self, key, value):
        setattr(self, key, value)

    def __repr__(self):
        return f"Position(asset={self.asset}, side={self.side}, qty={self.qty:.3f}, entry_price={self.entry_price:.2f})"

    def __str__(self):
        return self.__repr__()


def make_dirs(dirs):
    for dir in dirs:
        os.makedirs(dir, exist_ok=True)


def load_text(path):
    with open(path, "r") as f:
        text = f.read().splitlines()

    return text


def load_json(path):
    with open(path, "r") as f:
        loaded = json.load(f)

    return loaded


def to_parquet(df, path, compression="zstd"):
    pq.write_table(table=pa.Table.from_pandas(df), where=path, compression=compression)


def get_filename_by_path(path):
    return Path(path).stem.split(".")[0]


def to_abs_path(file, relative_path):
    return os.path.normpath(
        os.path.join(os.path.dirname(os.path.abspath(file)), relative_path)
    )


def initialize_main_logger():
    import logging
    import sys

    handler = logging.StreamHandler(sys.stdout)
    logging.basicConfig(
        level="INFO",
        handlers=[handler],
        format="%(asctime)s %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )


def initialize_trader_logger():
    from handler import SlackHandler
    import logging
    import sys

    handler = logging.StreamHandler(sys.stdout)
    slack_handler = SlackHandler()

    logging.basicConfig(
        level="INFO",
        handlers=[handler, slack_handler],
        format="%(asctime)s %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
</file>

<file path="services/src/config/__init__.py">
import os
import pandas as pd
from dataclasses import dataclass
from common_utils_svc import load_json
from werkzeug.utils import cached_property


@dataclass
class Config:
    @property
    def ENV(self):
        return os.environ

    @property
    def EXCHANGE_API_KEY(self):
        return self.ENV["EXCHANGE_API_KEY"]

    @property
    def EXCHANGE_SECRET_KEY(self):
        return self.ENV["EXCHANGE_SECRET_KEY"]

    @property
    def WEBHOOK_URL(self):
        return self.ENV["WEBHOOK_URL"]

    @cached_property
    def TEST_MODE(self):
        test_mode = self.ENV["TEST_MODE"]
        if test_mode in ("true", "True"):
            test_mode = True
        if test_mode in ("false", "False"):
            test_mode = False

        return test_mode

    @cached_property
    def DATASET_PARAMS(self):
        return load_json(
            f"/app/dev/experiments/{self.ENV['EXP_NAME']}/dataset_params.json"
        )

    @property
    def EXP_DIR(self):
        return f"/app/dev/experiments/{self.ENV['EXP_NAME']}"

    @cached_property
    def EXP_PARAMS(self):
        return load_json(
            f"/app/dev/experiments/{self.ENV['EXP_NAME']}/trainer_params.json"
        )

    @cached_property
    def EXP_MODEL_PARAMS(self):
        return self.EXP_PARAMS["model_config"]

    @cached_property
    def EXP_DATA_PARAMS(self):
        data_params = self.EXP_PARAMS["data_config"]
        data_params.pop("checkpoint_dir")
        data_params.pop("generate_output_dir")

        return data_params

    @cached_property
    def REPORT_PARAMS(self):
        return load_json(
            f"/app/dev/experiments/{self.ENV['EXP_NAME']}/reports/params_{self.ENV['REPORT_PREFIX']}_{self.ENV['REPORT_ID']}_{self.ENV['REPORT_BASE_CURRENCY']}.json"
        )

    @cached_property
    def PREDICTION_ABS_BINS(self):
        bins = pd.read_parquet(
            f"/app/dev/experiments/{self.ENV['EXP_NAME']}/generated_output/prediction_abs_bins.parquet.zstd"
        )
        bins.columns = bins.columns.map(lambda x: x.replace("-", "/"))
        return bins

    @cached_property
    def PROBABILITY_BINS(self):
        bins = pd.read_parquet(
            f"/app/dev/experiments/{self.ENV['EXP_NAME']}/generated_output/probability_bins.parquet.zstd"
        )
        bins.columns = bins.columns.map(lambda x: x.replace("-", "/"))
        return bins

    @cached_property
    def TRADABLE_COINS(self):
        return [
            tradable_coin.replace("-", "/")
            for tradable_coin in self.REPORT_PARAMS["tradable_coins"]
        ]

    @property
    def BASE_CURRENCY(self):
        return self.REPORT_PARAMS["base_currency"]

    @property
    def LEVERAGE(self):
        return int(self.ENV["LEVERAGE"])


CFG = Config()
</file>

<file path="develop/src/rawdata_builder/build_rawdata.py">
import os
import pandas as pd
from glob import glob
from tqdm import tqdm
from develop.src.common_utils_dev import (
    make_dirs,
    load_text,
    get_filename_by_path,
    to_parquet,
    get_filename_by_path,
    to_abs_path,
)


CONFIG = {
    "raw_spot_rawdata_dir": to_abs_path(
        __file__, "../../storage/dataset/rawdata/raw/spot/"
    ),
    "raw_future_rawdata_dir": to_abs_path(
        # __file__, "../../storage/dataset/rawdata/raw/future/"
        __file__, "../../storage/dataset/rawdata/raw/spot/"
    ),
    "cleaned_rawdata_store_dir": to_abs_path(
        __file__, "../../storage/dataset/rawdata/cleaned/"
    ),
    "candidate_assets_path": to_abs_path(__file__, "./candidate_assets.txt"),
    "query_min_start_dt": "2018-01-01",
    "boundary_dt_must_have_data": "2019-09-01",
}


def build_rawdata(
    raw_spot_rawdata_dir=CONFIG["raw_spot_rawdata_dir"],
    raw_future_rawdata_dir=CONFIG["raw_future_rawdata_dir"],
    cleaned_rawdata_store_dir=CONFIG["cleaned_rawdata_store_dir"],
    candidate_assets_path=CONFIG["candidate_assets_path"],
    query_min_start_dt=CONFIG["query_min_start_dt"],
    boundary_dt_must_have_data=CONFIG["boundary_dt_must_have_data"],
):
    make_dirs([cleaned_rawdata_store_dir])
    candidate_assets = load_text(path=candidate_assets_path)

    count_files = 0
    for candidate_asset in tqdm(candidate_assets):
        spot_file_path = os.path.join(
            raw_spot_rawdata_dir, f"{candidate_asset}.parquet"
        )
        future_file_path = os.path.join(
            raw_future_rawdata_dir, f"{candidate_asset}.parquet"
        )

        spot_df = pd.read_parquet(spot_file_path)[
            ["open", "high", "low", "close"]
        ].sort_index()
        future_df = pd.read_parquet(future_file_path)[
            ["open", "high", "low", "close"]
        ].sort_index()

        df = pd.concat([spot_df[spot_df.index < future_df.index[0]], future_df])
        df = df.resample("1T").ffill()

        df = df[query_min_start_dt:]
        if df.index[0] > pd.Timestamp(boundary_dt_must_have_data):
            print(f"[!] Skiped: {candidate_asset}")
            continue

        assert not df.isnull().any().any()
        assert len(df.index.unique()) == len(df.index)

        store_filename = candidate_asset + ".parquet.zstd"
        df.index = df.index.tz_localize("utc")
        to_parquet(df=df, path=os.path.join(cleaned_rawdata_store_dir, store_filename))
        count_files += 1

    print(f"[+] Built rawdata: {count_files}")


if __name__ == "__main__":
    import fire

    fire.Fire(build_rawdata)
</file>

<file path="develop/src/trainer/models/criterions.py">
import torch.nn as nn


CRITERIONS = {"l1": nn.L1Loss, "l2": nn.MSELoss, "bce": nn.BCELoss}
</file>

<file path="services/k8s/deployments-template/data_collector-template.yml">
kind: Deployment
apiVersion: apps/v1
metadata:
  name: data-collector-deployment
  namespace: dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: data-collector
  template:
    metadata:
      name: data-collector
      labels:
        app: data-collector
    spec:
      restartPolicy: Always
      containers:
        - name: data-collector
          image: binance_trader_services:latest
          command: ["python", "-m", "data_collector.data_collector", "run"]
          imagePullPolicy: "Never"
          env:
            - name: POSTGRES_HOST
              value: database.dev.svc.cluster.local         
            - name: POSTGRES_DB
              value: postgres
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: password
            - name: LEVERAGE
              value: {{LEVERAGE}}
            - name: EXP_NAME
              value: {{EXP_NAME}}
            - name: REPORT_PREFIX
              value: {{REPORT_PREFIX}}
            - name: REPORT_BASE_CURRENCY
              value: {{REPORT_BASE_CURRENCY}}
            - name: REPORT_ID
              value: {{REPORT_ID}}
            - name: EXCHANGE_API_KEY
              valueFrom:
                secretKeyRef:
                  name: market-secret
                  key: api_key
            - name: EXCHANGE_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: market-secret
                  key: secret_key
            - name: TEST_MODE
              valueFrom:
                secretKeyRef:
                  name: market-secret
                  key: test_mode
            - name: WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: market-secret
                  key: webhook_url
          volumeMounts:
            - name: svc-codes
              mountPath: "/app/src"
            - name: dev-exps
              mountPath: "/app/dev/experiments"
          securityContext:
            runAsUser: 0
          resources:
            requests:
              cpu: 250m
              memory: 450Mi
      volumes:
        - name: svc-codes
          hostPath:
            path: {{PWD}}/src
        - name: dev-exps
          hostPath:
            path: {{PARENT_PWD}}/develop/storage/experiments
</file>

<file path="develop/src/dataset_builder/build_dataset.py">
import os
import gc
import json
from glob import glob
from typing import Optional, List
import pandas as pd
import numpy as np
from tqdm import tqdm
from functools import partial
from itertools import combinations
from sklearn import preprocessing
import joblib
from develop.src.common_utils_dev import make_dirs, to_parquet, to_abs_path, get_filename_by_path
from pandarallel import pandarallel
from dataclasses import dataclass


CONFIG = {
    "rawdata_dir": to_abs_path(__file__, "../../storage/dataset/rawdata/cleaned/"),
    "data_store_dir": to_abs_path(__file__, "../../storage/dataset/dataset/v001/"),
    "lookahead_window": 30,
    "train_ratio": 0.80,
    "scaler_type": "StandardScaler",
    "winsorize_threshold": 6,
    "query_min_start_dt": "2018-06-01",
}
OHLC = ["open", "high", "low", "close"]


@dataclass
class DatasetBuilder:
    # Defined in running code.
    # Need to give below parameters when build in trader
    tradable_coins: Optional[List] = None
    feature_columns: Optional[List] = None
    feature_scaler: Optional[preprocessing.StandardScaler] = None
    label_scaler: Optional[preprocessing.StandardScaler] = None

    def build_rawdata(self, file_names, query_min_start_dt):
        def _load_rawdata_row(file_name):
            rawdata = pd.read_parquet(file_name)[OHLC]
            rawdata.index = pd.to_datetime(rawdata.index)
            rawdata = rawdata[query_min_start_dt:]

            return rawdata

        rawdata = {}
        for file_name in tqdm(file_names):
            coin = get_filename_by_path(file_name)
            rawdata[coin] = _load_rawdata_row(file_name=file_name)

        rawdata = pd.concat(rawdata, axis=1).sort_index()

        self.tradable_coins = sorted(rawdata.columns.levels[0].tolist())

        return rawdata[self.tradable_coins]

    def _build_feature_by_rawdata_row(self, rawdata_row):
        returns_1320m = (
            rawdata_row[OHLC]
            .pct_change(1320, fill_method=None)
            .rename(columns={key: key + "_return(1320)" for key in OHLC})
        ).dropna()

        madiv_1320m = (
            (
                rawdata_row[OHLC]
                .rolling(1320)
                .mean()
                .rename(columns={key: key + "_madiv(1320)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        returns_600m = (
            (
                rawdata_row[OHLC]
                .pct_change(600, fill_method=None)
                .rename(columns={key: key + "_return(600)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        madiv_600m = (
            (
                rawdata_row[OHLC]
                .rolling(600)
                .mean()
                .rename(columns={key: key + "_madiv(600)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        returns_240m = (
            (
                rawdata_row[OHLC]
                .pct_change(240, fill_method=None)
                .rename(columns={key: key + "_return(240)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        madiv_240m = (
            (
                rawdata_row[OHLC]
                .rolling(240)
                .mean()
                .rename(columns={key: key + "_madiv(240)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        returns_120m = (
            (
                rawdata_row[OHLC]
                .pct_change(120, fill_method=None)
                .rename(columns={key: key + "_return(120)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        madiv_120m = (
            (
                rawdata_row[OHLC]
                .rolling(120)
                .mean()
                .rename(columns={key: key + "_madiv(120)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        returns_1m = (
            (
                rawdata_row[OHLC]
                .pct_change(1, fill_method=None)
                .rename(columns={key: key + "_return(1)" for key in OHLC})
            )
            .dropna()
            .reindex(returns_1320m.index)
        )

        inner_changes = []
        for column_pair in sorted(list(combinations(OHLC, 2))):
            inner_changes.append(
                rawdata_row[list(column_pair)]
                .pct_change(1, axis=1, fill_method=None)[column_pair[-1]]
                .rename("_".join(column_pair) + "_change")
            )

        inner_changes = pd.concat(inner_changes, axis=1).reindex(returns_1320m.index)

        feature = pd.concat(
            [
                returns_1320m,
                madiv_1320m,
                returns_600m,
                madiv_600m,
                returns_240m,
                madiv_240m,
                returns_120m,
                madiv_120m,
                returns_1m,
                inner_changes,
            ],
            axis=1,
        ).sort_index()

        return feature

    def build_features(self, rawdata):
        features = {}
        for coin in tqdm(self.tradable_coins):
            features[coin] = self._build_feature_by_rawdata_row(
                rawdata_row=rawdata[coin]
            )

        features = pd.concat(features, axis=1).sort_index()[self.tradable_coins]

        if self.feature_columns is None:
            self.feature_columns = features.columns
            return features

        return features[self.feature_columns]

    def build_scaler(self, data, scaler_type):
        scaler = getattr(preprocessing, scaler_type)()
        scaler.fit(data)

        return scaler

    def preprocess_features(self, features, winsorize_threshold):
        assert self.feature_scaler is not None

        features = pd.DataFrame(
            self.feature_scaler.transform(features),
            index=features.index,
            columns=features.columns,
        )

        if winsorize_threshold is not None:
            features = (
                features.clip(-winsorize_threshold, winsorize_threshold)
                / winsorize_threshold
            )

        return features

    def preprocess_labels(self, labels, winsorize_threshold):
        assert self.label_scaler is not None

        labels = pd.DataFrame(
            self.label_scaler.transform(labels),
            index=labels.index,
            columns=labels.columns,
        )

        if winsorize_threshold is not None:
            labels = (
                labels.clip(-winsorize_threshold, winsorize_threshold)
                / winsorize_threshold
            )

        return labels

    def _build_label(self, rawdata_row, lookahead_window):
        # build fwd_return(window)
        pricing = rawdata_row["open"].sort_index()
        fwd_return = (
            pricing.pct_change(lookahead_window, fill_method=None)
            .shift(-lookahead_window - 1)
            .rename(f"fwd_return({lookahead_window})")
            .sort_index()
        )[: -lookahead_window - 1]

        return fwd_return

    def build_labels(self, rawdata, lookahead_window):
        labels = []
        for coin in tqdm(self.tradable_coins):
            labels.append(
                self._build_label(
                    rawdata_row=rawdata[coin], lookahead_window=lookahead_window
                ).rename(coin)
            )

        labels = pd.concat(labels, axis=1).sort_index()[self.tradable_coins]

        return labels

    def store_artifacts(
        self,
        features,
        labels,
        pricing,
        feature_scaler,
        label_scaler,
        train_ratio,
        params,
        data_store_dir,
    ):
        # Make dirs
        train_data_store_dir = os.path.join(data_store_dir, "train")
        test_data_store_dir = os.path.join(data_store_dir, "test")
        make_dirs([train_data_store_dir, test_data_store_dir])

        # Store params
        joblib.dump(feature_scaler, os.path.join(data_store_dir, "feature_scaler.pkl"))
        joblib.dump(label_scaler, os.path.join(data_store_dir, "label_scaler.pkl"))

        with open(os.path.join(data_store_dir, "dataset_params.json"), "w") as f:
            json.dump(params, f)

        print(f"[+] Metadata is stored")

        # Store dataset
        boundary_index = int(len(features.index) * train_ratio)

        for file_name, data in [
            ("X.parquet.zstd", features),
            ("Y.parquet.zstd", labels),
            ("pricing.parquet.zstd", pricing),
        ]:
            to_parquet(
                df=data.iloc[:boundary_index],
                path=os.path.join(train_data_store_dir, file_name),
            )

            to_parquet(
                df=data.iloc[boundary_index:],
                path=os.path.join(test_data_store_dir, file_name),
            )

        print(f"[+] Dataset is stored")

    def build(
        self,
        rawdata_dir=CONFIG["rawdata_dir"],
        data_store_dir=CONFIG["data_store_dir"],
        lookahead_window=CONFIG["lookahead_window"],
        train_ratio=CONFIG["train_ratio"],
        scaler_type=CONFIG["scaler_type"],
        winsorize_threshold=CONFIG["winsorize_threshold"],
        query_min_start_dt=CONFIG["query_min_start_dt"],
    ):
        assert scaler_type in ("RobustScaler", "StandardScaler")
        pandarallel.initialize()

        # Make dirs
        make_dirs([data_store_dir])

        # Set file_names
        file_names = sorted(glob(os.path.join(rawdata_dir, "*")))
        assert len(file_names) != 0

        # Build rawdata
        rawdata = self.build_rawdata(
            file_names=file_names, query_min_start_dt=query_min_start_dt
        )
        gc.collect()

        # Build features
        features = self.build_features(rawdata=rawdata)
        self.feature_scaler = self.build_scaler(data=features, scaler_type=scaler_type)
        features = self.preprocess_features(
            features=features, winsorize_threshold=winsorize_threshold
        )
        gc.collect()

        # build labels
        labels = self.build_labels(rawdata=rawdata, lookahead_window=lookahead_window)
        self.label_scaler = self.build_scaler(data=labels, scaler_type=scaler_type)
        labels = self.preprocess_labels(
            labels=labels, winsorize_threshold=winsorize_threshold
        )
        gc.collect()

        # Masking with common index
        # common_index = (features.index & labels.index).sort_values()
        common_index = pd.Index(np.intersect1d(features.index, labels.index)).sort_values()


        features = features.reindex(common_index)
        labels = labels.reindex(common_index)
        pricing = rawdata.reindex(common_index)

        params = {
            "lookahead_window": lookahead_window,
            "train_ratio": train_ratio,
            "scaler_type": scaler_type,
            "features_columns": features.columns.tolist(),
            "labels_columns": labels.columns.tolist(),
            "tradable_coins": self.tradable_coins,
            "winsorize_threshold": winsorize_threshold,
            "query_min_start_dt": query_min_start_dt,
        }

        # Store Artifacts
        self.store_artifacts(
            features=features,
            labels=labels,
            pricing=pricing,
            feature_scaler=self.feature_scaler,
            label_scaler=self.label_scaler,
            train_ratio=train_ratio,
            params=params,
            data_store_dir=data_store_dir,
        )


if __name__ == "__main__":
    import fire

    fire.Fire(DatasetBuilder)
</file>

<file path="services/k8s/deployments-template/trader-template.yml">
kind: Deployment
apiVersion: apps/v1
metadata:
  name: trader-deployment
  namespace: dev
spec:
  replicas: 1
  selector:
    matchLabels:
      app: trader
  template:
    metadata:
      name: trader
      labels:
        app: trader
    spec:
      restartPolicy: Always
      containers:
        - name: trader
          image: binance_trader_services:latest
          command: ["python", "-m", "trader.trader_v1", "run"]
          imagePullPolicy: "Never"
          env:
            - name: POSTGRES_HOST
              value: database.dev.svc.cluster.local         
            - name: POSTGRES_DB
              value: postgres
            - name: POSTGRES_USER
              value: postgres
            - name: POSTGRES_PASSWORD
              value: password
            - name: LEVERAGE
              value: {{LEVERAGE}}
            - name: EXP_NAME
              value: {{EXP_NAME}}
            - name: REPORT_PREFIX
              value: {{REPORT_PREFIX}}
            - name: REPORT_BASE_CURRENCY
              value: {{REPORT_BASE_CURRENCY}}
            - name: REPORT_ID
              value: {{REPORT_ID}}
            - name: EXCHANGE_API_KEY
              valueFrom:
                secretKeyRef:
                  name: market-secret
                  key: api_key
            - name: EXCHANGE_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: market-secret
                  key: secret_key
            - name: TEST_MODE
              valueFrom:
                secretKeyRef:
                  name: market-secret
                  key: test_mode
            - name: WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: market-secret
                  key: webhook_url
          volumeMounts:
            - name: svc-codes
              mountPath: "/app/src"
            - name: dev-codes
              mountPath: "/app/dev/src"
            - name: dev-exps
              mountPath: "/app/dev/experiments"
            - name: svc-trader-artifacts
              mountPath: "/app/storage/trader"
          securityContext:
            runAsUser: 0
          resources:
            requests:
              cpu: 850m
              memory: 1000Mi
      volumes:
        - name: svc-codes
          hostPath:
            path: {{PWD}}/src
        - name: dev-codes
          hostPath:
            path: {{PARENT_PWD}}/develop/src
        - name: dev-exps
          hostPath:
            path: {{PARENT_PWD}}/develop/storage/experiments
        - name: svc-trader-artifacts
          hostPath:
            path: {{PWD}}/storage/trader
</file>

<file path="services/src/handler/slack_handler.py">
import logging
import requests
from config import CFG


class SlackHandler(logging.StreamHandler):
    def __init__(self):
        super(SlackHandler, self).__init__()
        self.url = CFG.WEBHOOK_URL

    def emit(self, record):
        msg = self.format(record)
        self.send_message(msg)

    def send_message(self, text):
        if "[!] Error:" in text:
            text = "```" + text + " ```"
        else:
            text = ":sparkles: " + text

        message = {"text": text}

        requests.post(self.url, json=message)
</file>

<file path="develop/src/trainer/datasets/dataset.py">
from torch.utils.data import Dataset as _Dataset
from typing import Dict, List, Callable
import os
import numpy as np
import pandas as pd
from tqdm import tqdm
import gc


FILENAME_TEMPLATE = {
    "X": "X.parquet.zstd",
    "Y": "Y.parquet.zstd",
}


def build_X_and_BX(features, base_feature_assets):
    BX = features[base_feature_assets]
    return features, BX


class Dataset(_Dataset):
    def __init__(
        self,
        data_dir: str,
        transforms: Dict[str, Callable],
        base_feature_assets: List[str],
        asset_to_id: Dict[str, int],
        lookback_window: int = 120,
    ):
        print("[+] Start to build dataset")
        self.data_caches = {}

        # Build inputs
        self.data_caches["X"], self.data_caches["BX"] = build_X_and_BX(
            features=(
                pd.read_parquet(
                    os.path.join(data_dir, FILENAME_TEMPLATE["X"]), engine="pyarrow"
                ).astype("float32")
            ),
            base_feature_assets=base_feature_assets,
        )

        assert (self.data_caches["BX"].index == self.data_caches["X"].index).all()

        self.index = []
        for asset in tqdm(self.data_caches["X"].columns.levels[0]):
            self.index += [
                (index, asset)
                for index in self.data_caches["X"][[asset]]
                .dropna()
                .iloc[lookback_window - 1 :]
                .index
            ]

        self.index = pd.Index(self.index)
        gc.collect()

        # Build labels
        self.data_caches["Y"] = (
            pd.read_parquet(
                os.path.join(data_dir, FILENAME_TEMPLATE["Y"]), engine="pyarrow",
            )
            .sort_index()
            .stack()
            .reindex(self.index)
        ).astype("float32")

        self.transforms = transforms
        self.n_data = len(self.index)
        self.lookback_window = lookback_window
        self.asset_to_id = asset_to_id

        gc.collect()
        print("[+] built dataset")

    def __len__(self):
        return self.n_data

    def __getitem__(self, idx):
        # astype -> Y: int, else: float32
        data_dict = {}

        boundary_index = self.data_caches["X"][self.index[idx][1]].index.get_loc(
            self.index[idx][0]
        )

        # Concat with BX
        concat_df = pd.concat(
            [
                self.data_caches["BX"].iloc[
                    boundary_index - (self.lookback_window - 1) : boundary_index + 1
                ],
                self.data_caches["X"][self.index[idx][1]].iloc[
                    boundary_index - (self.lookback_window - 1) : boundary_index + 1
                ],
            ],
            axis=1,
        )

        data_dict["X"] = np.swapaxes(concat_df.values, 0, 1)

        data_dict["Y"] = self.data_caches["Y"].iloc[idx]

        data_dict["ID"] = self.asset_to_id[self.index[idx][1]]

        # transform
        for data_type, transform in self.transforms.items():
            data_dict[data_type] = transform(data_dict[data_type])

        del concat_df

        return data_dict
</file>

<file path="README.md">
# Binance Future Trader
Binance future trader deals in multiple cryptocurrencies simultaneously. In default, the model predict 30min ahead, and trade every 1min.
* Note: If want to predict other time periods, you can change lookahead_window variable to what you want to predict.

## Schema
![Schema](images/schema.png)
```
.
 Makefile
 README.md
 develop
    Makefile
    dockerfiles
       Dockerfile
       requirements.txt
    src
       backtester
          __init__.py
          backtester_v1.py
          basic_backtester.py
          utils.py
       common_utils_dev
          __init__.py
          common_utils_dev.py
       dataset_builder
          build_dataset.py
       rawdata_builder
          __init__.py
          build_rawdata.py
          candidate_assets.txt
          download_kaggle_data.py
       reviewer
          __init__.py
          paramset.py
          reviewer_v1.py
          utils.py
       trainer
           datasets
              dataset.py
           models
              __init__.py
              backbones
                 __init__.py
                 backbone_v1.py
              basic_predictor.py
              criterions.py
              predictor_v1.py
              utils.py
           modules
               acts.py
               block_1d
                   __init__.py
                   conv1d.py
                   dense.py
                   dense_block.py
                   norms.py
                   residual.py
                   seblock.py
                   self_attention.py
    storage
        dataset
        experiments
 services
     Makefile
     dockerfiles
        Dockerfile
        requirements.txt
     k8s
        admin
           namespace.yml
        deployments
           data_collector.yml
           database.yml
           trader.yml
        deployments-template
           data_collector-template.yml
           database-template.yml
           trader-template.yml
        service
            service.yml
     src
        common_utils_svc
           __init__.py
           common_utils_svc.py
        config
           __init__.py
        data_collector
           data_collector.py
        database
           database.py
           models.py
           usecase.py
        exchange
           custom_client.py
        handler
           __init__.py
           slack_handler.py
        trader
            trader_v1.py
            utils.py
     storage
         database
```
## Descriptions
### Model
 - Based on DenseNet.
 - Add Squeeze and Excitation block before transition block
 - Add Self-Attention block before transition block
 - Use Activation function as SELU. selectable [Mish, tanhexp]
 - Use Dropout block as Combine(AlphaDropout, SpatialDropout)
   - Dropout not only for time-series, but also for channels.
### Performance
#### Model performance
![Performance1](images/performance1.png)
![Performance2](images/performance2.png)
![Performance3](images/performance3.png)

#### Backtest performance
The results below are after the commission was paid to the exchange.
Backtesting setting `Commisions: {"entry": 0.04%, "exit": 0.02% | 0.04%, "spread": 0.04%}`
```
trade_winning_ratio    0.690894
trade_sharpe_ratio     2.100903
trade_avg_return       0.002093
max_drawdown          -0.204948
total_return           1.217380
```
![Performance4](images/performance4.png)


## Usages
Follow below steps.
 - You can give additional variable to execute command with tag `make <command> ARGS="--variable1 <value> --variable2 <value>"`
 - All code run on same container by reuse. Once if execute run container, it run continuously, and reused by other command.
### For develop
:sparkles:`make dev_run GPU_TYPE="<NVIDIA or AMD>"`: Run container to develop  
:sparkles:`make dev_download_kaggle_data ARGS="--username <kaggle_username> --key <kaggle_key>"`: Download historical pricing data
```
 - In here, I implemented kaggle data downloader, but never checked. I recommend to download files manually through website.  
 - Download from https://www.kaggle.com/jorijnsmit/binance-full-history, then store to <pwd>/develop/storage/dataset/rawdata/raw/spot/  
 - Download from https://www.kaggle.com/nicolaes/binance-futures, then store to <pwd>/develop/storage/dataset/rawdata/raw/future/
```
:sparkles:`make dev_build_rawdata`: Build cleaned rawdata  
:sparkles:`make dev_build_dataset`: Build features and labels to train model  
:sparkles:`make dev_train`: Train model  
:sparkles:`make dev_generate`: Generate predictions in test-periods  
:sparkles:`make dev_review`: Check Performance and find best parameters by backtesting in virtual-env to trading.  
:sparkles:`make dev_display_review`: Display performance plots, it should be run after `make dev_review` is done.
  
additional  
:sparkles:`make dev_rm`: You can delete the container  
:sparkles:`make dev_bash`: You can enter the container

### For service
:sparkles:`make svc_install`: Install requirements(kubectl, minikube), currently mac and linux environment is acceptable.  
:sparkles:`make svc_run`: Run to trade in real environment.
```
 It ask the params to set. You can use default setting by just enter without any input.
 - LEVERAGE: How much use leverage
 - EXP_NAME: If you set variable of exp_name when model train, you should give.
 - REPORT_PREFIX: If you set varialbe of prefix when model review, you should give.
 - REPORT_BASE_CURRENCY: Currently only USDT is acceptable. just skip.
 - REPORT_ID: `You should set this, which parameter set is best on model reviews`
 - EXCHANGE_API_KEY: API_KEY of binance. You can set API_KEY of binance future test-net, when you want to check behavior in test-net.
 - EXCHANGE_SECRET_KEY: SECRET_KEY of binance. You can set SECRET_KEY of binance future test-net, when you want to check behavior in test-net.
 - TEST_MODE: Give True or False. Give False when test in test-net.
 - WEBHOOK_URL: Give slack webhook url, the loggings will be sent to slack channel.
```

additional  
:sparkles:`make svc_rm`: Delete only pods(containers)  
:sparkles:`make svc_reapply`: Update changes without delete.  
:sparkles:`make svc_delete`: Delete minikube. Clean-up way. If once delete, it takes time to re-run.  
:sparkles:`make svc_pods`: Check status of pods.  
:sparkles:`make svc_db_bash`: Enter database container.  
:sparkles:`make svc_dc_bash`: Enter datacollector container.  
:sparkles:`make svc_td_bash`: Enter trader container.
</file>

<file path="develop/src/trainer/models/backbones/backbone_v1.py">
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

# from trainer.modules.block_1d import DenseBlock, TransitionBlock, NORMS
# from trainer.modules import acts



from develop.src.trainer.modules.block_1d import DenseBlock, TransitionBlock, NORMS
from develop.src.trainer.modules import acts




def identity(x):
    return x


class BackboneV1(nn.Module):
    def __init__(
        self,
        in_channels,
        n_assets,
        n_blocks=3,
        n_block_layers=6,
        growth_rate=12,
        dropout=0.0,
        channel_reduction=0.5,
        activation="relu",
        normalization="bn",
        seblock=True,
        sablock=True,
    ):
        super(BackboneV1, self).__init__()
        self.in_channels = in_channels
        self.n_assets = n_assets

        self.n_blocks = n_blocks
        self.n_block_layers = n_block_layers
        self.growth_rate = growth_rate
        self.dropout = dropout
        self.channel_reduction = channel_reduction

        self.activation = activation
        self.normalization = normalization
        self.seblock = seblock
        self.sablock = sablock

        # Build first_conv
        out_channels = 4 * growth_rate
        self.first_conv = nn.Conv1d(
            in_channels=in_channels,
            out_channels=out_channels,
            kernel_size=3,
            stride=1,
            padding=1,
            bias=False,
        )

        # Build blocks
        in_channels = out_channels

        blocks = []
        for idx in range(n_blocks):

            blocks.append(
                self._build_block(
                    in_channels=in_channels,
                    use_transition_block=True if idx != n_blocks - 1 else False,
                )
            )

            # mutate in_channels for next block
            in_channels = self._compute_out_channels(
                in_channels=in_channels,
                use_transition_block=True if idx != n_blocks - 1 else False,
            )

        self.blocks = nn.Sequential(*blocks)

        # Last layers
        self.norm = identity
        if normalization is not None:
            self.norm = NORMS[normalization.upper()](num_channels=in_channels)

        self.act = getattr(acts, activation)
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)

        self.embed = nn.Embedding(n_assets, in_channels)
        self.pred_fc = nn.Linear(in_channels, 2)
        self.last_sigmoid = nn.Sigmoid()

        # Initialize
        for m in self.modules():
            if isinstance(m, nn.Conv1d):
                n = m.kernel_size[0] * m.out_channels
                m.weight.data.normal_(0, math.sqrt(1.0 / n))

            elif isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)

                if m.bias is not None:
                    m.bias.data.zero_()

            elif isinstance(m, nn.Linear):
                if m.bias is not None:
                    m.bias.data.zero_()

    def _compute_out_channels(self, in_channels, use_transition_block=True):
        if use_transition_block is True:
            return int(
                math.floor(
                    (in_channels + (self.n_block_layers * self.growth_rate))
                    * self.channel_reduction
                )
            )

        return in_channels + (self.n_block_layers * self.growth_rate)

    def _build_block(self, in_channels, use_transition_block=True):
        assert use_transition_block in (False, True)

        dense_block = DenseBlock(
            n_layers=self.n_block_layers,
            in_channels=in_channels,
            growth_rate=self.growth_rate,
            dropout=self.dropout,
            activation=self.activation,
            normalization=self.normalization,
            seblock=self.seblock if use_transition_block is True else False,
            sablock=self.sablock if use_transition_block is True else False,
        )

        if use_transition_block is True:
            in_channels = int(in_channels + (self.n_block_layers * self.growth_rate))
            out_channels = int(math.floor(in_channels * self.channel_reduction))
            transition_block = TransitionBlock(
                in_channels=in_channels,
                out_channels=out_channels,
                dropout=self.dropout,
                activation=self.activation,
                normalization=self.normalization,
            )

            return nn.Sequential(*[dense_block, transition_block])

        return dense_block

    def forward(self, x, id):
        B, _, _ = x.size()
        out = self.blocks(self.first_conv(x))
        out = self.global_avg_pool(self.act(self.norm(out))).view(B, -1)

        preds = self.pred_fc(out) + (out * self.embed(id)).sum(axis=-1, keepdim=True)

        return preds[:, 0], self.last_sigmoid(preds[:, 1])
</file>

<file path="develop/src/backtester/backtester_v1.py">
import os
from .utils import nan_to_zero
from .basic_backtester import BasicBacktester
from tqdm import tqdm
import gc


CONFIG = {
    "report_prefix": "v001",
    "detail_report": False,
    "position_side": "longshort",
    "entry_ratio": 0.2,
    "commission": {"entry": 0.0004, "exit": 0.0002, "spread": 0.0004},
    "min_holding_minutes": 1,
    "max_holding_minutes": 30,
    "compound_interest": True,
    "order_criterion": "capital",
    "possible_in_debt": True,
    "exit_if_achieved": True,
    "achieve_ratio": 1,
    "achieved_with_commission": False,
    "max_n_updated": 0,
    "positive_entry_threshold": 8,
    "negative_entry_threshold": 8,
    "exit_threshold": "auto",
    "positive_probability_threshold": 8,
    "negative_probability_threshold": 8,
    "adjust_prediction": False,
}


class BacktesterV1(BasicBacktester):
    def __init__(
        self,
        base_currency,
        dataset_dir,
        exp_dir,
        report_prefix=CONFIG["report_prefix"],
        detail_report=CONFIG["detail_report"],
        position_side=CONFIG["position_side"],
        entry_ratio=CONFIG["entry_ratio"],
        commission=CONFIG["commission"],
        min_holding_minutes=CONFIG["min_holding_minutes"],
        max_holding_minutes=CONFIG["max_holding_minutes"],
        compound_interest=CONFIG["compound_interest"],
        order_criterion=CONFIG["order_criterion"],
        possible_in_debt=CONFIG["possible_in_debt"],
        exit_if_achieved=CONFIG["exit_if_achieved"],
        achieve_ratio=CONFIG["achieve_ratio"],
        achieved_with_commission=CONFIG["achieved_with_commission"],
        max_n_updated=CONFIG["max_n_updated"],
        positive_entry_threshold=CONFIG["positive_entry_threshold"],
        negative_entry_threshold=CONFIG["negative_entry_threshold"],
        exit_threshold=CONFIG["exit_threshold"],
        positive_probability_threshold=CONFIG["positive_probability_threshold"],
        negative_probability_threshold=CONFIG["negative_probability_threshold"],
        adjust_prediction=CONFIG["adjust_prediction"],
    ):
        super().__init__(
            base_currency=base_currency,
            dataset_dir=dataset_dir,
            exp_dir=exp_dir,
            report_prefix=report_prefix,
            detail_report=detail_report,
            position_side=position_side,
            entry_ratio=CONFIG["entry_ratio"],
            commission=CONFIG["commission"],
            min_holding_minutes=min_holding_minutes,
            max_holding_minutes=max_holding_minutes,
            compound_interest=compound_interest,
            order_criterion=order_criterion,
            possible_in_debt=possible_in_debt,
            exit_if_achieved=exit_if_achieved,
            achieve_ratio=achieve_ratio,
            achieved_with_commission=achieved_with_commission,
            max_n_updated=max_n_updated,
            positive_entry_threshold=positive_entry_threshold,
            negative_entry_threshold=negative_entry_threshold,
            exit_threshold=exit_threshold,
            positive_probability_threshold=positive_probability_threshold,
            negative_probability_threshold=negative_probability_threshold,
            adjust_prediction=adjust_prediction,
        )

    def run(self, display=True):

        self.build()
        self.initialize()

        for now in tqdm(self.index):
            # Step1: Prepare pricing and signal
            pricing = self.historical_data_dict["pricing"].loc[now]
            predictions = self.historical_data_dict["predictions"].loc[now]
            probabilities = self.historical_data_dict["probabilities"].loc[now]

            # Set assets which has signals
            positive_assets = self.tradable_coins[
                (predictions >= self.positive_entry_bins)
                & (probabilities >= self.positive_probability_bins)
            ]
            negative_assets = self.tradable_coins[
                (predictions <= self.negative_entry_bins)
                & (probabilities >= self.negative_probability_bins)
            ]

            # Debug: Print current state
            print(f"\nTime: {now}")
            print(f"Number of positions: {len(self.positions)}")
            print(f"Number of trade returns: {len(self.historical_trade_returns)}")
            if len(self.positions) > 0:
                print("Current positions:")
                for pos in self.positions:
                    print(f"  {pos.asset} ({pos.side})")

            # Exit
            self.handle_exit(
                positive_assets=positive_assets,
                negative_assets=negative_assets,
                pricing=pricing,
                now=now,
            )

            # Debug: Print state after exit
            print(f"Number of positions after exit: {len(self.positions)}")
            print(f"Number of trade returns after exit: {len(self.historical_trade_returns)}")

            # Compute how much use cache
            if self.compound_interest is False:
                cache_to_order = self.entry_ratio
            else:
                if self.order_criterion == "cache":
                    if self.cache > 0:
                        cache_to_order = nan_to_zero(
                            value=(self.cache * self.entry_ratio)
                        )
                    else:
                        cache_to_order = 0

                elif self.order_criterion == "capital":
                    # Entry with capital base
                    cache_to_order = nan_to_zero(
                        value=(
                            self.compute_capital(pricing=pricing, now=now)
                            * self.entry_ratio
                        )
                    )

            # Entry
            self.handle_entry(
                predictions=predictions,
                cache_to_order=cache_to_order,
                positive_assets=positive_assets,
                negative_assets=negative_assets,
                pricing=pricing,
                now=now,
            )

            # To report
            self.report(value=self.cache, target="historical_caches", now=now)
            self.report(
                value=self.compute_capital(pricing=pricing, now=now),
                target="historical_capitals",
                now=now,
            )
            self.report(value=self.positions, target="historical_positions", now=now)


        report = self.generate_report()



        


        self.store_report(report=report)

        if display is True:
            self.display_metrics()
            self.display_report(report=report)

        # Remove historical data dict to reduce memory usage
        del self.historical_data_dict
        gc.collect()


if __name__ == "__main__":
    import fire

    fire.Fire(BacktesterV1)
</file>

<file path="develop/src/reviewer/reviewer_v1.py">
from dataclasses import dataclass
from typing import Dict, List, Union
from joblib import Parallel, delayed
from IPython.display import display, display_markdown
from tqdm import tqdm
from develop.src import backtester
import os
import pandas as pd
from glob import glob
import matplotlib.pyplot as plt
from .utils import grid
import json
from develop.src.reviewer import paramset
from develop.src.common_utils_dev import to_abs_path
from tabulate import tabulate
import fancytable as ft


@dataclass
class ReviewerV1:
    # dataset_dir: str = to_abs_path(__file__, "../../storage/dataset/dataset/v001/")
    dataset_dir = "develop/storage/dataset/dataset/v001/"
    # exp_dir: str = to_abs_path(__file__, "../../storage/experiments/v001/")
    exp_dir = "develop/storage/experiments/v001/"
    reviewer_prefix: str = "v001"
    grid_params: Union[str, Dict[str, List]] = "V1_SET1"
    backtester_type: str = "BacktesterV1"
    exec_start: int = 0
    exec_end: int = None
    n_jobs: int = 16

    def __post_init__(self):
        if isinstance(self.grid_params, str):
            self.grid_params = getattr(paramset, self.grid_params)

        self.grid_params["dataset_dir"] = self.dataset_dir
        self.grid_params["exp_dir"] = self.exp_dir

        self._build_backtesters()

    def _load_data_dict(self):
        data_dict = {}
        for key in ("labels", "predictions", "probabilities"):
            data_dict[key] = pd.read_parquet(
                os.path.join(self.exp_dir, f"generated_output/{key}.parquet.zstd")
            )

        return data_dict

    def _display_timeseries(self, data_dict):
        columns = data_dict["predictions"].columns
        _, ax = plt.subplots(len(columns), 1, figsize=(24, 2.5 * len(columns)))

        for idx, column in enumerate(columns):
            data_dict["labels"][column].rename("label").plot(ax=ax[idx], alpha=0.5)
            data_dict["predictions"][column].rename("prediction").plot(ax=ax[idx])
            ax[idx].legend()
            ax[idx].set_title(column)

        plt.tight_layout()
        plt.show()

    def _build_levels(self, data):
        levels = {}
        for column in data.columns:
            levels[column] = pd.qcut(data[column], 10, labels=False, retbins=False)

        return pd.concat(levels, axis=1)

    def _build_total_performance(self, data_dict):
        total_performance = (data_dict["labels"] * data_dict["predictions"] >= 0).mean()
        total_performance["mean"] = total_performance.mean()

        return total_performance

    def _build_performance_on_levels(self, data_dict, levels):
        performance = data_dict["labels"] * data_dict["predictions"] >= 0

        performance_on_levels = []
        for column in performance.columns:
            performance_on_levels.append(
                performance[column].groupby(levels[column]).mean()
            )

        performance_on_levels = pd.concat(performance_on_levels, axis=1)
        performance_on_levels["mean"] = performance_on_levels.mean(axis=1)

        return performance_on_levels

    def display_performance(self):
        data_dict = self._load_data_dict()

        display_markdown("#### Timeseries", raw=True)
        self._display_timeseries(data_dict=data_dict)

        display_markdown("#### Total Performance", raw=True)
        total_performance = self._build_total_performance(data_dict=data_dict)
        display(ft.display(total_performance.rename("bin_acc").to_frame().T, axis=1))

        # Build levels
        label_levels = self._build_levels(data=data_dict["labels"])
        prediction_levels = self._build_levels(data=data_dict["predictions"])
        abs_prediction_levels = self._build_levels(data=data_dict["predictions"].abs())
        probability_levels = self._build_levels(data=data_dict["probabilities"])

        display_markdown("#### Performance on label levels", raw=True)
        display(
            ft.display(
                self._build_performance_on_levels(
                    data_dict=data_dict, levels=label_levels
                )
            )
        )

        display_markdown("#### Performance on prediction levels", raw=True)
        display(
            ft.display(
                self._build_performance_on_levels(
                    data_dict=data_dict, levels=prediction_levels
                )
            )
        )

        display_markdown("#### Performance on abs(prediction) levels", raw=True)
        display(
            ft.display(
                self._build_performance_on_levels(
                    data_dict=data_dict, levels=abs_prediction_levels
                )
            )
        )

        display_markdown("#### Performance on probability levels", raw=True)
        display(
            ft.display(
                self._build_performance_on_levels(
                    data_dict=data_dict, levels=probability_levels
                )
            )
        )

    def _exists_artifact(self, index):
        exists = []
        for artifact_type in ["metrics", "report", "params"]:
            file_path = os.path.join(
                self.grid_params["exp_dir"],
                f"reports/{artifact_type}_{self.reviewer_prefix}_{index}_{self.grid_params['base_currency']}.parquet.zstd",
            )

            if artifact_type in ("params"):
                exists.append(
                    os.path.exists(file_path.replace(".parquet.zstd", ".json"))
                )
                continue

            exists.append(os.path.exists(file_path))

        exists = all(exists)

        if exists is True:
            print(f"[!] Found backtests already done: {index}")

        return exists

    def _build_backtesters(self):
        def _is_valid_params(param):
            if param["adjust_prediction"] is True:
                if isinstance(param["exit_threshold"], (int, float)):
                    return False

                if param["max_n_updated"] is None:
                    return False

            if param["exit_threshold"] != "auto":
                if param["achieve_ratio"] != 1:
                    return False

            return True

        grid_params = list(grid(self.grid_params))

        # Filter grid_params
        grid_params = [
            grid_param
            for grid_param in grid_params
            if _is_valid_params(param=grid_param) is True
        ]

        # Build backtesters
        self.backtesters = [
            getattr(backtester, self.backtester_type)(
                report_prefix=f"{self.reviewer_prefix}_{idx}", **params
            )
            for idx, params in enumerate(grid_params)
        ][self.exec_start : self.exec_end]
        self.backtesters = [
            backtester
            for backtester in self.backtesters
            if self._exists_artifact(index=backtester.report_prefix.split("_")[-1])
            is not True
        ]
        import pdb
        # pdb.set_trace()

    def _load_artifact(self, artifact_type, index):
        assert artifact_type in ("metrics", "report", "params")

        file_path = os.path.join(
            self.grid_params["exp_dir"],
            f"reports/{artifact_type}_{self.reviewer_prefix}_{index}_{self.grid_params['base_currency']}.parquet.zstd",
        )

        if artifact_type in ("metrics", "report"):
            artifact = pd.read_parquet(file_path)
        else:
            artifact = json.load(open(file_path.replace(".parquet.zstd", ".json"), "r"))

        return artifact

    def _load_artifacts(self, artifact_type, with_index=False):
        assert artifact_type in ("metrics", "report")

        file_paths = glob(
            os.path.join(
                self.grid_params["exp_dir"],
                f"reports/{artifact_type}_{self.reviewer_prefix}_*_{self.grid_params['base_currency']}.parquet.zstd",
            )
        )
        file_paths = sorted(
            file_paths,
            key=lambda x: int(
                x.split(f"{self.reviewer_prefix}_")[-1].split(
                    f'_{self.grid_params["base_currency"]}'
                )[0]
            ),
        )

        artifacts = [pd.read_parquet(file_path) for file_path in file_paths]
        index = pd.Index(
            [
                int(
                    file_path.split(f"{artifact_type}_{self.reviewer_prefix}_")[
                        -1
                    ].split(f"_{self.grid_params['base_currency']}.parquet.zstd")[0]
                )
                for file_path in file_paths
            ]
        )

        if with_index is True:
            return artifacts, index

        return artifacts

    def _build_metrics(self):
        artifacts, index = self._load_artifacts(
            artifact_type="metrics", with_index=True
        )
        metrics = pd.concat(artifacts)
        metrics.index = index

        return metrics

    def display_params(self, index, in_shell=False):
        display_markdown(f"#### Params: {index}", raw=True)

        params = (
            pd.Series(self._load_artifact(artifact_type="params", index=index))
            .rename("params")
            .to_frame()
        )

        if in_shell is True:
            print(tabulate(params, headers="keys", tablefmt="psql"))
        else:
            display(params)

    def display_report(self, index, in_shell=False):
        report = self._load_artifact(artifact_type="report", index=index)

        display_markdown(f"#### Report: {index}", raw=True)
        _, ax = plt.subplots(4, 1, figsize=(12, 12), sharex=True)

        for idx, column in enumerate(["capital", "cache", "return", "trade_return"]):
            if column == "trade_return":
                report[column].dropna().apply(lambda x: sum(x)).plot(ax=ax[idx])

            else:
                report[column].plot(ax=ax[idx])

            ax[idx].set_title(f"historical {column}")

        plt.tight_layout()

        if in_shell is True:
            plt.show(block=True)
        else:
            plt.show()

    def display_metrics(self, in_shell=False):
        metrics = self._build_metrics()

        if in_shell is True:
            print(tabulate(metrics, headers="keys", tablefmt="psql"))
        else:
            display(metrics)

    def display(self, in_shell=False):
        self.display_metrics(in_shell=in_shell)

        metrics = self._build_metrics()
        best_index = metrics["total_return"].sort_values(ascending=False).index[0]

        display_markdown(f"### [+] Best index: {best_index}", raw=True)

        display(metrics.loc[best_index])
        self.display_params(index=best_index, in_shell=in_shell)
        self.display_report(index=best_index, in_shell=in_shell)

    def run(self, in_shell=False, display_performance=False):
        if in_shell is False:
            if display_performance is True:
                self.display_performance()

        print(f"[+] Found backtests to start: {len(self.backtesters)}")

        Parallel(n_jobs=self.n_jobs, verbose=1)(
            [delayed(backtester.run)(display=False) for backtester in self.backtesters]
        )

        self.display(in_shell=in_shell)


if __name__ == "__main__":
    import fire

    fire.Fire(ReviewerV1)
</file>

<file path="services/src/exchange/custom_client.py">
from dataclasses import dataclass
import ccxt
import pandas as pd
import time
from config import CFG
from datetime import datetime
from common_utils_svc import Position

API_REQUEST_DELAY = 0.01  # sec


@dataclass
class CustomClient:
    def __post_init__(self):
        self.binance_cli = ccxt.binance(
            {
                "apiKey": CFG.EXCHANGE_API_KEY,
                "secret": CFG.EXCHANGE_SECRET_KEY,
                "enableRateLimit": True,
                "options": {"defaultType": "future"},
                "hedgeMode": True,
            }
        )
        self.test_mode = CFG.TEST_MODE
        self.tradable_coins = CFG.TRADABLE_COINS

        self.__set_test_mode()
        self.__set_dual_position_mode()
        self.__set_leverage()
        self.__set_ammount_constraints()

    def __set_test_mode(self):
        if self.test_mode is True:
            self.binance_cli.set_sandbox_mode(True)

    def __set_dual_position_mode(self):
        try:
            self.binance_cli.fapiPrivatePostPositionSideDual(
                {"dualSidePosition": "true"}
            )
        except ccxt.ExchangeError as f:
            pass
        except:
            raise RuntimeError("[!] Failed to set dual position mode")

    def __set_leverage(self):
        for symbol in self.tradable_coins:
            leverage = CFG.LEVERAGE

            if self.test_mode is True:
                if symbol in ("XMR/USDT"):
                    leverage = max(2, CFG.LEVERAGE)

            self.binance_cli.fapiPrivate_post_leverage(
                {"symbol": symbol.replace("/", ""), "leverage": leverage}
            )
            time.sleep(API_REQUEST_DELAY)

    def __set_ammount_constraints(self):
        self.ammount_constraints = (
            pd.DataFrame(self.binance_cli.load_markets())
            .xs("limits")
            .apply(lambda x: x["amount"]["min"])
            .to_dict()
        )

    def revision_symbols(self, symbols):
        if "/" in symbols:
            return symbols

        return [
            symbol.replace(CFG.BASE_CURRENCY, "/" + CFG.BASE_CURRENCY)
            for symbol in symbols
        ]

    def get_tickers(self):
        return pd.DataFrame(self.binance_cli.fetch_tickers())

    def get_last_pricing(self):
        return self.get_tickers().xs("last", axis=0).to_dict()

    def get_balance(self):
        for _ in range(20):
            balance = pd.DataFrame(self.binance_cli.fetch_balance())
            if "USDT" in balance:
                return balance

            time.sleep(0.1)

    def get_last_trade_on(self, symbol):
        orders = self.get_closed_orders(symbol=symbol)
        orders = orders[orders["status"] == "FILLED"]

        trade_on = orders.iloc[0]["time"]
        return (
            pd.Timestamp(datetime.utcfromtimestamp(trade_on / 1000))
            .floor("T")
            .tz_localize("UTC")
        )

    def get_positions(self, balance=None, symbol=None):
        if balance is None:
            balance = self.get_balance()

        positions = pd.DataFrame(balance.xs("positions")["info"])
        positions["symbol"] = self.revision_symbols(positions["symbol"])

        if symbol is not None:
            positions = positions[positions["symbol"] == symbol]

        positions["positionAmt"] = (
            positions["positionAmt"].astype(float).map(lambda x: x if x >= 0 else -x)
        )

        positions["unrealizedProfit"] = positions["unrealizedProfit"].astype(float)

        return positions

    def get_position_objects(self, symbol=None, with_entry_at=False):
        posis = self.get_positions(symbol=symbol)
        posis = posis[posis["positionAmt"].astype(float) != 0.0]
        assert posis["symbol"].is_unique

        positions = []
        for posi in posis.to_dict(orient="records"):
            position = Position(
                asset=posi["symbol"],
                side=posi["positionSide"].lower(),
                qty=float(posi["positionAmt"]),
                entry_price=float(posi["entryPrice"]),
                entry_at=self.get_last_trade_on(symbol=posi["symbol"])
                if with_entry_at is True
                else None,
                profit=float(posi["unrealizedProfit"]),
            )
            positions.append(position)

        return positions

    def get_cache_dict(self, balance=None):
        if balance is None:
            balance = self.get_balance()

        return balance.xs(CFG.BASE_CURRENCY)[["free", "used", "total"]].to_dict()

    def entry_order(self, symbol, order_type, position, amount, price=None):
        position = position.upper()
        assert position in ("LONG", "SHORT")

        if position == "LONG":
            side = "buy"
        if position == "SHORT":
            side = "sell"

        try:
            order = self.binance_cli.create_order(
                symbol=symbol,
                type=order_type,
                side=side,
                amount=amount,
                price=price,
                params={"positionSide": position},
            )["info"]
        except ccxt.errors.ExchangeError as e:
            if CFG.TEST_MODE is True:
                return None

            elif "-2019" in str(e):
                return None

            raise ccxt.errors.ExchangeError(e)

        order["symbol"] = self.revision_symbols([order["symbol"]])[-1]
        return order

    def exit_order(self, symbol, order_type, position, amount, price=None):
        position = position.upper()
        assert position in ("LONG", "SHORT")

        if position == "LONG":
            side = "sell"
        if position == "SHORT":
            side = "buy"

        try:
            order = self.binance_cli.create_order(
                symbol=symbol,
                type=order_type,
                side=side,
                amount=amount,
                price=price,
                params={"positionSide": position},
            )["info"]
        except ccxt.errors.ExchangeError as e:
            if CFG.TEST_MODE is True:
                return None

            raise ccxt.errors.ExchangeError(e)

        order["symbol"] = self.revision_symbols([order["symbol"]])[-1]
        return order

    def get_orders(self, symbol, limit=10):
        orders = self.binance_cli.fetch_orders(symbol=symbol, limit=limit)
        orders = pd.DataFrame(reversed([order["info"] for order in orders]))

        if len(orders) != 0:
            orders["symbol"] = self.revision_symbols(orders["symbol"])
        return orders

    def get_open_orders(self, symbol, limit=5):
        orders = self.binance_cli.fetch_open_orders(symbol=symbol, limit=limit)
        orders = pd.DataFrame(reversed([order["info"] for order in orders]))

        if len(orders) != 0:
            orders["symbol"] = self.revision_symbols(orders["symbol"])
        return orders

    def get_closed_orders(self, symbol, limit=5):
        orders = self.binance_cli.fetch_closed_orders(symbol=symbol, limit=limit)
        orders = pd.DataFrame(reversed([order["info"] for order in orders]))

        if len(orders) != 0:
            orders["symbol"] = self.revision_symbols(orders["symbol"])
        return orders

    def cancel_orders(self, symbol):
        self.binance_cli.cancel_all_orders(symbol=symbol)
</file>

<file path="services/src/data_collector/data_collector.py">
import ccxt
import time
from dataclasses import dataclass
from datetime import datetime
import pandas as pd
from typing import List
from config import CFG
from database import database as DB
from database.usecase import Usecase
from logging import getLogger
from common_utils_svc import initialize_main_logger


logger = getLogger("data_collector")
initialize_main_logger()


@dataclass
class DataCollector:
    usecase = Usecase()
    binance_cli: ccxt.binance = ccxt.binance(
        {"timeout": 30000, "options": {"defaultType": "future"},}
    )

    def __post_init__(self):
        DB.init()

        self._set_target_coins()
        self._sync_historical_pricing(now=pd.Timestamp.utcnow())

    def _set_target_coins(self):
        list_coins_on_binance = sorted(self.binance_cli.fetch_tickers().keys())
        self.tradable_coins = sorted(
            [
                target_coin
                for target_coin in tuple(CFG.TRADABLE_COINS)
                if target_coin in list_coins_on_binance
            ]
        )

    def _build_inserts_dict_to_sync(self, now: pd.Timestamp, limit: int):
        inserts_pricings = []
        synced_timestamps = None
        for asset in self.tradable_coins:
            pricing = self._list_historical_pricing(now=now, symbol=asset, limit=limit)

            for pricing_row in pricing.reset_index(drop=False).to_dict(
                orient="records"
            ):
                inserts_pricings.append(
                    {
                        "timestamp": pricing_row["date"],
                        "asset": asset,
                        "open": pricing_row["open"],
                        "high": pricing_row["high"],
                        "low": pricing_row["low"],
                        "close": pricing_row["close"],
                        "volume": pricing_row["volume"],
                    }
                )

            if synced_timestamps is None:
                synced_timestamps = pricing.index
            else:
                synced_timestamps = synced_timestamps & pricing.index

        inserts_syncs = [
            {"timestamp": synced_timestamp}
            for synced_timestamp in synced_timestamps.sort_values().tolist()
        ]

        return (inserts_pricings, inserts_syncs)

    def _sync_historical_pricing(self, now: pd.Timestamp, limit: int = 1500):
        inserts_pricings, inserts_syncs = self._build_inserts_dict_to_sync(
            now=now, limit=limit
        )

        self.usecase.insert_pricings(inserts=inserts_pricings)
        self.usecase.insert_syncs(inserts=inserts_syncs)
        logger.info(f"[+] Synced: historical pricings")

    def _sync_live_pricing(self, now: pd.Timestamp, limit: int = 10):
        inserts_pricings, inserts_syncs = self._build_inserts_dict_to_sync(
            now=now, limit=limit
        )

        self.usecase.update_pricings(updates=inserts_pricings)
        self.usecase.update_syncs(updates=inserts_syncs)

        self.usecase.delete_old_records(
            table="pricings", limit=1500 * len(self.tradable_coins)
        )
        self.usecase.delete_old_records(table="syncs", limit=1500)
        self.usecase.delete_old_records(table="trades", limit=1500)

    def _list_historical_pricing(
        self, now: pd.Timestamp, symbol: str, limit: int = 1500
    ):
        assert limit < 2000

        if limit >= 1000:
            pricing = self.binance_cli.fetch_ohlcv(
                symbol=symbol, timeframe="1m", limit=1000
            )

            ext_limit = (limit + 1) - 1000
            pricing += self.binance_cli.fetch_ohlcv(
                symbol=symbol,
                timeframe="1m",
                limit=ext_limit,
                since=(pricing[0][0] - (60 * ext_limit * 1000)),
            )
        else:
            pricing = self.binance_cli.fetch_ohlcv(
                symbol=symbol, timeframe="1m", limit=limit + 1
            )

        pricing = pd.DataFrame(
            pricing, columns=["date", "open", "high", "low", "close", "volume"]
        ).set_index("date")
        pricing.index = pricing.index.map(
            lambda x: datetime.utcfromtimestamp(x / 1000)
        ).tz_localize("UTC")

        # We drop one value always
        pricing = pricing.sort_index()
        return pricing[pricing.index < now.floor("T")]

    def _get_minutes_to_sync(self, now: pd.Timestamp):
        last_sync_on = self.usecase.get_last_sync_on()
        minutes_delta = int((now.floor("T") - last_sync_on).total_seconds() // 60)

        return minutes_delta - 1

    def run(self):
        """Definitioin of demon to live sync
        """
        logger.info("[O] Start: demon of data_collector")

        error_count = 0
        while True:
            try:
                now = pd.Timestamp.utcnow()
                minutes_to_sync = self._get_minutes_to_sync(now=now)

                if minutes_to_sync != 0:
                    minutes_to_sync = min(max(minutes_to_sync, 5), 1500)

                    self._sync_live_pricing(now=now, limit=minutes_to_sync)
                    logger.info(f'[+] Synced: {now.floor("T")}')

                    error_count = 0

            except Exception as e:
                error_count += 1

                # Raise error if reached limitation
                if error_count >= 20:
                    # Sleep for Api limitation
                    time.sleep(60)

                    logger.error("[!] Error: ", exc_info=True)
                    raise Exception

                logger.info(f"[!] Synced Failed")
                time.sleep(1)

            time.sleep(0.1)


if __name__ == "__main__":
    import fire

    fire.Fire(DataCollector)
</file>

<file path="develop/src/backtester/basic_backtester.py">
import os
import json
import numpy as np
import pandas as pd
from copy import copy
import matplotlib.pyplot as plt
from abc import abstractmethod
from IPython.display import display, display_markdown
from .utils import load_parquet, Position
from develop.src.common_utils_dev import make_dirs
from collections import OrderedDict, defaultdict
import empyrical as emp
from develop.src.common_utils_dev import to_parquet


CONFIG = {
    "report_prefix": "v001",
    "detail_report": False,
    "position_side": "longshort",
    "entry_ratio": 0.055,
    "commission": {"entry": 0.0004, "exit": 0.0002, "spread": 0.0004},
    "min_holding_minutes": 1,
    "max_holding_minutes": 30,
    "compound_interest": True,
    "order_criterion": "capital",
    "possible_in_debt": False,
    "exit_if_achieved": True,
    "achieve_ratio": 1,
    "achieved_with_commission": False,
    "max_n_updated": 0,
    "positive_entry_threshold": 8,
    "negative_entry_threshold": 8,
    "exit_threshold": "auto",
    "positive_probability_threshold": 8,
    "negative_probability_threshold": 8,
    "adjust_prediction": False,
}


def make_flat(series):
    flatten = []
    for key, values in series.to_dict().items():
        if isinstance(values, list):
            for value in values:
                flatten.append(pd.Series({key: value}))
        else:
            flatten.append(pd.Series({key: values}))

    return pd.concat(flatten).sort_index()


class BasicBacktester:
    def __init__(
        self,
        base_currency,
        dataset_dir,
        exp_dir,
        report_prefix=CONFIG["report_prefix"],
        detail_report=CONFIG["detail_report"],
        position_side=CONFIG["position_side"],
        entry_ratio=CONFIG["entry_ratio"],
        commission=CONFIG["commission"],
        min_holding_minutes=CONFIG["min_holding_minutes"],
        max_holding_minutes=CONFIG["max_holding_minutes"],
        compound_interest=CONFIG["compound_interest"],
        order_criterion=CONFIG["order_criterion"],
        possible_in_debt=CONFIG["possible_in_debt"],
        exit_if_achieved=CONFIG["exit_if_achieved"],
        achieve_ratio=CONFIG["achieve_ratio"],
        achieved_with_commission=CONFIG["achieved_with_commission"],
        max_n_updated=CONFIG["max_n_updated"],
        positive_entry_threshold=CONFIG["positive_entry_threshold"],
        negative_entry_threshold=CONFIG["negative_entry_threshold"],
        exit_threshold=CONFIG["exit_threshold"],
        positive_probability_threshold=CONFIG["positive_probability_threshold"],
        negative_probability_threshold=CONFIG["negative_probability_threshold"],
        adjust_prediction=CONFIG["adjust_prediction"],
    ):
        assert position_side in ("long", "short", "longshort")
        self.base_currency = base_currency
        self.report_prefix = report_prefix
        self.detail_report = detail_report
        self.position_side = position_side
        self.entry_ratio = entry_ratio
        self.commission = commission
        self.min_holding_minutes = min_holding_minutes
        self.max_holding_minutes = max_holding_minutes
        self.compound_interest = compound_interest
        self.order_criterion = order_criterion
        assert self.order_criterion in ("cache", "capital")

        self.possible_in_debt = possible_in_debt
        self.exit_if_achieved = exit_if_achieved
        self.achieve_ratio = achieve_ratio
        self.achieved_with_commission = achieved_with_commission
        self.max_n_updated = max_n_updated
        self.positive_entry_threshold = positive_entry_threshold
        self.negative_entry_threshold = negative_entry_threshold
        self.exit_threshold = exit_threshold
        assert isinstance(exit_threshold, (float, int, str))
        if type(exit_threshold) == str:
            assert (exit_threshold == "auto") or ("*" in exit_threshold)

        self.positive_probability_threshold = positive_probability_threshold
        self.negative_probability_threshold = negative_probability_threshold

        self.adjust_prediction = adjust_prediction

        self.dataset_dir = dataset_dir
        self.exp_dir = exp_dir

        self.initialize()

    def _load_prediction_abs_bins(self):
        return load_parquet(
            path=os.path.join(
                self.exp_dir, "generated_output/prediction_abs_bins.parquet.zstd"
            )
        )

    def _load_probability_bins(self):
        return load_parquet(
            path=os.path.join(
                self.exp_dir, "generated_output/probability_bins.parquet.zstd"
            )
        )

    def _build_historical_data_dict(self, base_currency, historical_data_path_dict):
        historical_data_path_dict = copy(historical_data_path_dict)

        data_dict = {}

        # We use open pricing to handling, entry: open, exit: open
        data_dict["pricing"] = (
            load_parquet(path=historical_data_path_dict.pop("pricing"))
            .xs("open", axis=1, level=1)
            .astype("float16")
        )
        columns = data_dict["pricing"].columns
        columns_with_base_currency = columns[
            columns.str.endswith(base_currency.upper())
        ]
        data_dict["pricing"] = data_dict["pricing"][columns_with_base_currency]

        for data_type, data_path in historical_data_path_dict.items():
            data_dict[data_type] = load_parquet(path=data_path).astype("float16")

            # Filter by base_currency
            data_dict[data_type] = data_dict[data_type][columns_with_base_currency]

        return data_dict

    def _set_bins(self, prediction_abs_bins, probability_bins, index):
        assert (prediction_abs_bins >= 0).all().all()
        assert (probability_bins >= 0).all().all()

        self.positive_entry_bins = None
        self.negative_entry_bins = None
        self.exit_bins = None
        self.positive_probability_bins = None
        self.negative_probability_bins = None

        if isinstance(self.positive_entry_threshold, str):
            if "*" in self.positive_entry_threshold:
                self.positive_entry_bins = (
                    prediction_abs_bins.loc[
                        int(self.positive_entry_threshold.split("*")[0])
                    ]
                    * float(self.positive_entry_threshold.split("*")[-1])
                )[index]
        else:
            self.positive_entry_bins = prediction_abs_bins.loc[
                self.positive_entry_threshold
            ][index]

        if isinstance(self.negative_entry_threshold, str):
            if "*" in self.negative_entry_threshold:
                self.negative_entry_bins = -(
                    prediction_abs_bins.loc[
                        int(self.negative_entry_threshold.split("*")[0])
                    ]
                    * float(self.negative_entry_threshold.split("*")[-1])
                )[index]
        else:
            self.negative_entry_bins = -prediction_abs_bins.loc[
                self.negative_entry_threshold
            ][index]

        if isinstance(self.exit_threshold, str):
            if "*" in self.exit_threshold:
                self.exit_bins = (
                    prediction_abs_bins.loc[int(self.exit_threshold.split("*")[0])]
                    * float(self.exit_threshold.split("*")[-1])
                )[index]
        else:
            self.exit_bins = prediction_abs_bins.loc[self.exit_threshold][index]

        if isinstance(self.positive_probability_threshold, str):
            if "*" in self.positive_probability_threshold:
                self.positive_probability_bins = (
                    probability_bins.loc[
                        int(self.positive_probability_threshold.split("*")[0])
                    ]
                    * float(self.positive_probability_threshold.split("*")[-1])
                )[index]
        else:
            self.positive_probability_bins = probability_bins.loc[
                self.positive_probability_threshold
            ][index]

        if isinstance(self.negative_probability_threshold, str):
            if "*" in self.negative_probability_threshold:
                self.negative_probability_bins = (
                    probability_bins.loc[
                        int(self.negative_probability_threshold.split("*")[0])
                    ]
                    * float(self.negative_probability_threshold.split("*")[-1])
                )[index]
        else:
            self.negative_probability_bins = probability_bins.loc[
                self.negative_probability_threshold
            ][index]

    def build(self):
        self.report_store_dir = os.path.join(self.exp_dir, "reports/")
        make_dirs([self.report_store_dir])

        self.historical_data_dict = self._build_historical_data_dict(
            base_currency=self.base_currency,
            historical_data_path_dict={
                "pricing": os.path.join(self.dataset_dir, "test/pricing.parquet.zstd"),
                "predictions": os.path.join(
                    self.exp_dir, "generated_output/predictions.parquet.zstd"
                ),
                "probabilities": os.path.join(
                    self.exp_dir, "generated_output/probabilities.parquet.zstd"
                ),
                "labels": os.path.join(
                    self.exp_dir, "generated_output/labels.parquet.zstd"
                ),
            },
        )



        self.tradable_coins = self.historical_data_dict["predictions"].columns
        # self.index = (
        #     self.historical_data_dict["predictions"].index
        #     & self.historical_data_dict["pricing"].index
        # ).sort_values()

        common_dates = np.intersect1d(
            self.historical_data_dict["predictions"].index,
            self.historical_data_dict["pricing"].index
        )
        self.index = pd.DatetimeIndex(common_dates).sort_values()

        import pdb
        # pdb.set_trace()




        for key in self.historical_data_dict.keys():
            self.historical_data_dict[key] = self.historical_data_dict[key].reindex(
                self.index
            )

        prediction_abs_bins = self._load_prediction_abs_bins()
        probability_bins = self._load_probability_bins()
        self._set_bins(
            prediction_abs_bins=prediction_abs_bins,
            probability_bins=probability_bins,
            index=self.tradable_coins,
        )

    def initialize(self):
        self.historical_caches = {}
        self.historical_capitals = {}
        self.historical_trade_returns = defaultdict(list)

        if self.detail_report is True:
            self.historical_entry_reasons = defaultdict(list)
            self.historical_exit_reasons = defaultdict(list)
            self.historical_profits = defaultdict(list)
            self.historical_positions = {}

        self.positions = []
        self.cache = 1

    def report(self, value, target, now, append=False):
        if hasattr(self, target) is False:
            return

        if append is True:
            getattr(self, target)[now].append(value)
            return

        assert now not in getattr(self, target)
        getattr(self, target)[now] = value

    def generate_report(self):
        historical_caches = pd.Series(self.historical_caches).rename("cache")
        historical_capitals = pd.Series(self.historical_capitals).rename("capital")
        historical_returns = (
            pd.Series(self.historical_capitals)
            .pct_change(fill_method=None)
            .fillna(0)
            .rename("return")
        )
        historical_trade_returns = pd.Series(self.historical_trade_returns).rename(
            "trade_return"
        )

        report = [
            historical_caches,
            historical_capitals,
            historical_returns,
            historical_trade_returns,
        ]

        if self.detail_report is True:
            historical_entry_reasons = pd.Series(self.historical_entry_reasons).rename(
                "entry_reason"
            )
            historical_exit_reasons = pd.Series(self.historical_exit_reasons).rename(
                "exit_reason"
            )
            historical_profits = pd.Series(self.historical_profits).rename("profit")
            historical_positions = pd.Series(self.historical_positions).rename(
                "position"
            )

            report += [
                historical_entry_reasons,
                historical_exit_reasons,
                historical_profits,
                historical_positions,
            ]

        report = pd.concat(report, axis=1).sort_index()
        report.index = pd.to_datetime(report.index)

        return report

    def store_report(self, report):
        metrics = self.build_metrics().to_frame().T
        to_parquet(
            df=metrics.astype("float32"),
            path=os.path.join(
                self.report_store_dir,
                f"metrics_{self.report_prefix}_{self.base_currency}.parquet.zstd",
            ),
        )

        to_parquet(
            df=report,
            path=os.path.join(
                self.report_store_dir,
                f"report_{self.report_prefix}_{self.base_currency}.parquet.zstd",
            ),
        )

        params = {
            "base_currency": self.base_currency,
            "position_side": self.position_side,
            "entry_ratio": self.entry_ratio,
            "commission": self.commission,
            "min_holding_minutes": self.min_holding_minutes,
            "max_holding_minutes": self.max_holding_minutes,
            "compound_interest": self.compound_interest,
            "order_criterion": self.order_criterion,
            "possible_in_debt": self.possible_in_debt,
            "achieved_with_commission": self.achieved_with_commission,
            "max_n_updated": self.max_n_updated,
            "tradable_coins": tuple(self.tradable_coins.tolist()),
            "exit_if_achieved": self.exit_if_achieved,
            "achieve_ratio": self.achieve_ratio,
            "positive_entry_threshold": self.positive_entry_threshold,
            "negative_entry_threshold": self.negative_entry_threshold,
            "exit_threshold": self.exit_threshold,
            "positive_probability_threshold": self.positive_probability_threshold,
            "negative_probability_threshold": self.negative_probability_threshold,
            "adjust_prediction": self.adjust_prediction,
        }
        with open(
            os.path.join(
                self.report_store_dir,
                f"params_{self.report_prefix}_{self.base_currency}.json",
            ),
            "w",
        ) as f:
            json.dump(params, f)

        print(f"[+] Report is stored: {self.report_prefix}_{self.base_currency}")

    def build_metrics(self):
        assert len(self.historical_caches) != 0
        assert len(self.historical_capitals) != 0
        assert len(self.historical_trade_returns) != 0

        historical_returns = (
            pd.Series(self.historical_capitals).pct_change(fill_method=None).fillna(0)
        )
        historical_trade_returns = make_flat(
            pd.Series(self.historical_trade_returns).rename("trade_return").dropna()
        )

        metrics = OrderedDict()
        metrics["trade_winning_ratio"] = (
            historical_trade_returns[historical_trade_returns != 0] > 0
        ).mean()
        metrics["trade_sharpe_ratio"] = emp.sharpe_ratio(historical_trade_returns)
        metrics["trade_avg_return"] = historical_trade_returns.mean()
        metrics["max_drawdown"] = emp.max_drawdown(historical_returns)
        metrics["total_return"] = historical_returns.add(1).cumprod().sub(1).iloc[-1]

        return pd.Series(metrics)

    def display_metrics(self):
        display_markdown(f"#### Performance metrics: {self.base_currency}", raw=True)
        display(self.build_metrics())

    def display_report(self, report):
        display_markdown(f"#### Report: {self.base_currency}", raw=True)
        _, ax = plt.subplots(4, 1, figsize=(12, 12), sharex=True)

        for idx, column in enumerate(["capital", "cache", "return", "trade_return"]):
            if column == "trade_return":
                report[column].dropna().apply(lambda x: sum(x)).plot(ax=ax[idx])

            else:
                report[column].plot(ax=ax[idx])

            ax[idx].set_title(f"historical {column}")

        plt.tight_layout()
        plt.show()

    def compute_cost_to_order(self, position):
        cache_to_order = position.entry_price * position.qty
        commission_to_order = cache_to_order * (
            self.commission["entry"] + self.commission["spread"]
        )

        return cache_to_order + commission_to_order

    def check_if_executable_order(self, cost):
        if self.possible_in_debt is True:
            return True

        return bool((self.cache - cost) >= 0)

    def pay_cache(self, cost):
        self.cache = self.cache - cost

    def deposit_cache(self, profit):
        self.cache = self.cache + profit

    def compute_adjusted_prediction(
        self, side, entry_price, current_price, entry_prediction, current_prediction
    ):
        if side == "long":
            if entry_price * (1 + entry_prediction) < current_price * (
                1 + current_prediction
            ):
                return ((current_price * (1 + current_prediction)) / entry_price) - 1

        if side == "short":
            if entry_price * (1 + entry_prediction) > current_price * (
                1 + current_prediction
            ):
                return ((current_price * (1 + current_prediction)) / entry_price) - 1

        return entry_prediction

    def update_position_if_already_have(self, position):
        for idx, exist_position in enumerate(self.positions):
            if (exist_position.asset == position.asset) and (
                exist_position.side == position.side
            ):
                # Skip when max_n_updated is None
                if self.max_n_updated is None:
                    return True

                # Skip when position has max_n_updated
                if exist_position.n_updated == self.max_n_updated:

                    adjusted_prediction = exist_position.prediction
                    if self.adjust_prediction is True:
                        adjusted_prediction = self.compute_adjusted_prediction(
                            side=exist_position.side,
                            entry_price=exist_position.entry_price,
                            current_price=position.entry_price,
                            entry_prediction=exist_position.prediction,
                            current_prediction=position.prediction,
                        )

                    # Update only prediction, and entry_at
                    update_position = Position(
                        asset=exist_position.asset,
                        side=exist_position.side,
                        qty=exist_position.qty,
                        entry_price=exist_position.entry_price,
                        prediction=adjusted_prediction,
                        entry_at=position.entry_at,
                        n_updated=exist_position.n_updated,
                    )

                    self.positions[idx] = update_position
                    # return fake updated mark
                    return True

                update_entry_price = (
                    (exist_position.entry_price * exist_position.qty)
                    + (position.entry_price * position.qty)
                ) / (exist_position.qty + position.qty)

                # This is currently invalid way, but acceptable.
                update_prediction = (
                    (exist_position.prediction * exist_position.qty)
                    + (position.prediction * position.qty)
                ) / (exist_position.qty + position.qty)

                # Update entry_price, entry_at and qty
                update_position = Position(
                    asset=exist_position.asset,
                    side=exist_position.side,
                    qty=exist_position.qty + position.qty,
                    entry_price=update_entry_price,
                    entry_at=position.entry_at,
                    prediction=update_prediction,
                    n_updated=exist_position.n_updated + 1,
                )

                # Compute cost by only current order
                cost = self.compute_cost_to_order(position=position)
                executable_order = self.check_if_executable_order(cost=cost)

                # Update
                if executable_order is True:
                    self.pay_cache(cost=cost)
                    self.positions[idx] = update_position

                    # updated
                    return True

        return False

    def compute_profit(self, position, pricing, now, achieved=False):
        current_price = pricing[position.asset]

        if position.side == "long":
            profit_without_commission = current_price * position.qty

        if position.side == "short":
            profit_without_commission = position.entry_price * position.qty
            profit_without_commission += (
                (current_price - position.entry_price) * position.qty * -1
            )

        exit_commission = self.commission["exit"]
        if achieved is not True:
            exit_commission = (self.commission["exit"] * 2) + self.commission["spread"]

        commission_to_order = profit_without_commission * exit_commission

        return profit_without_commission - commission_to_order

    def compute_capital(self, pricing, now):
        # capital = cache + value of positions
        capital = self.cache

        for position in self.positions:
            current_price = pricing[position.asset]

            if position.side == "long":
                capital += current_price * position.qty

            if position.side == "short":
                capital += position.entry_price * position.qty
                capital += (current_price - position.entry_price) * position.qty * -1

        return capital

    def check_if_opposite_position_exists(self, order_asset, order_side):
        if order_side == "long":
            opposite_side = "short"
        if order_side == "short":
            opposite_side = "long"

        for exist_position in self.positions:
            if (exist_position.asset == order_asset) and (
                exist_position.side == opposite_side
            ):
                return True

        return False

    def entry_order(self, asset, side, cache_to_order, pricing, prediction, now):
        if cache_to_order == 0:
            return

        # if opposite position exists, we dont entry
        if (
            self.check_if_opposite_position_exists(order_asset=asset, order_side=side)
            is True
        ):
            return

        entry_price = pricing[asset]
        qty = cache_to_order / entry_price

        position = Position(
            asset=asset,
            side=side,
            qty=qty,
            entry_price=entry_price,
            prediction=prediction,
            entry_at=now,
        )

        updated = self.update_position_if_already_have(position=position)
        if updated is True:
            self.report(
                value={asset: "updated"},
                target="historical_entry_reasons",
                now=now,
                append=True,
            )
            return
        else:
            cost = self.compute_cost_to_order(position=position)
            executable_order = self.check_if_executable_order(cost=cost)

            if executable_order is True:
                self.pay_cache(cost=cost)
                self.positions.append(position)
                self.report(
                    value={asset: "signal"},
                    target="historical_entry_reasons",
                    now=now,
                    append=True,
                )

    def exit_order(self, position, pricing, now, achieved=False):
        print(f"\nCalculating profit for {position.asset} ({position.side})")
        profit = self.compute_profit(
            position=position, pricing=pricing, now=now, achieved=achieved
        )
        print(f"Profit: {profit}")
        self.deposit_cache(profit=profit)

        net_profit = profit - (position.entry_price * position.qty)
        print(f"Net profit: {net_profit}")
        print(f"Trade return: {net_profit / (position.entry_price * position.qty)}")
        
        self.report(value=net_profit, target="historical_profits", now=now, append=True)
        self.report(
            value=(net_profit / (position.entry_price * position.qty)),
            target="historical_trade_returns",
            now=now,
            append=True,
        )
        print(f"Historical trade returns after update: {len(self.historical_trade_returns)}")

    def handle_entry(
        self,
        predictions,
        cache_to_order,
        positive_assets,
        negative_assets,
        pricing,
        now,
    ):
        # Entry order
        if self.position_side in ("long", "longshort"):
            for order_asset in positive_assets:
                self.entry_order(
                    asset=order_asset,
                    side="long",
                    cache_to_order=cache_to_order,
                    pricing=pricing,
                    prediction=predictions[order_asset],
                    now=now,
                )

        if self.position_side in ("short", "longshort"):
            for order_asset in negative_assets:
                self.entry_order(
                    asset=order_asset,
                    side="short",
                    cache_to_order=cache_to_order,
                    pricing=pricing,
                    prediction=predictions[order_asset],
                    now=now,
                )

    def handle_exit(self, positive_assets, negative_assets, pricing, now):
        import pdb
        # pdb.set_trace()

        for position_idx, position in enumerate(self.positions):
            # Handle achievement
            if self.exit_if_achieved is True:
                if (
                    self.check_if_achieved(position=position, pricing=pricing, now=now)
                    is True
                ):
                    print(f"\nExiting position due to achievement: {position.asset} ({position.side})")
                    self.exit_order(
                        position=position, pricing=pricing, now=now, achieved=True
                    )
                    self.report(
                        value={position.asset: "achieved"},
                        target="historical_exit_reasons",
                        now=now,
                        append=True,
                    )
                    self.positions[position_idx].is_exited = True
                    continue

            # Keep position if matched
            if (position.side == "long") and (position.asset in positive_assets):
                continue

            if (position.side == "short") and (position.asset in negative_assets):
                continue

            passed_minutes = (
                pd.Timestamp(now) - pd.Timestamp(position.entry_at)
            ).total_seconds() / 60

            # Handle min_holding_minutes
            if passed_minutes <= self.min_holding_minutes:
                continue

            # Handle max_holding_minutes
            if passed_minutes >= self.max_holding_minutes:
                print(f"\nExiting position due to max holding time: {position.asset} ({position.side})")
                self.exit_order(position=position, pricing=pricing, now=now)
                self.report(
                    value={position.asset: "max_holding_minutes"},
                    target="historical_exit_reasons",
                    now=now,
                    append=True,
                )
                self.positions[position_idx].is_exited = True
                continue

            # Handle exit signal
            if (position.side == "long") and (position.asset in negative_assets):
                print(f"\nExiting position due to opposite signal: {position.asset} ({position.side})")
                self.exit_order(position=position, pricing=pricing, now=now)
                self.report(
                    value={position.asset: "opposite_signal"},
                    target="historical_exit_reasons",
                    now=now,
                    append=True,
                )
                self.positions[position_idx].is_exited = True
                continue

            if (position.side == "short") and (position.asset in positive_assets):
                print(f"\nExiting position due to opposite signal: {position.asset} ({position.side})")
                self.exit_order(position=position, pricing=pricing, now=now)
                self.report(
                    value={position.asset: "opposite_signal"},
                    target="historical_exit_reasons",
                    now=now,
                    append=True,
                )
                self.positions[position_idx].is_exited = True
                continue

        # Delete exited positions
        self.positions = [
            position for position in self.positions if position.is_exited is not True
        ]

    def check_if_achieved(self, position, pricing, now):
        current_price = pricing[position.asset]

        diff_price = current_price - position.entry_price
        if self.achieved_with_commission is True:
            if position.side == "long":
                commission = (
                    current_price
                    * (self.commission["exit"] + self.commission["spread"])
                ) + (
                    position.entry_price
                    * (self.commission["entry"] + self.commission["spread"])
                )
            if position.side == "short":
                commission = -(
                    (
                        current_price
                        * (self.commission["exit"] + self.commission["spread"])
                    )
                    + (
                        position.entry_price
                        * (self.commission["entry"] + self.commission["spread"])
                    )
                )

            diff_price = diff_price - commission

        if diff_price != 0:
            trade_return = diff_price / position.entry_price
        else:
            trade_return = 0

        trade_return = trade_return / self.achieve_ratio

        if self.exit_threshold == "auto":
            if position.side == "long":
                assert position.prediction > 0
                if trade_return >= position.prediction:
                    return True

            if position.side == "short":
                assert position.prediction < 0
                if trade_return <= position.prediction:
                    return True
        else:
            if position.side == "long":
                if trade_return >= self.exit_bins[position.asset]:
                    return True

            if position.side == "short":
                if trade_return <= -self.exit_bins[position.asset]:
                    return True

        return False

    @abstractmethod
    def run(self):
        pass
</file>

<file path="services/Makefile">
OS_NAME=$(shell (python -c "import platform; print(platform.system())"))
AVAILABLE_CPU_CORES=$(shell (python -c "import multiprocessing; print(int(multiprocessing.cpu_count()) - 1)"))

PARENT_PWD=$(shell dirname $(shell pwd))
SED_PWD=$(shell (echo $(shell pwd) | sed 's_/_\\/_g'))
SED_PARENT_PWD=$(shell (dirname $(shell pwd) | sed 's_/_\\/_g'))

POD_DC_NAME=kubectl -n dev get pods | grep data-collector | awk '{print $$1}'
POD_DB_NAME=kubectl -n dev get pods | grep database | awk '{print $$1}'
POD_TD_NAME=kubectl -n dev get pods | grep trader | awk '{print $$1}'

define GENERATE_YML
	cat "$(shell pwd)/k8s/deployments-template/$1-template.yml" |\
	sed "s/{{PWD}}/$(SED_PWD)/g" |\
	sed "s/{{PARENT_PWD}}/$(SED_PARENT_PWD)/g" |\
	sed "s/{{EXP_NAME}}/$2/g" |\
	sed "s/{{REPORT_PREFIX}}/$3/g" |\
	sed "s/{{REPORT_BASE_CURRENCY}}/$4/g" |\
	sed "s/{{REPORT_ID}}/\"$5\"/g" |\
	sed "s/{{LEVERAGE}}/\"$6\"/g" >\
	$(shell pwd)/k8s/deployments/$1.yml
endef

define BUILD_SECRET
	mkdir -p /tmp/k8s_secret; \
	printf $1 > /tmp/k8s_secret/api_key; \
	printf $2 > /tmp/k8s_secret/secret_key; \
	printf $3 > /tmp/k8s_secret/test_mode; \
	printf $4 > /tmp/k8s_secret/webhook_url; \
	kubectl -n dev create secret generic market-secret --from-file=/tmp/k8s_secret/api_key --from-file=/tmp/k8s_secret/secret_key --from-file=/tmp/k8s_secret/test_mode --from-file=/tmp/k8s_secret/webhook_url; \
	rm -rf /tmp/k8s_secret
endef

_mkdirs:
	@sudo chmod -R ug+rw $(PARENT_PWD)
	@mkdir -p $(shell pwd)/storage/trader
	@mkdir -p $(shell pwd)/k8s/deployments

_build_ymls:
	@read -p "LEVERAGE: " LEVERAGE; LEVERAGE=$${LEVERAGE:-1}; \
	read -p "EXP_NAME: " EXP_NAME; EXP_NAME=$${EXP_NAME:-v002}; \
	read -p "REPORT_PREFIX: " REPORT_PREFIX; REPORT_PREFIX=$${REPORT_PREFIX:-V1_CSET1}; \
	read -p "REPORT_BASE_CURRENCY: " REPORT_BASE_CURRENCY; REPORT_BASE_CURRENCY=$${REPORT_BASE_CURRENCY:-USDT}; \
	read -p "REPORT_ID: " REPORT_ID; REPORT_ID=$${REPORT_ID:-16}; \
	$(call GENERATE_YML,database,$$EXP_NAME,$$REPORT_PREFIX,$$REPORT_BASE_CURRENCY,$$REPORT_ID,$$LEVERAGE);\
	$(call GENERATE_YML,data_collector,$$EXP_NAME,$$REPORT_PREFIX,$$REPORT_BASE_CURRENCY,$$REPORT_ID,$$LEVERAGE);\
	$(call GENERATE_YML,trader,$$EXP_NAME,$$REPORT_PREFIX,$$REPORT_BASE_CURRENCY,$$REPORT_ID,$$LEVERAGE)

_apply_secret:
ifeq ($(shell kubectl -n dev get secret market-secret 2> /dev/null),)
	@read -p "EXCHANGE_API_KEY: " EXCHANGE_API_KEY; \
	read -p "EXCHANGE_SECRET_KEY: " EXCHANGE_SECRET_KEY; \
	read -p "TEST_MODE: " TEST_MODE; \
	read -p "WEBHOOK_URL: " WEBHOOK_URL; \
	$(call BUILD_SECRET,$$EXCHANGE_API_KEY,$$EXCHANGE_SECRET_KEY,$$TEST_MODE,$$WEBHOOK_URL)
endif
	@echo

_apply:
	@kubectl apply -f $(shell pwd)/k8s/admin/namespace.yml
	@$(MAKE) _apply_secret

	@kubectl apply -f $(shell pwd)/k8s/deployments/database.yml
	@kubectl apply -f $(shell pwd)/k8s/deployments/data_collector.yml
	@kubectl apply -f $(shell pwd)/k8s/deployments/trader.yml
	@kubectl apply -f $(shell pwd)/k8s/service/service.yml

_build_container:
	docker build dockerfiles -t binance_trader_services:latest
	docker pull postgres:latest

_set_minikube_config:
	minikube config set memory 8192
	minikube config set cpus $(AVAILABLE_CPU_CORES)

_run_if_not_exists: _mkdirs _build_ymls _set_minikube_config
ifneq ($(shell minikube status | grep host | cut -d ' ' -f 2),Running)
	@minikube start --mount-string="$(PARENT_PWD):$(PARENT_PWD)" --mount && \
	eval $$(minikube docker-env) && \
	$(MAKE) _build_container
endif
	@echo

_install_on_mac:
	brew install kubectl minikube
	brew cask install virtualbox virtualbox-extension-pack

_install_on_ubuntu:
	sudo apt update -y
	sudo apt install -y virtualbox virtualbox-ext-pack

	wget https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
	sudo mv minikube-linux-amd64 /usr/local/bin/minikube
	sudo chmod +x /usr/local/bin/minikube

	curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl
	sudo chmod +x ./kubectl
	sudo mv ./kubectl /usr/local/bin/kubectl

install:
ifeq ($(OS_NAME),Darwin)
	$(MAKE) _install_on_mac
else ifeq ($(OS_NAME),Linux)
	$(MAKE) _install_on_ubuntu
endif

run: _run_if_not_exists
	@$(MAKE) _apply

rm:
	@kubectl delete -f $(shell pwd)/k8s/admin/namespace.yml 2> /dev/null; \
	minikube stop

delete:
	@minikube delete 2> /dev/null

reapply:
	@kubectl delete -f $(shell pwd)/k8s/admin/namespace.yml
	@$(MAKE) _apply

pods:
	@kubectl -n dev get pods

db_bash:
	@kubectl -n dev exec -it $(shell $(POD_DB_NAME)) -- bash

dc_bash:
	@kubectl -n dev exec -it $(shell $(POD_DC_NAME)) -- bash

td_bash:
	@kubectl -n dev exec -it $(shell $(POD_TD_NAME)) -- bash

run_on_cluster: _mkdirs _build_ymls
	scp -r $(shell pwd)/dockerfiles docker@192.168.39.186:/tmp/dockerfiles && \
	ssh docker@192.168.39.186 'docker build /tmp/dockerfiles -t binance_trader_services:latest && docker pull postgres:latest'
	@$(MAKE) _apply

rm_on_cluster:
	@kubectl delete -f $(shell pwd)/k8s/admin/namespace.yml 2> /dev/null;
</file>

<file path="develop/src/trainer/models/predictor_v1.py">
import os
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Union, Optional, List, Dict
from tqdm import tqdm
from .basic_predictor import BasicPredictor
from .utils import inverse_preprocess_data
from develop.src.common_utils_dev import to_parquet, to_abs_path

COMMON_CONFIG = {
    "data_dir": to_abs_path(__file__, "../../../storage/dataset/dataset/v001/train"),
    "exp_dir": to_abs_path(__file__, "../../../storage/experiments/v001"),
    "test_data_dir": to_abs_path(
        __file__, "../../../storage/dataset/dataset/v001/test"
    ),
}

DATA_CONFIG = {
    "checkpoint_dir": "./check_point",
    "generate_output_dir": "./generated_output",
    "base_feature_assets": ["BTC-USDT"],
}

MODEL_CONFIG = {
    "lookback_window": 120,
    "batch_size": 512,
    "lr": 0.0001,
    # "epochs": 15,
    "epochs": 30,
    "print_epoch": 1,
    "print_iter": 50,
    "save_epoch": 1,
    "criterion": "l2",
    "criterion_params": {},
    "load_strict": False,
    "model_name": "BackboneV1",
    "model_params": {
        "in_channels": 84,
        "n_blocks": 5,
        "n_block_layers": 8,
        "growth_rate": 12,
        "dropout": 0.1,
        "channel_reduction": 0.5,
        "activation": "selu",
        "normalization": None,
        "seblock": True,
        "sablock": True,
    },
}


class PredictorV1(BasicPredictor):
    """
    Functions:
        train(): train the model with train_data
        generate(save_dir: str): generate predictions & labels with test_data
        predict(X: torch.Tensor): gemerate prediction with given data
    """

    def __init__(
        self,
        data_dir=COMMON_CONFIG["data_dir"],
        test_data_dir=COMMON_CONFIG["test_data_dir"],
        d_config={},
        m_config={},
        exp_dir=COMMON_CONFIG["exp_dir"],
        device=None,
        pin_memory=False,
        num_workers=8,
        mode="train",
        default_d_config=DATA_CONFIG,
        default_m_config=MODEL_CONFIG,
    ):
        #   
        if device is None:
            if torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
        self.device = device

        # MPS   num_workers 
        if self.device == "mps":
            self.num_workers = 0  # macOS MPS   num_workers=0 

        


        super().__init__(
            data_dir=data_dir,
            test_data_dir=test_data_dir,
            d_config=d_config,
            m_config=m_config,
            exp_dir=exp_dir,
            device=device,
            pin_memory=pin_memory,
            num_workers=num_workers,
            mode=mode,
            default_d_config=default_d_config,
            default_m_config=default_m_config,
        )

    def _invert_to_prediction(self, pred_abs_factor, pred_sign_factor):
        multiply = ((pred_sign_factor >= 0.5) * 1.0) + ((pred_sign_factor < 0.5) * -1.0)
        return pred_abs_factor * multiply

    def _compute_train_loss(self, train_data_dict):
        # Set train mode
        self.model.train()
        self.model.zero_grad()

        # Set loss
        pred_abs_factor, pred_sign_factor = self.model(
            x=train_data_dict["X"], id=train_data_dict["ID"]
        )

        # Y loss
        loss = self.criterion(pred_abs_factor, train_data_dict["Y"].view(-1).abs()) * 10
        loss += self.binary_cross_entropy(
            pred_sign_factor, (train_data_dict["Y"].view(-1) >= 0) * 1.0
        )

        return (
            loss,
            self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            ),
        )

    def _compute_test_loss(self, test_data_dict):
        # Set eval mode
        self.model.eval()

        # Set loss
        pred_abs_factor, pred_sign_factor = self.model(
            x=test_data_dict["X"], id=test_data_dict["ID"]
        )

        # Y loss
        loss = self.criterion(pred_abs_factor, test_data_dict["Y"].view(-1).abs()) * 10
        loss += self.binary_cross_entropy(
            pred_sign_factor, (test_data_dict["Y"].view(-1) >= 0) * 1.0
        )

        return (
            loss,
            self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            ),
        )

    def _step(self, train_data_dict):
        loss, _ = self._compute_train_loss(train_data_dict=train_data_dict)
        loss.backward()
        self.optimizer.step()

        return loss

    def _display_info(self, train_loss, test_loss, test_predictions, test_labels):
        pred_norm = test_predictions[test_predictions >= 0].abs().mean()
        label_norm = test_labels[test_labels >= 0].abs().mean()

        # Print loss info
        print(
            f""" [+] train_loss: {train_loss:.2f}, test_loss: {test_loss:.2f} | [+] pred_norm: {pred_norm:.2f}, label_norm: {label_norm:.2f}"""
        )

    def _build_abs_bins(self, df):
        abs_bins = {}
        for column in df.columns:
            _, abs_bins[column] = pd.qcut(
                df[column].abs(), 10, labels=False, retbins=True
            )
            abs_bins[column] = np.concatenate([[0], abs_bins[column][1:-1], [np.inf]])

        return pd.DataFrame(abs_bins)

    def _build_probabilities(self, pred_sign_factor):
        return ((pred_sign_factor - 0.5) * 2).abs()

    def train(self):
        for epoch in range(self.model_config["epochs"]):
            if epoch <= self.last_epoch:
                continue

            for iter_ in tqdm(range(len(self.train_data_loader))):
                # Optimize
                train_data_dict = self._generate_train_data_dict()
                train_loss = self._step(train_data_dict=train_data_dict)

                # Display losses
                if epoch % self.model_config["print_epoch"] == 0:
                    if iter_ % self.model_config["print_iter"] == 0:
                        test_data_dict = self._generate_test_data_dict()
                        test_loss, test_predictions = self._compute_test_loss(
                            test_data_dict=test_data_dict
                        )
                        self._display_info(
                            train_loss=train_loss,
                            test_loss=test_loss,
                            test_predictions=test_predictions,
                            test_labels=test_data_dict["Y"],
                        )

            # Store the check-point
            if (epoch % self.model_config["save_epoch"] == 0) or (
                epoch == self.model_config["epochs"] - 1
            ):
                self._save_model(model=self.model, epoch=epoch)

    def generate(self, save_dir=None):
        assert self.mode in ("test")
        self.model.eval()

        if save_dir is None:
            save_dir = self.data_config["generate_output_dir"]

        import pdb
        # pdb.set_trace()

        # Mutate 1 min to handle logic, entry: open, exit: open
        index = self.test_data_loader.dataset.index
        index = index.set_levels(index.levels[0] + pd.Timedelta(minutes=1), level=0)

        predictions = []
        labels = []
        probabilities = []
        for idx in tqdm(range(len(self.test_data_loader))):
            test_data_dict = self._generate_test_data_dict()

            pred_abs_factor, pred_sign_factor = self.model(
                x=test_data_dict["X"], id=test_data_dict["ID"]
            )
            preds = self._invert_to_prediction(
                pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
            )

            predictions += preds.view(-1).cpu().tolist()
            labels += test_data_dict["Y"].view(-1).cpu().tolist()
            probabilities += (
                self._build_probabilities(pred_sign_factor=pred_sign_factor)
                .view(-1)
                .cpu()
                .tolist()
            )

        predictions = (
            pd.Series(predictions, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )
        labels = (
            pd.Series(labels, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )
        probabilities = (
            pd.Series(probabilities, index=index)
            .sort_index()
            .unstack()[self.dataset_params["labels_columns"]]
        )

        # Rescale
        predictions = inverse_preprocess_data(
            data=predictions * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        )
        labels = inverse_preprocess_data(
            data=labels * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        )

        prediction_abs_bins = self._build_abs_bins(df=predictions)
        probability_bins = self._build_abs_bins(df=probabilities)

        # Store signals
        for data_type, data in [
            ("predictions", predictions),
            ("labels", labels),
            ("probabilities", probabilities),
            ("prediction_abs_bins", prediction_abs_bins),
            ("probability_bins", probability_bins),
        ]:
            to_parquet(
                df=data, path=os.path.join(save_dir, f"{data_type}.parquet.zstd"),
            )

    def predict(
        self,
        X: Union[np.ndarray, torch.Tensor],
        id: Union[List, torch.Tensor],
        id_to_asset: Optional[Dict] = None,
    ):
        assert self.mode in ("predict")
        self.model.eval()

        if not isinstance(X, torch.Tensor):
            X = torch.Tensor(X)
        if not isinstance(id, torch.Tensor):
            id = torch.Tensor(id)

        pred_abs_factor, pred_sign_factor = self.model(
            x=X.to(self.device), id=id.to(self.device).long()
        )
        preds = self._invert_to_prediction(
            pred_abs_factor=pred_abs_factor, pred_sign_factor=pred_sign_factor
        )
        predictions = pd.Series(preds.view(-1).cpu().tolist(), index=id.int().tolist(),)
        probabilities = pd.Series(
            self._build_probabilities(pred_sign_factor=pred_sign_factor)
            .view(-1)
            .cpu()
            .tolist(),
            index=id.int().tolist(),
        )

        # Post-process
        assert id_to_asset is not None
        predictions.index = predictions.index.map(lambda x: id_to_asset[x])
        probabilities.index = probabilities.index.map(lambda x: id_to_asset[x])

        # Rescale
        labels_columns = self.dataset_params["labels_columns"]
        labels_columns = [
            labels_column.replace("-", "/") for labels_column in labels_columns
        ]

        predictions = predictions.rename("predictions").to_frame().T[labels_columns]
        predictions = inverse_preprocess_data(
            data=predictions * self.dataset_params["winsorize_threshold"],
            scaler=self.label_scaler,
        ).loc["predictions"]

        probabilities = probabilities.rename("probabilities")[labels_columns]

        return {"predictions": predictions, "probabilities": probabilities}


if __name__ == "__main__":
    import fire

    fire.Fire(PredictorV1)
</file>

<file path="develop/src/reviewer/paramset.py">
from collections import OrderedDict

V1_SET1 = OrderedDict(
    dict(
        base_currency="USDT",
        position_side="longshort",
        exit_if_achieved=True,
        achieve_ratio=[0.8, 1],
        achieved_with_commission=True,
        min_holding_minutes=[0, 1],
        max_holding_minutes=30,
        entry_ratio=0.09,
        commission={"entry": 0.0, "exit": 0.0, "spread": 0.0},
        compound_interest=True,
        order_criterion="capital",
        max_n_updated=0,
        positive_entry_threshold=[9],
        negative_entry_threshold=[9],
        exit_threshold="auto",
        positive_probability_threshold=[5, 6, 7, 8, 9],
        negative_probability_threshold=[8, 9, "9*1.25", "9*1.5"],
        adjust_prediction=False,
        possible_in_debt=False,
    )
)

V1_SET2 = OrderedDict(
    dict(
        base_currency="USDT",
        position_side="longshort",
        exit_if_achieved=True,
        achieve_ratio=[0.8, 1],
        achieved_with_commission=True,
        min_holding_minutes=[0, 1],
        max_holding_minutes=30,
        entry_ratio=0.1,
        commission={"entry": 0.0, "exit": 0.0, "spread": 0.0},
        compound_interest=True,
        order_criterion="cache",
        max_n_updated=0,
        positive_entry_threshold=[8, 9],
        negative_entry_threshold=[8, 9],
        exit_threshold="auto",
        positive_probability_threshold=[5, 6, 7, 8, 9],
        negative_probability_threshold=[8, 9, "9*1.25", "9*1.5"],
        adjust_prediction=False,
    )
)


V1_CSET1 = OrderedDict(
    dict(
        base_currency="USDT",
        position_side="longshort",
        exit_if_achieved=True,
        achieve_ratio=[0.8, 1],
        achieved_with_commission=True,
        min_holding_minutes=[0, 1],
        max_holding_minutes=30,
        entry_ratio=0.09,
        commission={"entry": 0.0004, "exit": 0.0002, "spread": 0.0004},
        compound_interest=True,
        order_criterion="capital",
        max_n_updated=0,
        positive_entry_threshold=[9],
        negative_entry_threshold=[9],
        exit_threshold="auto",
        positive_probability_threshold=[5, 6, 7, 8, 9],
        negative_probability_threshold=[8, 9, "9*1.25", "9*1.5"],
        adjust_prediction=False,
        possible_in_debt=False,
    )
)

V1_CSET2 = OrderedDict(
    dict(
        base_currency="USDT",
        position_side="longshort",
        exit_if_achieved=True,
        achieve_ratio=[0.8, 1],
        achieved_with_commission=True,
        min_holding_minutes=[0, 1],
        max_holding_minutes=30,
        entry_ratio=0.1,
        commission={"entry": 0.0004, "exit": 0.0002, "spread": 0.0004},
        compound_interest=True,
        order_criterion="cache",
        max_n_updated=0,
        positive_entry_threshold=[8, 9],
        negative_entry_threshold=[8, 9],
        exit_threshold="auto",
        positive_probability_threshold=[5, 6, 7, 8, 9],
        negative_probability_threshold=[8, 9, "9*1.25", "9*1.5"],
        adjust_prediction=False,
    )
)
</file>

<file path="develop/src/trainer/models/basic_predictor.py">
import os
import shutil
import fire
import json
import joblib
from tqdm import tqdm
import pandas as pd
from copy import copy
from contextlib import contextmanager
from abc import abstractmethod

import torch
import torch.nn as nn
from develop.src.common_utils_dev import load_text, load_json, to_abs_path, get_parent_dir
from .utils import save_model, load_model, weights_init
from .criterions import CRITERIONS
from ..datasets.dataset import Dataset
from torch.utils.data import DataLoader
# from trainer.models import backbones
from develop.src.trainer.models import backbones

COMMON_CONFIG = {
    "data_dir": to_abs_path(__file__, "../../../storage/dataset/dataset/v001/train"),
    "exp_dir": to_abs_path(__file__, "../../../storage/experiments/v001"),
    "test_data_dir": to_abs_path(
        __file__, "../../../storage/dataset/dataset/v001/test"
    ),
}


DATA_CONFIG = {
    "checkpoint_dir": "./check_point",
    "generate_output_dir": "./generated_output",
    "base_feature_assets": ["BTC-USDT"],
}

MODEL_CONFIG = {
    "lookback_window": 120,
    "batch_size": 512,
    "lr": 0.0001,
    "epochs": 15,
    "print_epoch": 1,
    "print_iter": 50,
    "save_epoch": 1,
    "criterion": "l2",
    "criterion_params": {},
    "load_strict": False,
    "model_name": "BackboneV1",
    "model_params": {
        "in_channels": 84,
        "n_blocks": 5,
        "n_block_layers": 8,
        "growth_rate": 12,
        "dropout": 0.1,
        "channel_reduction": 0.5,
        "activation": "selu",
        "normalization": None,
        "seblock": True,
        "sablock": True,
    },
}


def _mutate_config_path(data_config, exp_dir):
    for key in ["checkpoint_dir", "generate_output_dir"]:
        if data_config[key][0] != "/":
            data_config[key] = os.path.join(exp_dir, data_config[key])

        if "_dir" in key:
            os.makedirs(data_config[key], exist_ok=True)

    return data_config


class BasicPredictor:
    def __init__(
        self,
        data_dir=COMMON_CONFIG["data_dir"],
        test_data_dir=COMMON_CONFIG["test_data_dir"],
        d_config={},
        m_config={},
        exp_dir=COMMON_CONFIG["exp_dir"],
        device=None,
        pin_memory=False,
        num_workers=8,
        mode="train",
        default_d_config=DATA_CONFIG,
        default_m_config=MODEL_CONFIG,
    ):
        #   
        if device is None:
            if torch.backends.mps.is_available():
                device = "mps"
            elif torch.cuda.is_available():
                device = "cuda"
            else:
                device = "cpu"
        self.device = device

        # MPS   num_workers 
        if self.device == "mps":
            self.num_workers = 0  # macOS MPS   num_workers=0 


        assert mode in ("train", "test", "predict")
        self.data_dir = data_dir
        self.test_data_dir = test_data_dir
        self.exp_dir = exp_dir
        self.device = device
        self.pin_memory = pin_memory
        self.num_workers = num_workers
        self.mode = mode

        # Build params & configs
        self._load_dataset_params(mode=mode)
        self.data_config, self.model_config = self._build_config(
            d_config=d_config,
            m_config=m_config,
            default_d_config=default_d_config,
            default_m_config=default_m_config,
        )
        self.asset_to_id = self._build_asset_to_id()

        self.model = self._build_model()

        self.iterable_train_data_loader = None
        self.iterable_test_data_loader = None

        if mode == "train":
            self.train_data_loader, self.test_data_loader = self._build_data_loaders(
                mode=mode
            )
            self.optimizer = self._build_optimizer()
            self.criterion = self._build_criterion()
            self.binary_cross_entropy = CRITERIONS["bce"]().to(self.device)

            # Store params
            self._copy_dataset_artifacts()
            self._store_params()

        if mode == "test":
            _, self.test_data_loader = self._build_data_loaders(mode=mode)

        if mode in ("test", "predict"):
            self._load_label_scaler()

    def _copy_dataset_artifacts(self):
        # Copy files from dataset
        for base_file, target_file in [
            (
                os.path.join(get_parent_dir(self.data_dir), "dataset_params.json"),
                os.path.join(self.exp_dir, "dataset_params.json"),
            ),
            (
                os.path.join(get_parent_dir(self.data_dir), "feature_scaler.pkl"),
                os.path.join(self.exp_dir, "feature_scaler.pkl"),
            ),
            (
                os.path.join(get_parent_dir(self.data_dir), "label_scaler.pkl"),
                os.path.join(self.exp_dir, "label_scaler.pkl"),
            ),
        ]:
            shutil.copy(base_file, target_file)

    def _store_params(self):
        params = {
            "data_dir": self.data_dir,
            "test_data_dir": self.test_data_dir,
            "model_config": self.model_config,
            "data_config": self.data_config,
            "asset_to_id": self.asset_to_id,
        }
        with open(os.path.join(self.exp_dir, f"trainer_params.json"), "w") as f:
            json.dump(params, f)

        print(f"[+] Params are stored")

    def _load_label_scaler(self):
        self.label_scaler = joblib.load(os.path.join(self.exp_dir, "label_scaler.pkl"))

    def _load_dataset_params(self, mode):
        if mode == "train":
            self.dataset_params = load_json(
                os.path.join(get_parent_dir(self.data_dir), "dataset_params.json")
            )
            return

        self.dataset_params = load_json(
            os.path.join(self.exp_dir, "dataset_params.json")
        )

    def _build_config(self, d_config, m_config, default_d_config, default_m_config):
        # refine path with exp_dirs
        data_config = copy(default_d_config)
        model_config = copy(default_m_config)
        if not set(m_config.keys()).issubset(set(model_config.keys())):
            raise ValueError(f"{set(m_config.keys()) - set(model_config.keys())}")

        if not set(d_config.keys()).issubset(set(data_config.keys())):
            raise ValueError(f"{set(d_config.keys()) - set(data_config.keys())}")

        data_config = {**data_config, **d_config}

        model_params = {
            **model_config.pop("model_params", {}),
            **m_config.pop("model_params", {}),
        }
        model_config = {**model_config, **m_config, **{"model_params": model_params}}

        data_config = _mutate_config_path(data_config=data_config, exp_dir=self.exp_dir)

        # Mutate model_params' n_assets
        if "n_assets" not in model_config["model_params"]:
            n_assets = len(
                [
                    tradable_coin
                    for tradable_coin in self.dataset_params["tradable_coins"]
                ]
            )

            model_config["model_params"]["n_assets"] = n_assets

        return data_config, model_config

    def _build_asset_to_id(self):
        tradable_coins = [
            tradable_coin for tradable_coin in self.dataset_params["tradable_coins"]
        ]
        asset_to_id = {
            tradable_coin: idx for idx, tradable_coin in enumerate(tradable_coins)
        }
        return asset_to_id

    def _build_transfroms(self):
        return {}

    def _build_data_loaders(self, mode):
        assert mode in ("train", "test")
        transforms = self._build_transfroms()

        # Build base params
        base_dataset_params = {
            "transforms": transforms,
            "lookback_window": self.model_config["lookback_window"],
            "base_feature_assets": self.data_config["base_feature_assets"],
            "asset_to_id": self.asset_to_id,
        }

        #   (MPS  pin_memory=False  )
        base_data_loader_params = {
            "batch_size": self.model_config["batch_size"],
            "pin_memory": self.pin_memory if self.device != "mps" else False,
            "num_workers": self.num_workers,
        }

        # Build dataset & data_loader
        test_dataset = Dataset(data_dir=self.test_data_dir, **base_dataset_params)

        train_data_loader = None
        if mode == "train":
            # Define: dataset
            train_dataset = Dataset(data_dir=self.data_dir, **base_dataset_params)

            # Define data_loader
            train_data_loader = DataLoader(
                dataset=train_dataset, shuffle=True, **base_data_loader_params
            )

            test_data_loader = DataLoader(
                dataset=test_dataset, shuffle=True, **base_data_loader_params
            )

        if mode == "test":
            test_data_loader = DataLoader(
                dataset=test_dataset, shuffle=False, **base_data_loader_params
            )

        return train_data_loader, test_data_loader

    def _load_model(self, model):
        # load model (inplace)
        self.last_epoch = load_model(
            model=model,
            dir=self.data_config["checkpoint_dir"],
            strict=self.model_config["load_strict"],
            device=self.device,
        )
        if self.mode in ("test", "predict"):
            assert self.last_epoch != -1

    def _save_model(self, model, epoch):
        save_model(model=model, dir=self.data_config["checkpoint_dir"], epoch=epoch)

    def _build_model(self):
        # Define  model
        model = getattr(backbones, self.model_config["model_name"])(
            **self.model_config["model_params"]
        )

        # Init model's weights
        model.apply(weights_init)

        # Setup device
        # if torch.cuda.device_count() > 1:
        #     print("Notice: use ", torch.cuda.device_count(), "GPUs")
        #     model = nn.DataParallel(model)
        # else:
        #     print(f"Notice: use {self.device}")
        if self.device.startswith("cuda") and torch.cuda.device_count() > 1:
            print(f"Notice: using {torch.cuda.device_count()} GPUs")
            model = nn.DataParallel(model)
        else:
            print(f"Notice: using {self.device}")


        # Load models
        self._load_model(model=model)
        model.to(self.device)

        return model

    def _build_optimizer(self):
        # set optimizer
        optimizer = torch.optim.AdamW(
            params=self.model.parameters(), lr=self.model_config["lr"]
        )

        return optimizer

    def _build_criterion(self):
        return CRITERIONS[self.model_config["criterion"]](
            **self.model_config["criterion_params"]
        ).to(self.device)

    def _generate_train_data_dict(self):
        if self.iterable_train_data_loader is None:
            self.iterable_train_data_loader = iter(self.train_data_loader)

        # Pick data
        try:
            train_data_dict = next(self.iterable_train_data_loader)
        except StopIteration:
            self.iterable_train_data_loader = iter(self.train_data_loader)
            train_data_dict = next(self.iterable_train_data_loader)

        train_data_dict = {
            key: value.to(self.device) for key, value in train_data_dict.items()
        }
        return train_data_dict

    def _generate_test_data_dict(self):
        if self.iterable_test_data_loader is None:
            self.iterable_test_data_loader = iter(self.test_data_loader)

        # Pick data
        try:
            test_data_dict = next(self.iterable_test_data_loader)
        except StopIteration:
            self.iterable_test_data_loader = iter(self.test_data_loader)
            test_data_dict = next(self.iterable_test_data_loader)

        test_data_dict = {
            key: value.to(self.device) for key, value in test_data_dict.items()
        }
        return test_data_dict

    @abstractmethod
    def _step(self, train_data_dict):
        pass

    @abstractmethod
    def train(self):
        """
        Train model
        """
        pass

    @abstractmethod
    def generate(self, save_dir=None):
        """
        Generate historical predictions csv
        """

    @abstractmethod
    def predict(self, X):
        """
        Predict
        """
</file>

<file path="services/src/trader/trader_v1.py">
import os
import gc
import time
import ccxt
import requests
import urllib3
import joblib
import pandas as pd
import numpy as np
from dataclasses import dataclass

from config import CFG
from trainer.models import PredictorV1
from database.usecase import Usecase
from exchange.custom_client import CustomClient
from .utils import nan_to_zero
from logging import getLogger
from common_utils_svc import initialize_trader_logger, Position
from dataset_builder.build_dataset import DatasetBuilder
from trainer.datasets.dataset import build_X_and_BX


logger = getLogger("trader")
initialize_trader_logger()

LAST_ENTRY_AT_FILE_PATH = "/app/storage/trader/last_entry_at.pkl"


@dataclass
class TraderV1:
    usecase = Usecase()
    possible_in_debt = False
    commission = {"entry": 0.0004, "exit": 0.0002, "spread": 0.0004}
    skip_executable_order_check = True  # To prevent api limitation

    def __post_init__(self):
        self.custom_cli = CustomClient()
        self.tradable_coins = pd.Index(self.custom_cli.tradable_coins)

        self._set_params()
        self._set_test_params()
        self._set_bins(
            prediction_abs_bins=self.prediction_abs_bins,
            probability_bins=self.probability_bins,
            index=self.tradable_coins,
        )
        self._build_dataset_builder()
        self._build_model()
        self._load_last_entry_at()
        self._initialize_order_books()

        self.cached_pricing = None

        if self.skip_executable_order_check is True:
            assert self.order_criterion == "capital"

    def _set_params(self):
        # Set params which has dependency on trader logic
        self.base_currency = CFG.REPORT_PARAMS["base_currency"]
        self.position_side = CFG.REPORT_PARAMS["position_side"]
        self.entry_ratio = CFG.REPORT_PARAMS["entry_ratio"] * CFG.LEVERAGE
        logger.info(f"[O] Info: leverage is {CFG.LEVERAGE}")

        self.min_holding_minutes = CFG.REPORT_PARAMS["min_holding_minutes"]
        self.max_holding_minutes = CFG.REPORT_PARAMS["max_holding_minutes"]
        self.compound_interest = CFG.REPORT_PARAMS["compound_interest"]
        self.order_criterion = CFG.REPORT_PARAMS["order_criterion"]
        self.exit_if_achieved = CFG.REPORT_PARAMS["exit_if_achieved"]
        self.achieve_ratio = CFG.REPORT_PARAMS["achieve_ratio"]
        self.achieved_with_commission = CFG.REPORT_PARAMS["achieved_with_commission"]
        self.max_n_updated = CFG.REPORT_PARAMS["max_n_updated"]

        # Currently we accept only 0
        assert self.max_n_updated == 0

        self.positive_entry_threshold = CFG.REPORT_PARAMS["positive_entry_threshold"]
        self.negative_entry_threshold = CFG.REPORT_PARAMS["negative_entry_threshold"]
        self.exit_threshold = CFG.REPORT_PARAMS["exit_threshold"]
        self.positive_probability_threshold = CFG.REPORT_PARAMS[
            "positive_probability_threshold"
        ]
        self.negative_probability_threshold = CFG.REPORT_PARAMS[
            "negative_probability_threshold"
        ]
        self.adjust_prediction = CFG.REPORT_PARAMS["adjust_prediction"]

        # Currently we accept False adjust_prediction
        assert self.adjust_prediction is False

        self.prediction_abs_bins = CFG.PREDICTION_ABS_BINS
        self.probability_bins = CFG.PROBABILITY_BINS

        # Set data builder params
        self.dataset_builder_params = {}
        self.dataset_builder_params["features_columns"] = [
            (column[0].replace("-", "/"), column[1])
            for column in CFG.DATASET_PARAMS["features_columns"]
        ]
        self.dataset_builder_params["winsorize_threshold"] = CFG.DATASET_PARAMS[
            "winsorize_threshold"
        ]
        self.dataset_builder_params["base_feature_assets"] = [
            base_feature_asset.replace("-", "/")
            for base_feature_asset in CFG.EXP_DATA_PARAMS["base_feature_assets"]
        ]
        self.dataset_builder_params["asset_to_id"] = {
            key.replace("-", "/"): value
            for key, value in CFG.EXP_PARAMS["asset_to_id"].items()
        }
        self.dataset_builder_params["id_to_asset"] = {
            value: key.replace("-", "/")
            for key, value in CFG.EXP_PARAMS["asset_to_id"].items()
        }

    def _set_test_params(self):
        if CFG.TEST_MODE is True:
            assert self.custom_cli.test_mode is True
            self.entry_ratio = 0.0001

    def _set_bins(self, prediction_abs_bins, probability_bins, index):
        assert (prediction_abs_bins >= 0).all().all()
        assert (probability_bins >= 0).all().all()

        self.positive_entry_bins = None
        self.negative_entry_bins = None
        self.exit_bins = None
        self.positive_probability_bins = None
        self.negative_probability_bins = None

        if isinstance(self.positive_entry_threshold, str):
            if "*" in self.positive_entry_threshold:
                self.positive_entry_bins = (
                    prediction_abs_bins.loc[
                        int(self.positive_entry_threshold.split("*")[0])
                    ]
                    * float(self.positive_entry_threshold.split("*")[-1])
                )[index]
        else:
            self.positive_entry_bins = prediction_abs_bins.loc[
                self.positive_entry_threshold
            ][index]

        if isinstance(self.negative_entry_threshold, str):
            if "*" in self.negative_entry_threshold:
                self.negative_entry_bins = -(
                    prediction_abs_bins.loc[
                        int(self.negative_entry_threshold.split("*")[0])
                    ]
                    * float(self.negative_entry_threshold.split("*")[-1])
                )[index]
        else:
            self.negative_entry_bins = -prediction_abs_bins.loc[
                self.negative_entry_threshold
            ][index]

        if isinstance(self.exit_threshold, str):
            if "*" in self.exit_threshold:
                self.exit_bins = (
                    prediction_abs_bins.loc[int(self.exit_threshold.split("*")[0])]
                    * float(self.exit_threshold.split("*")[-1])
                )[index]
        else:
            self.exit_bins = prediction_abs_bins.loc[self.exit_threshold][index]

        if isinstance(self.positive_probability_threshold, str):
            if "*" in self.positive_probability_threshold:
                self.positive_probability_bins = (
                    probability_bins.loc[
                        int(self.positive_probability_threshold.split("*")[0])
                    ]
                    * float(self.positive_probability_threshold.split("*")[-1])
                )[index]
        else:
            self.positive_probability_bins = probability_bins.loc[
                self.positive_probability_threshold
            ][index]

        if isinstance(self.negative_probability_threshold, str):
            if "*" in self.negative_probability_threshold:
                self.negative_probability_bins = (
                    probability_bins.loc[
                        int(self.negative_probability_threshold.split("*")[0])
                    ]
                    * float(self.negative_probability_threshold.split("*")[-1])
                )[index]
        else:
            self.negative_probability_bins = probability_bins.loc[
                self.negative_probability_threshold
            ][index]

    def _build_dataset_builder(self):
        feature_scaler = joblib.load(os.path.join(CFG.EXP_DIR, "feature_scaler.pkl"))
        label_scaler = joblib.load(os.path.join(CFG.EXP_DIR, "label_scaler.pkl"))

        self.dataset_builder = DatasetBuilder(
            tradable_coins=self.tradable_coins,
            feature_columns=self.dataset_builder_params["features_columns"],
            feature_scaler=feature_scaler,
            label_scaler=label_scaler,
        )

    def _build_model(self):
        self.model = PredictorV1(
            exp_dir=CFG.EXP_DIR,
            m_config=CFG.EXP_MODEL_PARAMS,
            d_config=CFG.EXP_DATA_PARAMS,
            device="cpu",
            mode="predict",
        )

    def _store_last_entry_at(self):
        joblib.dump(self.last_entry_at, LAST_ENTRY_AT_FILE_PATH)

    def _load_last_entry_at(self):
        if os.path.exists(LAST_ENTRY_AT_FILE_PATH):
            self.last_entry_at = joblib.load(LAST_ENTRY_AT_FILE_PATH)
            logger.info(f"[O] Info: loaded last_entry_at")
        else:
            self.last_entry_at = {key: None for key in self.tradable_coins}

        # Initialize
        positions = self.custom_cli.get_position_objects(with_entry_at=True)
        for position in positions:
            if self.last_entry_at[position.asset] is not None:
                self.last_entry_at[position.asset] = max(
                    position.entry_at, self.last_entry_at[position.asset]
                )
            else:
                self.last_entry_at[position.asset] = position.entry_at

    def _initialize_order_books(self):
        positions = self.custom_cli.get_position_objects(with_entry_at=False)

        for position in positions:
            orders = self.custom_cli.get_open_orders(symbol=position.asset)

            # When already limit order exists, we skip it.
            if len(orders) >= 1:
                continue

            assert position.entry_price != 0.0
            self.custom_cli.exit_order(
                symbol=position.asset,
                order_type="limit",
                position=position.side,
                amount=position.qty,
                price=self.compute_price_to_achieve(
                    position=position, entry_price=position.entry_price
                ),
            )

        logger.info(f"[O] Info: initialized order books")

    def _build_features(self, pricing):
        features = self.dataset_builder.build_features(rawdata=pricing)
        features = self.dataset_builder.preprocess_features(
            features=features,
            winsorize_threshold=self.dataset_builder_params["winsorize_threshold"],
        )

        return features

    def _build_inputs(self, features):
        features, base_features = build_X_and_BX(
            features=features.astype("float32"),
            base_feature_assets=self.dataset_builder_params["base_feature_assets"],
        )

        inputs = []
        for target_coin in self.tradable_coins:
            to_input = pd.concat([base_features, features[target_coin]], axis=1)

            to_input = np.swapaxes(to_input.values, 0, 1)

            inputs.append(to_input)

        inputs = np.stack(inputs, axis=0)
        ids = [
            self.dataset_builder_params["asset_to_id"][target_coin]
            for target_coin in self.tradable_coins
        ]

        return inputs, ids

    def build_prediction_dict(self, last_sync_on):
        query_start_on = last_sync_on - pd.Timedelta(
            minutes=(1320 + CFG.EXP_MODEL_PARAMS["lookback_window"] - 1)
        )
        query_end_on = last_sync_on

        if self.cached_pricing is None:
            pricing = self.usecase.get_pricing(
                start_on=query_start_on, end_on=query_end_on
            )
        else:
            # Get extra 1 candle, cause it has potential to be changed.
            pricing = self.usecase.get_pricing(
                start_on=self.cached_pricing.index.levels[0][-1], end_on=query_end_on
            )
            pricing = pd.concat(
                [
                    self.cached_pricing[
                        query_start_on : self.cached_pricing.index.levels[0][-2]
                    ],
                    pricing,
                ]
            ).sort_index()

        self.cached_pricing = pricing

        pricing = pricing.unstack().swaplevel(0, 1, axis=1)
        features = self._build_features(pricing=pricing)
        inputs, ids = self._build_inputs(features=features)

        pred_dict = self.model.predict(
            X=inputs, id=ids, id_to_asset=self.dataset_builder_params["id_to_asset"]
        )

        return pred_dict

    def build_positive_and_negative_assets(self, pred_dict):
        # Set assets which has signals
        positive_assets = self.tradable_coins[
            (pred_dict["predictions"] >= self.positive_entry_bins)
            & (pred_dict["probabilities"] >= self.positive_probability_bins)
        ]
        negative_assets = self.tradable_coins[
            (pred_dict["predictions"] <= self.negative_entry_bins)
            & (pred_dict["probabilities"] >= self.negative_probability_bins)
        ]

        return positive_assets, negative_assets

    def is_executable(self, last_sync_on: pd.Timestamp, now: pd.Timestamp):
        if last_sync_on is None:
            return False

        sync_min_delta = int((now - last_sync_on).total_seconds() // 60)

        if sync_min_delta == 1:
            last_trade_on = self.usecase.get_last_trade_on()
            if last_trade_on is None:
                return True
            else:
                if int((now - last_trade_on).total_seconds() // 60) >= 1:
                    return True

        return False

    def exit_order(self, position):
        self.custom_cli.cancel_orders(symbol=position.asset)

        ordered = self.custom_cli.exit_order(
            symbol=position.asset,
            order_type="market",
            position=position.side,
            amount=position.qty,
        )
        if ordered is None:
            assert CFG.TEST_MODE is True
            return

    def handle_exit(self, positions, positive_assets, negative_assets, now):
        for position_idx, position in enumerate(positions):
            # Keep position if matched
            if (position.side == "long") and (position.asset in positive_assets):
                continue

            if (position.side == "short") and (position.asset in negative_assets):
                continue

            position_entry_at = self.last_entry_at[position.asset]
            passed_minutes = (now - position_entry_at).total_seconds() // 60

            # Handle min_holding_minutes
            if passed_minutes <= self.min_holding_minutes:
                continue

            # Handle max_holding_minutes
            if passed_minutes >= self.max_holding_minutes:
                self.exit_order(position=position)
                positions[position_idx].is_exited = True
                logger.info(f"[-] Exit: {str(position)}, max_holding")
                continue

            # Handle exit signal
            if (position.side == "long") and (position.asset in negative_assets):
                self.exit_order(position=position)
                positions[position_idx].is_exited = True
                logger.info(f"[-] Exit: {str(position)}, opposite")
                continue

            if (position.side == "short") and (position.asset in positive_assets):
                self.exit_order(position=position)
                positions[position_idx].is_exited = True
                logger.info(f"[-] Exit: {str(position)}, opposite")
                continue

        # Delete exited positions
        positions = [
            position for position in positions if position.is_exited is not True
        ]

        return positions

    def check_if_opposite_position_exists(self, positions, order_asset, order_side):
        if order_side == "long":
            opposite_side = "short"
        if order_side == "short":
            opposite_side = "long"

        for exist_position in positions:
            if (exist_position.asset == order_asset) and (
                exist_position.side == opposite_side
            ):
                return True

        return False

    def compute_cost_to_order(self, position):
        cache_to_order = position.entry_price * position.qty
        commission_to_order = cache_to_order * (
            self.commission["entry"] + self.commission["spread"]
        )

        return cache_to_order + commission_to_order

    def check_if_already_have(self, positions, position):
        for exist_position in positions:
            if (exist_position.asset == position.asset) and (
                exist_position.side == position.side
            ):
                return True

        return False

    def check_if_executable_order(self, position):
        if self.skip_executable_order_check is True:
            is_enough_ammount = bool(
                position.qty >= self.custom_cli.ammount_constraints[position.asset]
            )
            return is_enough_ammount

        cache = self.custom_cli.get_cache_dict()["free"]
        cost = self.compute_cost_to_order(position=position)

        is_enough_cache = bool((cache - cost) >= 0)
        is_enough_ammount = bool(
            position.qty >= self.custom_cli.ammount_constraints[position.asset]
        )

        return is_enough_cache & is_enough_ammount

    def compute_price_to_achieve(self, position, entry_price, predictions=None):
        if predictions is not None:
            prediction = predictions[position.asset]
        else:
            if position.side == "long":
                prediction = self.positive_entry_bins[position.asset]

            if position.side == "short":
                prediction = self.negative_entry_bins[position.asset]

        commission = self.commission
        if self.achieved_with_commission is not True:
            commission["entry"] = 0
            commission["exit"] = 0
            commission["spread"] = 0

        if position.side == "long":
            assert prediction >= 0
            price_to_achieve = (
                entry_price
                * (
                    (prediction * self.achieve_ratio)
                    + 1
                    + (commission["entry"] + commission["spread"])
                )
                / (1 - (commission["exit"] + commission["spread"]))
            )

        if position.side == "short":
            assert prediction <= 0
            price_to_achieve = (
                entry_price
                * (
                    (prediction * self.achieve_ratio)
                    + 1
                    - (commission["entry"] + commission["spread"])
                )
                / (1 + (commission["exit"] + commission["spread"]))
            )

        return price_to_achieve

    def entry_order(self, positions, asset, side, cache_to_order, pricing, now):
        if cache_to_order == 0:
            return

        # if opposite position exists, we dont entry
        if (
            self.check_if_opposite_position_exists(
                positions=positions, order_asset=asset, order_side=side
            )
            is True
        ):
            return

        entry_price = pricing[asset]
        qty = cache_to_order / entry_price

        position = Position(
            asset=asset, side=side, qty=qty, entry_price=entry_price, entry_at=now
        )

        # Currently update_position_if_already_have is not supported.
        already_have = self.check_if_already_have(
            positions=positions, position=position
        )
        if already_have is True:
            self.last_entry_at[position.asset] = now
            return

        executable_order = self.check_if_executable_order(position=position)
        if executable_order is True:
            ordered = self.custom_cli.entry_order(
                symbol=position.asset,
                order_type="market",
                position=position.side,
                amount=position.qty,
            )
            if ordered is None:
                return

            self.last_entry_at[position.asset] = now

            if self.exit_if_achieved is True:
                self.assets_to_limit_order.append(position.asset)

            logger.info(f"[+] Entry: {str(position)}")

    def handle_entry(
        self,
        positions,
        cache_to_order,
        positive_assets,
        negative_assets,
        pricing,
        predictions,
        now,
    ):
        # Set init to handle limit order
        self.assets_to_limit_order = []

        # Entry order
        if self.position_side in ("long", "longshort"):
            for order_asset in positive_assets:
                self.entry_order(
                    positions=positions,
                    asset=order_asset,
                    side="long",
                    cache_to_order=cache_to_order,
                    pricing=pricing,
                    now=now,
                )

        if self.position_side in ("short", "longshort"):
            for order_asset in negative_assets:
                self.entry_order(
                    positions=positions,
                    asset=order_asset,
                    side="short",
                    cache_to_order=cache_to_order,
                    pricing=pricing,
                    now=now,
                )

        # Limit order
        if len(self.assets_to_limit_order) > 0:
            positions = self.custom_cli.get_position_objects(with_entry_at=False)

            for position in positions:
                if position.asset not in self.assets_to_limit_order:
                    continue

                assert position.entry_price != 0.0
                self.custom_cli.exit_order(
                    symbol=position.asset,
                    order_type="limit",
                    position=position.side,
                    amount=position.qty,
                    price=self.compute_price_to_achieve(
                        position=position,
                        entry_price=position.entry_price,
                        predictions=predictions,
                    ),
                )

    def run(self):
        logger.info(f"[O] Start: demon of trader")
        n_traded = 0

        while True:
            # Handle relogin
            if n_traded == 60:
                self.custom_cli = CustomClient()
                n_traded = 0

            # Main
            try:
                # Use timestamp without second info
                now = pd.Timestamp.utcnow().floor("T")
                last_sync_on = self.usecase.get_last_sync_on()

                if self.is_executable(last_sync_on=last_sync_on, now=now) is True:
                    pred_dict = self.build_prediction_dict(last_sync_on=last_sync_on)
                    (
                        positive_assets,
                        negative_assets,
                    ) = self.build_positive_and_negative_assets(pred_dict=pred_dict)

                    # Handle exit
                    positions = self.custom_cli.get_position_objects(
                        with_entry_at=False
                    )
                    positions = self.handle_exit(
                        positions=positions,
                        positive_assets=positive_assets,
                        negative_assets=negative_assets,
                        now=now,
                    )
                    long_positions = [
                        position for position in positions if position.side == "long"
                    ]
                    short_positions = [
                        position for position in positions if position.side == "short"
                    ]

                    # Compute how much use cache to order
                    cache_dict = self.custom_cli.get_cache_dict()
                    capital = cache_dict["total"]
                    cache = cache_dict["free"]

                    logger.info(
                        f"[_] Capital: {capital:.2f}$ | Holds: long({len(long_positions)}), short({len(short_positions)}) | Signals: pos({len(positive_assets)}), neg({len(negative_assets)})"
                    )

                    if self.compound_interest is False:
                        cache_to_order = self.entry_ratio
                    else:
                        if self.order_criterion == "cache":
                            if cache > 0:
                                cache_to_order = nan_to_zero(
                                    value=(cache * self.entry_ratio)
                                )
                            else:
                                cache_to_order = 0

                        elif self.order_criterion == "capital":
                            # Entry with capital base
                            cache_to_order = nan_to_zero(
                                value=(capital * self.entry_ratio)
                            )

                    # Handle entry
                    pricing = self.custom_cli.get_last_pricing()
                    self.handle_entry(
                        positions=positions,
                        cache_to_order=cache_to_order,
                        positive_assets=positive_assets,
                        negative_assets=negative_assets,
                        pricing=pricing,
                        predictions=pred_dict["predictions"],
                        now=now,
                    )

                    # Record traded
                    self.usecase.insert_trade({"timestamp": now})
                    self._store_last_entry_at()

                    n_traded += 1
                else:
                    time.sleep(0.1)

            except Exception as e:
                logger.error("[!] Error: ", exc_info=True)
                raise Exception


if __name__ == "__main__":
    import fire

    fire.Fire(TraderV1)
</file>

</files>
